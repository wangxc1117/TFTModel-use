{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa646ef",
   "metadata": {},
   "source": [
    "# SpatialNeuralAdapter Demo\n",
    "\n",
    "This notebook demonstrates the optimized SpatialNeuralAdapter with comprehensive training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, Any\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "from geospatial_neural_adapter import (\n",
    "    SpatialNeuralAdapter,\n",
    "    SpatialBasisLearner,\n",
    "    TrendModel,\n",
    "    compute_metrics,\n",
    ")\n",
    "\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling, denormalize_predictions\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae7aa1",
   "metadata": {},
   "source": [
    "## 1. Data Generation with Meaningful Correlations\n",
    "\n",
    "We'll use the improved data generator that creates features with strong correlations to targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic temporal data with meaningful correlations\n",
    "print(\"Generating correlated temporal synthetic data...\")\n",
    "\n",
    "n_locations = 50\n",
    "n_time_steps = 200\n",
    "locations = np.linspace(-5, 5, n_locations)\n",
    "noise_std = 0.1\n",
    "eigenvalue = 2.0\n",
    "\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locations,\n",
    "    n_time_steps=n_time_steps,\n",
    "    noise_std=noise_std,\n",
    "    eigenvalue=eigenvalue,\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Data shapes: {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature-target correlations\n",
    "print(\"Feature-Target Correlations:\")\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    print(f\"  Feature {i}: {corr:.4f}\")\n",
    "\n",
    "# Visualize data characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Target distribution\n",
    "axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Target Distribution')\n",
    "axes[0, 0].set_xlabel('Target Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Spatial pattern at first time step\n",
    "axes[0, 1].plot(locations, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "axes[0, 1].set_xlabel('Location')\n",
    "axes[0, 1].set_ylabel('Target Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Temporal pattern at middle location\n",
    "time_steps = np.arange(len(targets))\n",
    "axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "axes[1, 0].set_xlabel('Time Step')\n",
    "axes[1, 0].set_ylabel('Target Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature correlations\n",
    "feature_corrs = []\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    feature_corrs.append(corr)\n",
    "\n",
    "axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Feature-Target Correlations')\n",
    "axes[1, 1].set_xlabel('Feature Index')\n",
    "axes[1, 1].set_ylabel('Correlation')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55a2b",
   "metadata": {},
   "source": [
    "## 2. Enhanced Preprocessing with Automatic Scaling\n",
    "\n",
    "We'll use the improved preprocessing pipeline that handles normalization and denormalization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets with automatic scaling (enhanced preprocessing)\n",
    "print(\"Preparing datasets with automatic scaling...\")\n",
    "\n",
    "# Use enhanced preprocessing with automatic scaling\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "\n",
    "train_cat, train_cont, train_targets = train_dataset.tensors\n",
    "val_cat, val_cont, val_targets = val_dataset.tensors\n",
    "test_cat, test_cont, test_targets = test_dataset.tensors\n",
    "\n",
    "print(f\"Dataset sizes: {len(train_dataset)}, {len(val_dataset)}, {len(test_dataset)}\")\n",
    "\n",
    "# Print scaler information\n",
    "scaler_info = preprocessor.get_scaler_info()\n",
    "print(f\"Target scaler - Mean: {scaler_info['target_mean'][0]:.2f}, Std: {scaler_info['target_scale'][0]:.2f}\")\n",
    "\n",
    "# Create data loader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Get data dimensions\n",
    "T, N, F = cont_features.shape\n",
    "print(f\"Original data shape: {cont_features.shape}\")\n",
    "print(f\"Training time steps: {len(train_dataset)}\")\n",
    "print(f\"Validation time steps: {len(val_dataset)}\")\n",
    "print(f\"Test time steps: {len(test_dataset)}\")\n",
    "print(f\"Number of locations: {N}\")\n",
    "print(f\"Number of features: {F}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of standardization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Original vs standardized distributions\n",
    "axes[0].hist(targets.flatten(), bins=30, alpha=0.7, label='Original', density=True)\n",
    "axes[0].hist(train_targets.numpy().flatten(), bins=30, alpha=0.7, label='Standardized', density=True)\n",
    "axes[0].set_title('Target Distribution Comparison')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Original vs standardized spatial patterns\n",
    "val_targets_orig = denormalize_predictions(val_targets.numpy(), preprocessor)\n",
    "axes[1].plot(locations, val_targets_orig[0], 'o-', label='Original', alpha=0.7, linewidth=2)\n",
    "axes[1].plot(locations, val_targets[0].numpy(), 's-', label='Standardized', alpha=0.7, linewidth=2)\n",
    "axes[1].set_title('Spatial Pattern Comparison')\n",
    "axes[1].set_xlabel('Location')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4695b37",
   "metadata": {},
   "source": [
    "## 3. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "print(\"Creating models...\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Trend model\n",
    "trend = TrendModel(\n",
    "    num_continuous_features=F,\n",
    "    hidden_layer_sizes=[256, 128, 64],\n",
    "    n_locations=N,\n",
    "    init_weight=None,\n",
    "    init_bias=None,\n",
    "    freeze_init=False,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "# Spatial basis learner\n",
    "basis = SpatialBasisLearner(\n",
    "    num_locations=N,\n",
    "    latent_dim=10,\n",
    "    pca_init=None,\n",
    ")\n",
    "\n",
    "print(f\"Trend model parameters: {sum(p.numel() for p in trend.parameters()):,}\")\n",
    "print(f\"Basis model parameters: {sum(p.numel() for p in basis.parameters()):,}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nTrend Model Architecture:\")\n",
    "print(trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6456c67",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    \"rho\": 5.0,\n",
    "    \"dual_momentum\": 0.2,\n",
    "    \"max_iters\": 100,  # Moderate training\n",
    "    \"min_outer\": 50,\n",
    "    \"lr_mu\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \"phi_every\": 5,\n",
    "    \"phi_freeze\": 50,\n",
    "    \"tol\": 1e-4,\n",
    "    \"adaptive_rho_mu\": 10.0,\n",
    "    \"adaptive_rho_tau_inc\": 2.0,\n",
    "    \"adaptive_rho_tau_dec\": 2.0,\n",
    "    \"matrix_reg\": 1e-6,\n",
    "    \"irl1_max_iters\": 10,\n",
    "    \"irl1_eps\": 1e-6,\n",
    "    \"irl1_tol\": 5e-4,\n",
    "    \"coord_threshold\": 1e-12,\n",
    "    \"avoid_zero_eps\": 1e-12,\n",
    "    \"pretrain_epochs\": 5,\n",
    "    \"use_mixed_precision\": True,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6c6f1",
   "metadata": {},
   "source": [
    "## 5. SpatialNeuralAdapter Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "print(\"Creating SpatialNeuralAdapter...\")\n",
    "\n",
    "# Create tensorboard writer\n",
    "writer = SummaryWriter(\"./logs/spatial_neural_adapter_demo\")\n",
    "\n",
    "trainer = SpatialNeuralAdapter(\n",
    "    trend=trend,\n",
    "    basis=basis,\n",
    "    train_loader=train_loader,\n",
    "    val_cont=val_cont,\n",
    "    val_y=val_targets,\n",
    "    locs=locations,\n",
    "    config=config,\n",
    "    device=device,\n",
    "    writer=writer,\n",
    "    tau1=0.1,\n",
    "    tau2=0.1,\n",
    ")\n",
    "\n",
    "# Print configuration\n",
    "trainer.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain trend model\n",
    "print(\"Pretraining trend model...\")\n",
    "trainer.pretrain_trend(epochs=config.get(\"pretrain_epochs\", 5))\n",
    "\n",
    "# Initialize basis\n",
    "print(\"Initializing spatial basis...\")\n",
    "trainer.init_basis_dense()\n",
    "\n",
    "# Run ADMM training\n",
    "print(\"Starting ADMM training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "best_val = trainer.run()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f}s\")\n",
    "print(f\"Best validation RMSE: {best_val:.6f}\")\n",
    "\n",
    "# Close tensorboard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe2818",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating model...\")\n",
    "\n",
    "trainer.trend.eval()\n",
    "trainer.basis.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get predictions (on standardized scale)\n",
    "    y_pred_std = trainer.predict(val_cont.to(device), val_targets.to(device))\n",
    "    \n",
    "    # Compute metrics on standardized scale\n",
    "    rmse_std, mae_std, r2_std = compute_metrics(val_targets.to(device), y_pred_std)\n",
    "    \n",
    "    # Compute additional metrics on standardized scale\n",
    "    mse_std = torch.nn.functional.mse_loss(val_targets.to(device), y_pred_std).item()\n",
    "    \n",
    "    # Denormalize predictions for original scale evaluation\n",
    "    y_pred_denorm = denormalize_predictions(y_pred_std.cpu().numpy(), preprocessor)\n",
    "    val_targets_denorm = denormalize_predictions(val_targets.cpu().numpy(), preprocessor)\n",
    "    \n",
    "    # Compute metrics on original scale\n",
    "    rmse_denorm = np.sqrt(np.mean((val_targets_denorm - y_pred_denorm) ** 2))\n",
    "    mae_denorm = np.mean(np.abs(val_targets_denorm - y_pred_denorm))\n",
    "    \n",
    "    # R-squared on original scale\n",
    "    ss_res_denorm = np.sum((val_targets_denorm - y_pred_denorm) ** 2)\n",
    "    ss_tot_denorm = np.sum((val_targets_denorm - val_targets_denorm.mean()) ** 2)\n",
    "    r2_denorm = 1 - (ss_res_denorm / ss_tot_denorm)\n",
    "    \n",
    "    mse_denorm = np.mean((val_targets_denorm - y_pred_denorm) ** 2)\n",
    "    \n",
    "    metrics = {\n",
    "        \"rmse_std\": rmse_std,\n",
    "        \"mae_std\": mae_std,\n",
    "        \"r2_std\": r2_std,\n",
    "        \"mse_std\": mse_std,\n",
    "        \"rmse_denorm\": rmse_denorm,\n",
    "        \"mae_denorm\": mae_denorm,\n",
    "        \"r2_denorm\": r2_denorm,\n",
    "        \"mse_denorm\": mse_denorm,\n",
    "    }\n",
    "\n",
    "print(f\"Standardized metrics: RMSE={rmse_std:.6f}, MAE={mae_std:.6f}, RÂ²={r2_std:.6f}\")\n",
    "print(f\"Denormalized metrics: RMSE={rmse_denorm:.6f}, MAE={mae_denorm:.6f}, RÂ²={r2_denorm:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964b3e5",
   "metadata": {},
   "source": [
    "## 7. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Use denormalized data for visualization\n",
    "val_y_np = val_targets_denorm\n",
    "y_pred_np = y_pred_denorm\n",
    "\n",
    "# Plot 1: Predictions vs Actual scatter plot (original scale)\n",
    "axes[0, 0].scatter(val_y_np.flatten(), y_pred_np.flatten(), alpha=0.5, s=20)\n",
    "axes[0, 0].plot([val_y_np.min(), val_y_np.max()], \n",
    "                [val_y_np.min(), val_y_np.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_title('Predictions vs Actual Values')\n",
    "axes[0, 0].set_xlabel('Actual Values')\n",
    "axes[0, 0].set_ylabel('Predicted Values')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Spatial pattern comparison\n",
    "sample_idx = 0\n",
    "axes[0, 1].plot(locations, val_y_np[sample_idx], 'o-', label='Actual', alpha=0.7, linewidth=2, markersize=4)\n",
    "axes[0, 1].plot(locations, y_pred_np[sample_idx], 's-', label='Predicted', alpha=0.7, linewidth=2, markersize=4)\n",
    "axes[0, 1].set_title('Spatial Pattern Comparison')\n",
    "axes[0, 1].set_xlabel('Location')\n",
    "axes[0, 1].set_ylabel('Target Value')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals analysis\n",
    "residuals = val_y_np.flatten() - y_pred_np.flatten()\n",
    "axes[1, 0].scatter(y_pred_np.flatten(), residuals, alpha=0.5, s=20)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.7)\n",
    "axes[1, 0].set_title('Residuals vs Predicted Values')\n",
    "axes[1, 0].set_xlabel('Predicted Values')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Residuals distribution\n",
    "axes[1, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Residuals Distribution')\n",
    "axes[1, 1].set_xlabel('Residual Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61118e",
   "metadata": {},
   "source": [
    "## 8. Spatial Basis Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the learned spatial basis\n",
    "print(\"Analyzing learned spatial basis...\")\n",
    "\n",
    "# Get the basis matrix\n",
    "basis_matrix = trainer.basis.basis.detach().cpu().numpy()\n",
    "print(f\"Basis matrix shape: {basis_matrix.shape}\")\n",
    "print(f\"Basis norm: {np.linalg.norm(basis_matrix):.4f}\")\n",
    "\n",
    "# Visualize the basis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, basis_matrix.shape[1])):\n",
    "    # Plot basis vector as spatial pattern\n",
    "    axes[i].plot(locations, basis_matrix[:, i], 'o-', linewidth=2, markersize=4)\n",
    "    axes[i].set_title(f'Spatial Basis {i+1}')\n",
    "    axes[i].set_xlabel('Location')\n",
    "    axes[i].set_ylabel('Basis Value')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Basis statistics\n",
    "print(\"\\nBasis Statistics:\")\n",
    "print(f\"Mean: {basis_matrix.mean():.4f}\")\n",
    "print(f\"Std: {basis_matrix.std():.4f}\")\n",
    "print(f\"Min: {basis_matrix.min():.4f}\")\n",
    "print(f\"Max: {basis_matrix.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74924cf7",
   "metadata": {},
   "source": [
    "## 9. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 50)\n",
    "print(\"SPATIAL NEURAL ADAPTER DEMO SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Training time: {training_time:.2f}s\")\n",
    "print(f\"Final RMSE (std): {metrics['rmse_std']:.6f}\")\n",
    "print(f\"Final MAE (std): {metrics['mae_std']:.6f}\")\n",
    "print(f\"Final RÂ² (std): {metrics['r2_std']:.6f}\")\n",
    "print(f\"Final MSE (std): {metrics['mse_std']:.6f}\")\n",
    "print(f\"Final RMSE (denorm): {metrics['rmse_denorm']:.6f}\")\n",
    "print(f\"Final MAE (denorm): {metrics['mae_denorm']:.6f}\")\n",
    "print(f\"Final RÂ² (denorm): {metrics['r2_denorm']:.6f}\")\n",
    "print(f\"Final MSE (denorm): {metrics['mse_denorm']:.6f}\")\n",
    "print(f\"Best validation RMSE: {best_val:.6f}\")\n",
    "print(f\"Tensorboard logs: ./logs/spatial_neural_adapter_demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nPerformance Analysis:\")\n",
    "print(f\"Scale recovery: {y_pred_np.max() - y_pred_np.min():.2f} / {val_y_np.max() - val_y_np.min():.2f} = {(y_pred_np.max() - y_pred_np.min()) / (val_y_np.max() - val_y_np.min()):.2%}\")\n",
    "print(f\"Std recovery: {y_pred_np.std():.2f} / {val_y_np.std():.2f} = {y_pred_np.std() / val_y_np.std():.2%}\")\n",
    "\n",
    "print(\"\\nâœ… SpatialNeuralAdapter demo completed successfully!\")\n",
    "print(\"ðŸ’¡ Run 'tensorboard --logdir ./logs/spatial_neural_adapter_demo' to view training progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942d220",
   "metadata": {},
   "source": [
    "## 10. Additional Analysis: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db32d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"Feature Importance Analysis:\")\n",
    "\n",
    "# Calculate feature-target correlations\n",
    "feature_correlations = []\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    feature_correlations.append(corr)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Feature correlations\n",
    "axes[0].bar(range(len(feature_correlations)), feature_correlations, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Feature-Target Correlations')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Correlation')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance ranking\n",
    "feature_importance = np.abs(feature_correlations)\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "axes[1].bar(range(len(sorted_indices)), feature_importance[sorted_indices], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Feature Importance Ranking')\n",
    "axes[1].set_xlabel('Feature Rank')\n",
    "axes[1].set_ylabel('Absolute Correlation')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for i, idx in enumerate(sorted_indices[:5]):\n",
    "    print(f\"  {i+1}. Feature {idx}: {feature_correlations[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
