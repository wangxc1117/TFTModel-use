{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa646ef",
   "metadata": {},
   "source": [
    "# TFT Model Demo\n",
    "\n",
    "This notebook demonstrates the TFT (Temporal Fusion Transformer) model with comprehensive training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, Any\n",
    "import time\n",
    "\n",
    "# Darts imports\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Local imports\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling, denormalize_predictions\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae7aa1",
   "metadata": {},
   "source": [
    "## 1. Data Generation with Meaningful Correlations\n",
    "\n",
    "We'll use the same data generator that creates features with strong correlations to targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic temporal data with meaningful correlations\n",
    "print(\"Generating correlated temporal synthetic data...\")\n",
    "\n",
    "n_locations = 50\n",
    "n_time_steps = 500\n",
    "locations = np.linspace(-5, 5, n_locations)\n",
    "noise_std = 0.1\n",
    "eigenvalue = 2.0\n",
    "\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locations,\n",
    "    n_time_steps=n_time_steps,\n",
    "    noise_std=noise_std,\n",
    "    eigenvalue=eigenvalue,\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Data shapes: {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature-target correlations\n",
    "print(\"Feature-Target Correlations:\")\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    print(f\"  Feature {i}: {corr:.4f}\")\n",
    "\n",
    "# Visualize data characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Target distribution\n",
    "axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Target Distribution')\n",
    "axes[0, 0].set_xlabel('Target Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Spatial pattern at first time step\n",
    "axes[0, 1].plot(locations, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "axes[0, 1].set_xlabel('Location')\n",
    "axes[0, 1].set_ylabel('Target Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Temporal pattern at middle location\n",
    "time_steps = np.arange(len(targets))\n",
    "axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "axes[1, 0].set_xlabel('Time Step')\n",
    "axes[1, 0].set_ylabel('Target Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature correlations\n",
    "feature_corrs = []\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    feature_corrs.append(corr)\n",
    "\n",
    "axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Feature-Target Correlations')\n",
    "axes[1, 1].set_xlabel('Feature Index')\n",
    "axes[1, 1].set_ylabel('Correlation')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55a2b",
   "metadata": {},
   "source": [
    "## 2. Enhanced Preprocessing with Automatic Scaling\n",
    "\n",
    "We'll use the same preprocessing pipeline that handles normalization and denormalization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets with automatic scaling (enhanced preprocessing)\n",
    "print(\"Preparing datasets with automatic scaling...\")\n",
    "\n",
    "# Use enhanced preprocessing with automatic scaling\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.15,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "\n",
    "train_cat, train_cont, train_targets = train_dataset.tensors\n",
    "val_cat, val_cont, val_targets = val_dataset.tensors\n",
    "test_cat, test_cont, test_targets = test_dataset.tensors\n",
    "\n",
    "print(f\"Dataset sizes: {len(train_dataset)}, {len(val_dataset)}, {len(test_dataset)}\")\n",
    "\n",
    "# Print scaler information\n",
    "scaler_info = preprocessor.get_scaler_info()\n",
    "print(f\"Target scaler - Mean: {scaler_info['target_mean'][0]:.2f}, Std: {scaler_info['target_scale'][0]:.2f}\")\n",
    "\n",
    "# Get data dimensions\n",
    "T, N, F = cont_features.shape\n",
    "print(f\"Original data shape: {cont_features.shape}\")\n",
    "print(f\"Training time steps: {len(train_dataset)}\")\n",
    "print(f\"Validation time steps: {len(val_dataset)}\")\n",
    "print(f\"Test time steps: {len(test_dataset)}\")\n",
    "print(f\"Number of locations: {N}\")\n",
    "print(f\"Number of features: {F}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of standardization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Original vs standardized distributions\n",
    "axes[0].hist(targets.flatten(), bins=30, alpha=0.7, label='Original', density=True)\n",
    "axes[0].hist(train_targets.numpy().flatten(), bins=30, alpha=0.7, label='Standardized', density=True)\n",
    "axes[0].set_title('Target Distribution Comparison')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Original vs standardized spatial patterns\n",
    "val_targets_orig = denormalize_predictions(val_targets.numpy(), preprocessor)\n",
    "axes[1].plot(locations, val_targets_orig[0], 'o-', label='Original', alpha=0.7, linewidth=2)\n",
    "axes[1].plot(locations, val_targets[0].numpy(), 's-', label='Standardized', alpha=0.7, linewidth=2)\n",
    "axes[1].set_title('Spatial Pattern Comparison')\n",
    "axes[1].set_xlabel('Location')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4695b37",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for TFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for TFT model\n",
    "print(\"Preparing data for TFT model...\")\n",
    "\n",
    "# Use the SCALED data from preprocessing (not raw data) - this is the key fix!\n",
    "# Combine training and validation data for TFT (since TFT works on full time series)\n",
    "train_cont_np = train_cont.numpy().astype(np.float32)\n",
    "train_targets_np = train_targets.numpy().astype(np.float32)\n",
    "val_cont_np = val_cont.numpy().astype(np.float32)\n",
    "val_targets_np = val_targets.numpy().astype(np.float32)\n",
    "test_cont_np = test_cont.numpy().astype(np.float32)\n",
    "test_targets_np = test_targets.numpy().astype(np.float32)\n",
    "\n",
    "# Combine all scaled data for TFT (TFT works on full time series)\n",
    "cont_np = np.concatenate([train_cont_np, val_cont_np, test_cont_np], axis=0)\n",
    "targets_np = np.concatenate([train_targets_np, val_targets_np, test_targets_np], axis=0)\n",
    "T, N, p = cont_np.shape\n",
    "\n",
    "print(f\"Data shape: T={T}, N={N}, p={p}\")\n",
    "print(f\"Target shape: {targets_np.shape}\")\n",
    "print(f\"Using SCALED data (mean={targets_np.mean():.4f}, std={targets_np.std():.4f})\")\n",
    "\n",
    "# Create time index\n",
    "time_index = pd.date_range(\"2020-01-01\", periods=T, freq=\"D\")\n",
    "\n",
    "# Create multivariate time series for the full TÃ—N target matrix\n",
    "# Each column represents a spatial location\n",
    "target_df = pd.DataFrame(targets_np, index=time_index, \n",
    "                        columns=[f\"loc_{i}\" for i in range(N)])\n",
    "\n",
    "# Create covariates per site and align with target channels\n",
    "# Reshape covariates to (T, NÂ·p) and include spatial information\n",
    "# Reshape locations to (T, N) by repeating for each time step\n",
    "locations_expanded = np.tile(locations, (T, 1))  # Shape: (T, N)\n",
    "cov_full = np.concatenate([cont_np.reshape(T, -1), locations_expanded], axis=1).astype(np.float32)\n",
    "\n",
    "covariate_df = pd.DataFrame(cov_full, index=time_index,\n",
    "                           columns=[f\"cov_{j}_loc_{i}\" for i in range(N) for j in range(p)] + \n",
    "                                   [f\"spatial_loc_{i}\" for i in range(N)])\n",
    "\n",
    "print(f\"Target DataFrame shape: {target_df.shape}\")\n",
    "print(f\"Covariate DataFrame shape: {covariate_df.shape}\")\n",
    "\n",
    "# Create multivariate TimeSeries objects\n",
    "target_ts = TimeSeries.from_dataframe(target_df, fill_missing_dates=True)\n",
    "covariate_ts = TimeSeries.from_dataframe(covariate_df, fill_missing_dates=True)\n",
    "\n",
    "print(f\"Target TimeSeries shape: {target_ts.values().shape}\")\n",
    "print(f\"Covariate TimeSeries shape: {covariate_ts.values().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6456c67",
   "metadata": {},
   "source": [
    "## 4. TFT Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFT model configuration\n",
    "tft_config = {\n",
    "    \"input_chunk_length\": min(64, T // 4),\n",
    "    \"output_chunk_length\": 1,\n",
    "    \"n_epochs\": 100,\n",
    "    \"hidden_size\": 64,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"random_state\": 42,\n",
    "    \"force_reset\": True,\n",
    "    \"add_relative_index\": True,\n",
    "    \"use_static_covariates\": False,\n",
    "}\n",
    "\n",
    "print(\"TFT Configuration:\")\n",
    "for key, value in tft_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6c6f1",
   "metadata": {},
   "source": [
    "## 5. TFT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train TFT model\n",
    "print(\"Creating and training TFT model...\")\n",
    "\n",
    "# Split data for training/validation\n",
    "training_cutoff = int(0.8 * T)\n",
    "print(f\"Training cutoff: {training_cutoff}\")\n",
    "\n",
    "# Create TFT model\n",
    "tft_model = TFTModel(\n",
    "    input_chunk_length=tft_config[\"input_chunk_length\"],\n",
    "    output_chunk_length=tft_config[\"output_chunk_length\"],\n",
    "    n_epochs=tft_config[\"n_epochs\"],\n",
    "    hidden_size=tft_config[\"hidden_size\"],\n",
    "    num_attention_heads=tft_config[\"num_attention_heads\"],\n",
    "    dropout=tft_config[\"dropout\"],\n",
    "    random_state=tft_config[\"random_state\"],\n",
    "    force_reset=tft_config[\"force_reset\"],\n",
    "    add_relative_index=tft_config[\"add_relative_index\"],\n",
    "    use_static_covariates=tft_config[\"use_static_covariates\"],\n",
    "    pl_trainer_kwargs={\n",
    "        \"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"devices\": -1,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"enable_model_summary\": True,\n",
    "        \"enable_checkpointing\": True,\n",
    "        \"logger\": pl.loggers.TensorBoardLogger(\n",
    "            save_dir=\"./logs\", name=\"tft_demo\", version=\"v1\"\n",
    "        ),\n",
    "        \"log_every_n_steps\": 10,\n",
    "        \"max_epochs\": tft_config[\"n_epochs\"],\n",
    "        \"callbacks\": [\n",
    "            pl.callbacks.EarlyStopping(\n",
    "                monitor=\"train_loss\", patience=10, mode=\"min\"\n",
    "            ),\n",
    "            pl.callbacks.ModelCheckpoint(\n",
    "                monitor=\"train_loss\",\n",
    "                dirpath=\"./tft_checkpoints\",\n",
    "                filename=\"tft_demo_best\",\n",
    "                save_top_k=1,\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Train TFT model\n",
    "print(\"Training TFT model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "tft_model.fit(\n",
    "    target_ts[:training_cutoff], \n",
    "    past_covariates=covariate_ts[:training_cutoff],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f}s\")\n",
    "\n",
    "# Save the trained TFT model\n",
    "model_save_path = \"tft_demo_model.pth\"\n",
    "print(f\"Saving TFT model to {model_save_path}\")\n",
    "tft_model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe2818",
   "metadata": {},
   "source": [
    "## 6. TFT Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate TFT model\n",
    "print(\"Evaluating TFT model...\")\n",
    "\n",
    "# The model was trained with output_chunk_length=1, so we need to use that\n",
    "# We can predict multiple steps by using n=1 and doing it iteratively\n",
    "# But for simplicity, let's just predict 1 step ahead for evaluation\n",
    "holdout_length = 1  # Match the trained output_chunk_length\n",
    "prediction_start = T - holdout_length\n",
    "print(f\"Holdout length: {holdout_length}\")\n",
    "print(f\"Prediction start: {prediction_start}\")\n",
    "print(f\"Using output_chunk_length: 1 (as trained)\")\n",
    "\n",
    "try:\n",
    "    # Make prediction using the trained model's output_chunk_length\n",
    "    forecast = tft_model.predict(\n",
    "        n=holdout_length,  # This should equal the trained output_chunk_length\n",
    "        series=target_ts[:prediction_start],  # Use full context up to prediction point\n",
    "        past_covariates=covariate_ts[:prediction_start],  # Only use past covariates\n",
    "    )\n",
    "\n",
    "    print(f\"Forecast shape: {forecast.values().shape}\")\n",
    "\n",
    "    # Extract true values for holdout period\n",
    "    true_values = target_ts[prediction_start:]\n",
    "    print(f\"True values shape: {true_values.values().shape}\")\n",
    "\n",
    "    # Convert to numpy arrays (these are on standardized scale)\n",
    "    pred_values_scaled = forecast.values().flatten()\n",
    "    true_values_scaled = true_values.values().flatten()\n",
    "\n",
    "    # Ensure same length\n",
    "    min_len = min(len(pred_values_scaled), len(true_values_scaled))\n",
    "    pred_values_scaled = pred_values_scaled[:min_len]\n",
    "    true_values_scaled = true_values_scaled[:min_len]\n",
    "    \n",
    "    rmse_std = np.sqrt(mean_squared_error(true_values_scaled, pred_values_scaled))\n",
    "    mae_std = mean_absolute_error(true_values_scaled, pred_values_scaled)\n",
    "    r2_std = r2_score(true_values_scaled, pred_values_scaled)\n",
    "    mse_std = mean_squared_error(true_values_scaled, pred_values_scaled)\n",
    "\n",
    "    print(f\"Standardized metrics: RMSE={rmse_std:.6f}, MAE={mae_std:.6f}, RÂ²={r2_std:.6f}\")\n",
    "\n",
    "    # Denormalize predictions for original scale evaluation (following SpatialNeuralAdapter pattern)\n",
    "    y_pred_denorm = denormalize_predictions(pred_values_scaled.reshape(-1, 1), preprocessor)\n",
    "    val_targets_denorm = denormalize_predictions(true_values_scaled.reshape(-1, 1), preprocessor)\n",
    "    \n",
    "    # Compute metrics on original scale (following SpatialNeuralAdapter pattern)\n",
    "    rmse_denorm = np.sqrt(np.mean((val_targets_denorm - y_pred_denorm) ** 2))\n",
    "    mae_denorm = np.mean(np.abs(val_targets_denorm - y_pred_denorm))\n",
    "    \n",
    "    # R-squared on original scale (following SpatialNeuralAdapter pattern)\n",
    "    ss_res_denorm = np.sum((val_targets_denorm - y_pred_denorm) ** 2)\n",
    "    ss_tot_denorm = np.sum((val_targets_denorm - val_targets_denorm.mean()) ** 2)\n",
    "    r2_denorm = 1 - (ss_res_denorm / ss_tot_denorm)\n",
    "    \n",
    "    mse_denorm = np.mean((val_targets_denorm - y_pred_denorm) ** 2)\n",
    "\n",
    "    print(f\"Denormalized metrics: RMSE={rmse_denorm:.6f}, MAE={mae_denorm:.6f}, RÂ²={r2_denorm:.6f}\")\n",
    "\n",
    "    # Store denormalized values for visualization\n",
    "    pred_values = y_pred_denorm.flatten()\n",
    "    true_values_np = val_targets_denorm.flatten()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in prediction: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Fallback values\n",
    "    rmse_std = np.inf\n",
    "    mae_std = np.inf\n",
    "    r2_std = -np.inf\n",
    "    mse_std = np.inf\n",
    "    rmse_denorm = np.inf\n",
    "    mae_denorm = np.inf\n",
    "    r2_denorm = -np.inf\n",
    "    mse_denorm = np.inf\n",
    "    pred_values = np.array([])\n",
    "    true_values_np = np.array([])\n",
    "\n",
    "metrics = {\n",
    "    \"rmse_std\": rmse_std,\n",
    "    \"mae_std\": mae_std,\n",
    "    \"r2_std\": r2_std,\n",
    "    \"mse_std\": mse_std,\n",
    "    \"rmse_denorm\": rmse_denorm,\n",
    "    \"mae_denorm\": mae_denorm,\n",
    "    \"r2_denorm\": r2_denorm,\n",
    "    \"mse_denorm\": mse_denorm,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964b3e5",
   "metadata": {},
   "source": [
    "## 7. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "if len(pred_values) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Plot 1: Predictions vs Actual scatter plot\n",
    "    axes[0, 0].scatter(pred_values, true_values_np, alpha=0.5, s=20)\n",
    "    axes[0, 0].plot([pred_values.min(), pred_values.max()], \n",
    "                    [pred_values.min(), pred_values.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    axes[0, 0].set_title('TFT Predictions vs Actual Values')\n",
    "    axes[0, 0].set_xlabel('Predicted Values')\n",
    "    axes[0, 0].set_ylabel('Actual Values')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Temporal predictions\n",
    "    time_steps = range(len(pred_values))\n",
    "    axes[0, 1].plot(time_steps, pred_values, 'b-', alpha=0.7, label='Predicted', linewidth=2)\n",
    "    axes[0, 1].plot(time_steps, true_values_np, 'r-', alpha=0.7, label='True', linewidth=2)\n",
    "    axes[0, 1].set_title('Temporal Predictions')\n",
    "    axes[0, 1].set_xlabel('Time Step')\n",
    "    axes[0, 1].set_ylabel('Value')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Residuals analysis\n",
    "    residuals = true_values_np - pred_values\n",
    "    axes[1, 0].scatter(pred_values, residuals, alpha=0.5, s=20)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.7)\n",
    "    axes[1, 0].set_title('Residuals vs Predicted Values')\n",
    "    axes[1, 0].set_xlabel('Predicted Values')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Residuals distribution\n",
    "    axes[1, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_title('Residuals Distribution')\n",
    "    axes[1, 1].set_xlabel('Residual Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No predictions available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74924cf7",
   "metadata": {},
   "source": [
    "## 8. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec13ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 50)\n",
    "print(\"TFT MODEL DEMO SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Training time: {training_time:.2f}s\")\n",
    "print(f\"Final RMSE (scaled): {metrics['rmse_std']:.6f}\")\n",
    "print(f\"Final MAE (scaled): {metrics['mae_std']:.6f}\")\n",
    "print(f\"Final RÂ² (scaled): {metrics['r2_std']:.6f}\")\n",
    "print(f\"Final MSE (scaled): {metrics['mse_std']:.6f}\")\n",
    "print(f\"Final RMSE (denorm): {metrics['rmse_denorm']:.6f}\")\n",
    "print(f\"Final MAE (denorm): {metrics['mae_denorm']:.6f}\")\n",
    "print(f\"Final RÂ² (denorm): {metrics['r2_denorm']:.6f}\")\n",
    "print(f\"Final MSE (denorm): {metrics['mse_denorm']:.6f}\")\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(f\"Tensorboard logs: ./logs/tft_demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if len(pred_values) > 0:\n",
    "    print(\"\\nPerformance Analysis:\")\n",
    "    print(f\"Scale recovery: {pred_values.max() - pred_values.min():.2f} / {true_values_np.max() - true_values_np.min():.2f} = {(pred_values.max() - pred_values.min()) / (true_values_np.max() - true_values_np.min()):.2%}\")\n",
    "    print(f\"Std recovery: {pred_values.std():.2f} / {true_values_np.std():.2f} = {pred_values.std() / true_values_np.std():.2%}\")\n",
    "\n",
    "print(\"\\nâœ… TFT model demo completed successfully!\")\n",
    "print(\"ðŸ’¡ Run 'tensorboard --logdir ./logs/tft_demo' to view training progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942d220",
   "metadata": {},
   "source": [
    "## 9. Additional Analysis: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db32d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"Feature Importance Analysis:\")\n",
    "\n",
    "# Calculate feature-target correlations\n",
    "feature_correlations = []\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    feature_correlations.append(corr)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Feature correlations\n",
    "axes[0].bar(range(len(feature_correlations)), feature_correlations, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Feature-Target Correlations')\n",
    "axes[0].set_xlabel('Feature Index')\n",
    "axes[0].set_ylabel('Correlation')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance ranking\n",
    "feature_importance = np.abs(feature_correlations)\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "axes[1].bar(range(len(sorted_indices)), feature_importance[sorted_indices], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Feature Importance Ranking')\n",
    "axes[1].set_xlabel('Feature Rank')\n",
    "axes[1].set_ylabel('Absolute Correlation')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for i, idx in enumerate(sorted_indices[:5]):\n",
    "    print(f\"  {i+1}. Feature {idx}: {feature_correlations[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
