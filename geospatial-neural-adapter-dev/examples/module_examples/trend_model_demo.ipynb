{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrendModel Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, Any\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling, denormalize_predictions\n",
    "from geospatial_neural_adapter.models.trend_model import TrendModel, train_trend_model\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea038889",
   "metadata": {},
   "source": [
    "## 1. Data Generation with Meaningful Correlations\n",
    "\n",
    "We'll use the improved data generator that creates features with strong correlations to targets, enabling proper scale learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with meaningful correlations\n",
    "print(\"Generating correlated synthetic data...\")\n",
    "\n",
    "n_locations = 50\n",
    "n_time_steps = 200\n",
    "locations = np.linspace(-5, 5, n_locations)\n",
    "\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locations,\n",
    "    n_time_steps=n_time_steps,\n",
    "    noise_std=1.0,\n",
    "    eigenvalue=2.0,\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Data shapes: {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature-target correlations\n",
    "print(\"Feature-Target Correlations:\")\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    print(f\"  Feature {i}: {corr:.4f}\")\n",
    "\n",
    "# Visualize data characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Target distribution\n",
    "axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Target Distribution')\n",
    "axes[0, 0].set_xlabel('Target Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Spatial pattern at first time step\n",
    "axes[0, 1].plot(locations, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "axes[0, 1].set_xlabel('Location')\n",
    "axes[0, 1].set_ylabel('Target Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Temporal pattern at middle location\n",
    "time_steps = np.arange(len(targets))\n",
    "axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "axes[1, 0].set_xlabel('Time Step')\n",
    "axes[1, 0].set_ylabel('Target Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature correlations\n",
    "feature_corrs = []\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    feature_corrs.append(corr)\n",
    "\n",
    "axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Feature-Target Correlations')\n",
    "axes[1, 1].set_xlabel('Feature Index')\n",
    "axes[1, 1].set_ylabel('Correlation')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Preprocessing with Automatic Scaling\n",
    "\n",
    "We'll use the improved preprocessing pipeline that handles normalization and denormalization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets with automatic scaling\n",
    "print(\"Preparing datasets with automatic scaling...\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "\n",
    "train_cat, train_cont, train_targets = train_dataset.tensors\n",
    "val_cat, val_cont, val_targets = val_dataset.tensors\n",
    "test_cat, test_cont, test_targets = test_dataset.tensors\n",
    "\n",
    "print(f\"Dataset sizes: {len(train_dataset)}, {len(val_dataset)}, {len(test_dataset)}\")\n",
    "\n",
    "# Print scaler information\n",
    "scaler_info = preprocessor.get_scaler_info()\n",
    "print(f\"Target scaler - Mean: {scaler_info['target_mean'][0]:.2f}, Std: {scaler_info['target_scale'][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of standardization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Original vs standardized distributions\n",
    "axes[0].hist(targets.flatten(), bins=30, alpha=0.7, label='Original', density=True)\n",
    "axes[0].hist(train_targets.numpy().flatten(), bins=30, alpha=0.7, label='Standardized', density=True)\n",
    "axes[0].set_title('Target Distribution Comparison')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Original vs standardized spatial patterns\n",
    "val_targets_orig = denormalize_predictions(val_targets.numpy(), preprocessor)\n",
    "axes[1].plot(locations, val_targets_orig[0], 'o-', label='Original', alpha=0.7, linewidth=2)\n",
    "axes[1].plot(locations, val_targets[0].numpy(), 's-', label='Standardized', alpha=0.7, linewidth=2)\n",
    "axes[1].set_title('Spatial Pattern Comparison')\n",
    "axes[1].set_xlabel('Location')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TrendModel Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24945bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "num_continuous_features = cont_features.shape[-1]\n",
    "hidden_layer_sizes = [128, 64, 32]\n",
    "\n",
    "model = TrendModel(\n",
    "    n_locations=n_locations,\n",
    "    num_continuous_features=num_continuous_features,\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Model initialized on device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function with Modern Optimization\n",
    "\n",
    "We'll implement a training function with modern optimization techniques for better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training Improved TrendModel...\")\n",
    "trained_model, train_losses, val_losses = train_trend_model(\n",
    "    model=model,\n",
    "    train_cont=train_cont,\n",
    "    train_targets=train_targets,\n",
    "    val_cont=val_cont,\n",
    "    val_targets=val_targets,\n",
    "    num_epochs=100,\n",
    "    learning_rate=1e-3,\n",
    "    device=device,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "print(f\"Training completed! Final train loss: {train_losses[-1]:.4f}, Final val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training and validation loss\n",
    "axes[0].plot(train_losses, 'b-', linewidth=2, label='Training Loss')\n",
    "axes[0].plot(val_losses, 'r-', linewidth=2, label='Validation Loss')\n",
    "axes[0].set_title('Training and Validation Loss Over Epochs')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Loss difference (overfitting check)\n",
    "loss_diff = np.array(train_losses) - np.array(val_losses)\n",
    "axes[1].plot(loss_diff, 'g-', linewidth=2)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "axes[1].set_title('Training - Validation Loss Difference')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss Difference')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation with Denormalization\n",
    "\n",
    "We'll evaluate the model and demonstrate proper denormalization of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_denorm(\n",
    "    model: nn.Module, \n",
    "    cont_data: torch.Tensor, \n",
    "    targets: torch.Tensor, \n",
    "    preprocessor,\n",
    "    device: str\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate model and return both standardized and denormalized predictions and metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cont_data = cont_data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Get standardized predictions\n",
    "        preds_std = model(cont_data)\n",
    "        \n",
    "        # Calculate metrics on standardized scale\n",
    "        mse_std = torch.nn.functional.mse_loss(preds_std, targets)\n",
    "        mae_std = torch.nn.functional.l1_loss(preds_std, targets)\n",
    "        \n",
    "        # R-squared calculation on standardized scale\n",
    "        ss_res = torch.sum((targets - preds_std) ** 2)\n",
    "        ss_tot = torch.sum((targets - targets.mean()) ** 2)\n",
    "        r2_std = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        # Denormalize predictions using the preprocessor\n",
    "        preds_denorm = denormalize_predictions(preds_std.cpu().numpy(), preprocessor)\n",
    "        targets_denorm = denormalize_predictions(targets.cpu().numpy(), preprocessor)\n",
    "        \n",
    "        # Calculate metrics on original scale\n",
    "        mse_denorm = np.mean((targets_denorm - preds_denorm) ** 2)\n",
    "        mae_denorm = np.mean(np.abs(targets_denorm - preds_denorm))\n",
    "        \n",
    "        # R-squared on original scale\n",
    "        ss_res_denorm = np.sum((targets_denorm - preds_denorm) ** 2)\n",
    "        ss_tot_denorm = np.sum((targets_denorm - targets_denorm.mean()) ** 2)\n",
    "        r2_denorm = 1 - (ss_res_denorm / ss_tot_denorm)\n",
    "        \n",
    "    return preds_std.cpu().numpy(), preds_denorm, {\n",
    "        'MSE_std': mse_std.item(),\n",
    "        'MAE_std': mae_std.item(),\n",
    "        'R2_std': r2_std.item(),\n",
    "        'MSE_denorm': mse_denorm,\n",
    "        'MAE_denorm': mae_denorm,\n",
    "        'R2_denorm': r2_denorm\n",
    "    }\n",
    "\n",
    "# Evaluate model on validation and test sets\n",
    "print(\"Evaluating model performance...\")\n",
    "\n",
    "val_preds_std, val_preds_denorm, val_metrics = evaluate_model_with_denorm(\n",
    "    trained_model, val_cont, val_targets, preprocessor, device\n",
    ")\n",
    "\n",
    "test_preds_std, test_preds_denorm, test_metrics = evaluate_model_with_denorm(\n",
    "    trained_model, test_cont, test_targets, preprocessor, device\n",
    ")\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "print(f\"  Standardized - MSE: {val_metrics['MSE_std']:.4f}, R²: {val_metrics['R2_std']:.4f}\")\n",
    "print(f\"  Denormalized - MSE: {val_metrics['MSE_denorm']:.4f}, R²: {val_metrics['R2_denorm']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"  Standardized - MSE: {test_metrics['MSE_std']:.4f}, R²: {test_metrics['R2_std']:.4f}\")\n",
    "print(f\"  Denormalized - MSE: {test_metrics['MSE_denorm']:.4f}, R²: {test_metrics['R2_denorm']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Results Visualization\n",
    "\n",
    "Let's create comprehensive visualizations to demonstrate the scale detection improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Spatial pattern comparison\n",
    "time_idx = 0\n",
    "val_targets_orig = denormalize_predictions(val_targets.numpy(), preprocessor)\n",
    "axes[0, 0].plot(locations, val_targets_orig[time_idx], 'o-', label='Actual', alpha=0.7, linewidth=2, markersize=4)\n",
    "axes[0, 0].plot(locations, val_preds_denorm[time_idx], 's-', label='Predicted', alpha=0.7, linewidth=2, markersize=4)\n",
    "axes[0, 0].set_title('Validation: Spatial Pattern (Original Scale)')\n",
    "axes[0, 0].set_xlabel('Location')\n",
    "axes[0, 0].set_ylabel('Target Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predictions vs Actual scatter plot\n",
    "all_targets_orig = np.concatenate([val_targets_orig.flatten(), \n",
    "                                 denormalize_predictions(test_targets.numpy(), preprocessor).flatten()])\n",
    "all_preds_denorm = np.concatenate([val_preds_denorm.flatten(), test_preds_denorm.flatten()])\n",
    "\n",
    "axes[0, 1].scatter(all_targets_orig, all_preds_denorm, alpha=0.5, s=20)\n",
    "axes[0, 1].plot([all_targets_orig.min(), all_targets_orig.max()], \n",
    "                [all_targets_orig.min(), all_targets_orig.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_title('Predictions vs Actual Values (Original Scale)')\n",
    "axes[0, 1].set_xlabel('Actual Values')\n",
    "axes[1, 0].set_ylabel('Predicted Values')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Temporal pattern\n",
    "loc_idx = 25\n",
    "test_targets_orig = denormalize_predictions(test_targets.numpy(), preprocessor)\n",
    "time_steps = np.arange(len(test_targets_orig))\n",
    "axes[1, 0].plot(time_steps, test_targets_orig[:, loc_idx], 'b-', label='Actual', linewidth=2)\n",
    "axes[1, 0].plot(time_steps, test_preds_denorm[:, loc_idx], 'r-', label='Predicted', linewidth=2)\n",
    "axes[1, 0].set_title(f'Temporal Pattern at Location {loc_idx} (Original Scale)')\n",
    "axes[1, 0].set_xlabel('Time Step')\n",
    "axes[1, 0].set_ylabel('Target Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Residuals analysis\n",
    "residuals = all_targets_orig - all_preds_denorm\n",
    "axes[1, 1].scatter(all_preds_denorm, residuals, alpha=0.5, s=20)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].set_title('Residuals vs Predicted Values')\n",
    "axes[1, 1].set_xlabel('Predicted Values')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Scale Analysis and Summary\n",
    "\n",
    "Let's analyze the scale recovery and provide a comprehensive summary of the improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale analysis\n",
    "print(\"=== Scale Analysis ===\")\n",
    "print(f\"Original target range: {targets.min():.2f} to {targets.max():.2f}\")\n",
    "print(f\"Predicted range: {all_preds_denorm.min():.2f} to {all_preds_denorm.max():.2f}\")\n",
    "print(f\"Scale recovery: {all_preds_denorm.max() - all_preds_denorm.min():.2f} / {targets.max() - targets.min():.2f} = {(all_preds_denorm.max() - all_preds_denorm.min()) / (targets.max() - targets.min()):.2%}\")\n",
    "\n",
    "print(f\"\\nOriginal std: {targets.std():.2f}\")\n",
    "print(f\"Predicted std: {all_preds_denorm.std():.2f}\")\n",
    "print(f\"Std recovery: {all_preds_denorm.std() / targets.std():.2%}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\n=== Feature Importance ===\")\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    print(f\"Feature {i} correlation: {corr:.4f}\")\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\n=== Model Performance Summary ===\")\n",
    "print(f\"Validation R²: {val_metrics['R2_denorm']:.4f}\")\n",
    "print(f\"Test R²: {test_metrics['R2_denorm']:.4f}\")\n",
    "print(f\"Validation MAE: {val_metrics['MAE_denorm']:.4f}\")\n",
    "print(f\"Test MAE: {test_metrics['MAE_denorm']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution comparison\n",
    "axes[0].hist(targets.flatten(), bins=30, alpha=0.7, label='Original', density=True, edgecolor='black')\n",
    "axes[0].hist(all_preds_denorm, bins=30, alpha=0.7, label='Predicted', density=True, edgecolor='black')\n",
    "axes[0].set_title('Distribution Comparison')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scale recovery visualization\n",
    "metrics = ['Scale Recovery', 'Std Recovery', 'R² Score']\n",
    "values = [\n",
    "    (all_preds_denorm.max() - all_preds_denorm.min()) / (targets.max() - targets.min()),\n",
    "    all_preds_denorm.std() / targets.std(),\n",
    "    test_metrics['R2_denorm']\n",
    "]\n",
    "values = [v * 100 for v in values]  # Convert to percentage\n",
    "\n",
    "bars = axes[1].bar(metrics, values, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Performance Metrics')\n",
    "axes[1].set_ylabel('Percentage (%)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{value:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
