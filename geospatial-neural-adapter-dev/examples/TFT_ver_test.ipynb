{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS vs. Spatial Adapter Comparison with Tuning Parameter Selection\n",
    "\n",
    "This notebook implements a comprehensive comparison between:\n",
    "1. **TFT** - Linear baseline (no spatial term)\n",
    "2. **Unregularized Spatial Adapter** - Neural spatial model without regularization\n",
    "3. **Regularized Spatial Adapter** - Neural spatial model with optimized tau1, tau2 parameters\n",
    "\n",
    "The experiment uses Optuna for hyperparameter optimization and evaluates performance across multiple random seeds.\n",
    "\n",
    "my work: ols 換成 TFT 然後執行模擬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345997df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "# ==== New: Darts / TFT as the backbone (replaces OLS) ====\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "\n",
    "from geospatial_neural_adapter.cpp_extensions import estimate_covariance\n",
    "from geospatial_neural_adapter.utils.experiment import log_covariance_and_basis\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    ModelCache,\n",
    "    clear_gpu_memory,\n",
    "    create_experiment_config,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "from geospatial_neural_adapter.models.spatial_basis_learner import SpatialBasisLearner\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapter\n",
    "from geospatial_neural_adapter.models.trend_model import TrendModel\n",
    "from geospatial_neural_adapter.models.wrapper_examples.tft_wrapper import TFTWrapper\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import (\n",
    "    prepare_all_with_scaling,\n",
    "    denormalize_predictions,\n",
    ")\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Imports successful (TFT backbone enabled; all OLS utilities removed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f86c36",
   "metadata": {},
   "source": [
    "## 1. Parameter Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_time_steps': 1024//2,\n",
    "    'n_locations': 512//2,\n",
    "    'noise_std': 4.0,\n",
    "    'eigenvalue': 16.0,\n",
    "    'latent_dim': 1,\n",
    "    'ckpt_dir': \"admm_bcd_ckpts\",\n",
    "}\n",
    "\n",
    "# ==== New: Split & TFT configs (baseline uses TFT, no spatial term) ====\n",
    "SPLIT_CONFIG = {\n",
    "    \"train_ratio\": 0.70,\n",
    "    \"val_ratio\": 0.15,   # 剩餘即為 test_ratio\n",
    "}\n",
    "\n",
    "TFT_CONFIG = {\n",
    "    \"input_chunk_length\": 48,\n",
    "    \"output_chunk_length\": 1,\n",
    "    \"hidden_size\": 64,\n",
    "    \"lstm_layers\": 1,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"dropout\": 0.10,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 10,  #30\n",
    "    \"random_state\": 42,\n",
    "    \"add_relative_index\": True,\n",
    "}\n",
    "\n",
    "PL_TRAINER_KWARGS = (\n",
    "    {\"accelerator\": \"gpu\", \"devices\": 1,\n",
    "     \"logger\": True,                 # ← 開啟 logger\n",
    "     \"enable_progress_bar\": True,   # ← 繼續關閉進度條\n",
    "     \"enable_model_summary\": False,\n",
    "     \"num_sanity_val_steps\": 0}\n",
    "    if torch.cuda.is_available()\n",
    "    else {\"accelerator\": \"cpu\", \"devices\": 1,\n",
    "          \"logger\": True,\n",
    "          \"enable_progress_bar\": True,\n",
    "          \"enable_model_summary\": False,\n",
    "          \"num_sanity_val_steps\": 0}\n",
    ")\n",
    "\n",
    "# Spatial Neural Adapter Configuration using dataclasses\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import (\n",
    "    SpatialNeuralAdapterConfig, ADMMConfig, TrainingConfig, BasisConfig\n",
    ")\n",
    "\n",
    "# ADMM Configuration\n",
    "admm_config = ADMMConfig(\n",
    "    rho=1.0,            # Base ADMM penalty parameter\n",
    "    dual_momentum=0.2,  # Dual variable momentum\n",
    "    max_iters=3000,     # Maximum ADMM iterations\n",
    "    min_outer=20,       # Minimum outer iterations before convergence check\n",
    "    tol=1e-4,           # Convergence tolerance\n",
    ")\n",
    "\n",
    "# Training Configuration\n",
    "# 提醒：這裡的 lr_mu 原先是給 OLS/線性趨勢用；改為 TFT 後，趨勢學習改由 TFT 本身處理。\n",
    "# 若你的 SpatialNeuralAdapter 內部仍參考該欄位，先保留以維持相容；實際學習率以 TFT_CONFIG 為準。\n",
    "training_config = TrainingConfig(\n",
    "    lr_mu=1e-2,           # 保留以維持相容，實際趨勢由 TFT 學\n",
    "    batch_size=64,        # Adapter theta step 批次\n",
    "    pretrain_epochs=5,    # Default pretraining epochs（若流程用得到）\n",
    "    use_mixed_precision=False,\n",
    ")\n",
    "\n",
    "# Basis Configuration\n",
    "basis_config = BasisConfig(\n",
    "    phi_every=5,        # Update basis every N iterations\n",
    "    phi_freeze=200,     # Stop updating basis after N iterations\n",
    "    matrix_reg=1e-6,    # Matrix regularization for basis update\n",
    "    irl1_max_iters=10,  # IRL₁ maximum iterations\n",
    "    irl1_eps=1e-6,      # IRL₁ epsilon\n",
    "    irl1_tol=5e-4,      # IRL₁ inner tolerance\n",
    ")\n",
    "\n",
    "# Complete Spatial Neural Adapter Configuration\n",
    "SPATIAL_CONFIG = SpatialNeuralAdapterConfig(\n",
    "    admm=admm_config,\n",
    "    training=training_config,\n",
    "    basis=basis_config\n",
    ")\n",
    "\n",
    "# Legacy config dict for backward compatibility (if needed)\n",
    "CFG = SPATIAL_CONFIG.to_dict()\n",
    "CFG.update(EXPERIMENT_CONFIG)\n",
    "CFG.update({\n",
    "    \"split\": SPLIT_CONFIG,\n",
    "    \"tft\": TFT_CONFIG,\n",
    "})\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "torch.manual_seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "Path(EXPERIMENT_CONFIG[\"ckpt_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info['device'] == 'cuda':\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Experiment Configuration ===\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Split Configuration ===\")\n",
    "for k, v in SPLIT_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== TFT Configuration (Baseline) ===\")\n",
    "for k, v in TFT_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Spatial Neural Adapter Configuration ===\")\n",
    "SPATIAL_CONFIG.log_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a32565",
   "metadata": {},
   "source": [
    "## 2. Initialize Utilities\n",
    " 目前減少跑的次數 確認完之後要跑完整的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model cache for hyperparameter optimization\n",
    "cache = ModelCache()\n",
    "\n",
    "# Create experiment configuration\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=20 if torch.cuda.is_available() else 50,\n",
    "    n_dataset_seeds=10,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=11,\n",
    ")\n",
    "\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "print(\"Utilities initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777490a",
   "metadata": {},
   "source": [
    "## 3. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19da385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with meaningful correlations\n",
    "print(\"Generating correlated synthetic data...\")\n",
    "\n",
    "locs = np.linspace(-3, 3, CFG[\"n_locations\"])\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locs,\n",
    "    n_time_steps=CFG[\"n_time_steps\"],\n",
    "    noise_std=CFG[\"noise_std\"],\n",
    "    eigenvalue=CFG[\"eigenvalue\"],\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=CFG[\"seed\"]\n",
    ")\n",
    "\n",
    "# Prepare datasets with scaling\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=SPLIT_CONFIG[\"train_ratio\"],\n",
    "    val_ratio=SPLIT_CONFIG[\"val_ratio\"],\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "\n",
    "# （保留）給 Adapter 內部 batch 可能會用到\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG[\"tft\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Extract tensors (scaled)\n",
    "_, train_X, train_y = train_dataset.tensors\n",
    "_, val_X,   val_y   = val_dataset.tensors\n",
    "_, test_X,  test_y  = test_dataset.tensors\n",
    "\n",
    "if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "\n",
    "# ===== New: 將經過 scaling 的 y 串回完整時間，供 Darts/TFT 使用 =====\n",
    "# 原本 OLS 會用這些張量直接做回歸；現在改成組成多變量 TimeSeries\n",
    "y_all_scaled = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T_full, N)\n",
    "x_all_scaled = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T_full, N, F)\n",
    "\n",
    "T_full = y_all_scaled.shape[0]\n",
    "N      = y_all_scaled.shape[1]\n",
    "F      = x_all_scaled.shape[2]\n",
    "\n",
    "# 依時間切分索引（與上面的 ratio 一致）\n",
    "train_T = int(T_full * SPLIT_CONFIG[\"train_ratio\"])\n",
    "val_T   = int(T_full * (SPLIT_CONFIG[\"train_ratio\"] + SPLIT_CONFIG[\"val_ratio\"]))\n",
    "test_T  = T_full\n",
    "\n",
    "# 建立 Darts 多變量 TimeSeries（以 y 的歷史作為唯一輸入；若要加入 covariates 之後再接）\n",
    "series_all   = TimeSeries.from_values(y_all_scaled)      # shape (T_full, N)\n",
    "series_train = series_all[:train_T]\n",
    "series_val   = series_all[train_T:val_T]\n",
    "series_test  = series_all[val_T:]\n",
    "\n",
    "# 保留給後續評估對齊\n",
    "y_true_future = y_all_scaled[train_T:]   # (T_val+T_test, N)\n",
    "\n",
    "#（保留）原本你就會印的資訊\n",
    "p_dim = train_X.shape[-1]\n",
    "print(f\"Data shapes (cont_features, targets): {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")\n",
    "print(f\"Feature dimension: {p_dim}\")\n",
    "print(f\"T_full={T_full}, N={N}, F={F} | splits: train={train_T}, val={val_T-train_T}, test={test_T-val_T}\")\n",
    "print(\"Scaled series prepared for TFT (Darts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize data characteristics\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Plot 1: Target distribution\n",
    "# axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "# axes[0, 0].set_title('Target Distribution')\n",
    "# axes[0, 0].set_xlabel('Target Value')\n",
    "# axes[0, 0].set_ylabel('Frequency')\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 2: Spatial pattern at first time step\n",
    "# axes[0, 1].plot(locs, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "# axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "# axes[0, 1].set_xlabel('Location')\n",
    "# axes[0, 1].set_ylabel('Target Value')\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 3: Temporal pattern at middle location\n",
    "# time_steps = np.arange(len(targets))\n",
    "# axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "# axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "# axes[1, 0].set_xlabel('Time Step')\n",
    "# axes[1, 0].set_ylabel('Target Value')\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 4: Feature correlations\n",
    "# feature_corrs = []\n",
    "# for i in range(cont_features.shape[-1]):\n",
    "#     corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "#     feature_corrs.append(corr)\n",
    "\n",
    "# axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "# axes[1, 1].set_title('Feature-Target Correlations')\n",
    "# axes[1, 1].set_xlabel('Feature Index')\n",
    "# axes[1, 1].set_ylabel('Correlation')\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fb6b7",
   "metadata": {},
   "source": [
    "## 4. Baseline Implementation\n",
    " OLS 改成 TFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== TFT Baseline (demo style; no spatial term, no future covariates) ====\n",
    "# print(\"Training TFT baseline (demo-only, no spatial)...\")\n",
    "\n",
    "# pl.seed_everything(CFG[\"tft\"][\"random_state\"])\n",
    "\n",
    "# # 1) Per-location target & past covariates（不建 future covariates）\n",
    "# series_list, past_cov_list = [], []\n",
    "# for i in range(N):\n",
    "#     s_i   = TimeSeries.from_values(y_all_scaled[:, i])       # target (T,) -> TS\n",
    "#     cov_i = TimeSeries.from_values(x_all_scaled[:, i, :])    # cont features (T, F) -> TS\n",
    "#     series_list.append(s_i)\n",
    "#     past_cov_list.append(cov_i)\n",
    "\n",
    "# # 2) Split（train/val 對齊）\n",
    "# series_train_list = [s[:train_T]      for s in series_list]\n",
    "# series_val_list   = [s[train_T:val_T] for s in series_list]\n",
    "# past_cov_train    = [c[:train_T]      for c in past_cov_list]\n",
    "# val_past_covs     = [c[train_T:val_T] for c in past_cov_list]\n",
    "\n",
    "# # 3) Train TFT（只傳 past；驗證也傳 val_past_covariates）\n",
    "# tft = TFTModel(\n",
    "#     **TFT_CONFIG,                      # 需含 add_relative_index=True 或等價 encoders\n",
    "#     pl_trainer_kwargs=PL_TRAINER_KWARGS\n",
    "# )\n",
    "# tft.fit(\n",
    "#     series=series_train_list,\n",
    "#     val_series=series_val_list,\n",
    "#     past_covariates=past_cov_train,\n",
    "#     val_past_covariates=val_past_covs,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # 4) One-step rolling forecast via historical_forecasts（只用 past_covariates）\n",
    "# val_len  = val_y.shape[0]\n",
    "# test_len = test_y.shape[0]\n",
    "\n",
    "# yhat_val_list, yhat_test_list = [], []\n",
    "# for i in range(N):\n",
    "#     yhat_i = tft.historical_forecasts(\n",
    "#         series=series_list[i],\n",
    "#         past_covariates=past_cov_list[i],\n",
    "#         start=train_T,\n",
    "#         forecast_horizon=1,\n",
    "#         retrain=False,\n",
    "#         verbose=False\n",
    "#     ).values()  # (Tval+Ttest,)\n",
    "#     yhat_val_list.append(yhat_i[:val_len])\n",
    "#     yhat_test_list.append(yhat_i[val_len:val_len + test_len])\n",
    "\n",
    "# # 回到 (T, N)（scaled space）\n",
    "# yhat_val_sc  = np.stack(yhat_val_list,  axis=1)\n",
    "# yhat_test_sc = np.stack(yhat_test_list, axis=1)\n",
    "\n",
    "# # 5) 指標在原始尺度計算（denorm → torch.Tensor 2D）\n",
    "# def to_tensor_2d(x):\n",
    "#     if isinstance(x, np.ndarray): x = torch.from_numpy(x)\n",
    "#     if x.ndim == 3: x = x.squeeze(-1)\n",
    "#     return x.float()\n",
    "\n",
    "# y_val_den_t     = to_tensor_2d(denormalize_predictions(val_y.squeeze(-1),  preprocessor))               # (Tval, N)\n",
    "# y_test_den_t    = to_tensor_2d(denormalize_predictions(test_y.squeeze(-1), preprocessor))               # (Ttest, N)\n",
    "# yhat_val_den_t  = to_tensor_2d(denormalize_predictions(torch.from_numpy(yhat_val_sc).float(),  preprocessor))\n",
    "# yhat_test_den_t = to_tensor_2d(denormalize_predictions(torch.from_numpy(yhat_test_sc).float(), preprocessor))\n",
    "\n",
    "# rmse_tft_val,  mae_tft_val,  r2_tft_val  = compute_metrics(y_val_den_t,  yhat_val_den_t)\n",
    "# rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(y_test_den_t, yhat_test_den_t)\n",
    "\n",
    "# print(f\"TFT Validation - RMSE: {rmse_tft_val:.4f}, R²: {r2_tft_val:.4f}\")\n",
    "# print(f\"TFT Test       - RMSE: {rmse_tft_test:.4f}, R²: {r2_tft_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285ba2",
   "metadata": {},
   "source": [
    "## 5. Main Experiment Function\n",
    "存檔從seed改為TFT_seed, \n",
    " 呼叫 OLS 的函式改成呼叫 TFT 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc94b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mk_series_per_station(\n",
    "    train_y, val_y, test_y, train_X, val_X, test_X, split_cfg\n",
    ") -> Tuple[List[TimeSeries], List[TimeSeries], List[TimeSeries], int, int, int, int, int]:\n",
    "    \"\"\"\n",
    "    將 (T, N, F) / (T, N[,1]) 的張量串回完整時間，轉成每站一條 TimeSeries + covariates（demo 風格）。\n",
    "    回傳：series_list, past_cov_list, future_cov_list, T_tr, T_va, val_len, test_len, N, F\n",
    "    \"\"\"\n",
    "    if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "    if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "    if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "\n",
    "    y_all_scaled = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T_full, N)\n",
    "    x_all_scaled = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T_full, N, F)\n",
    "\n",
    "    T_full = y_all_scaled.shape[0]\n",
    "    N      = y_all_scaled.shape[1]\n",
    "    F      = x_all_scaled.shape[2]\n",
    "\n",
    "    T_tr = int(T_full * split_cfg[\"train_ratio\"])\n",
    "    T_va = int(T_full * (split_cfg[\"train_ratio\"] + split_cfg[\"val_ratio\"]))\n",
    "    val_len  = T_va - T_tr\n",
    "    test_len = T_full - T_va\n",
    "\n",
    "    series_list, past_cov_list, future_cov_list = [], [], []\n",
    "    for i in range(N):\n",
    "        s_i   = TimeSeries.from_values(y_all_scaled[:, i])    # (T,)\n",
    "        cov_i = TimeSeries.from_values(x_all_scaled[:, i, :]) # (T, F)\n",
    "        series_list.append(s_i)\n",
    "        past_cov_list.append(cov_i)\n",
    "        future_cov_list.append(cov_i)  # 保留介面；若不用 future cov 可忽略\n",
    "\n",
    "    return series_list, past_cov_list, future_cov_list, T_tr, T_va, val_len, test_len, N, F\n",
    "\n",
    "\n",
    "def build_tft_and_data(\n",
    "    *,\n",
    "    cat_features: np.ndarray,          # (T, N, ?) 目前未用；保留參數以相容\n",
    "    cont_features: np.ndarray,         # (T, N, F)\n",
    "    targets: np.ndarray,               # (T, N)\n",
    "    split_cfg: Dict[str, Any],         # {\"train_ratio\": 0.7, \"val_ratio\": 0.15}\n",
    "    tft_cfg: Dict[str, Any],           # 要含 add_relative_index=True 或等價 encoders\n",
    "    pl_trainer_kwargs: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    # 1) scaling\n",
    "    train_ds, val_ds, test_ds, preproc = prepare_all_with_scaling(\n",
    "        cat_features=cat_features,\n",
    "        cont_features=cont_features,\n",
    "        targets=targets,\n",
    "        train_ratio=split_cfg[\"train_ratio\"],\n",
    "        val_ratio=split_cfg[\"val_ratio\"],\n",
    "        feature_scaler_type=\"standard\",\n",
    "        target_scaler_type=\"standard\",\n",
    "        fit_on_train_only=True,\n",
    "    )\n",
    "    _, train_X, train_y = train_ds.tensors\n",
    "    _, val_X,   val_y   = val_ds.tensors\n",
    "    _, test_X,  test_y  = test_ds.tensors\n",
    "\n",
    "    # 2) 每站一條 series（demo 風格）— 使用外層工具函式\n",
    "    (series_list, past_cov_list, _future_cov_list,\n",
    "     T_tr, T_va, val_len, test_len, N, F) = _mk_series_per_station(\n",
    "        train_y, val_y, test_y, train_X, val_X, test_X, split_cfg\n",
    "    )\n",
    "\n",
    "    # 3) 訓練 TFT（只用 past cov；驗證也給 val_past_covariates）\n",
    "    pl.seed_everything(tft_cfg.get(\"random_state\", 42))\n",
    "    tft_model = TFTModel(**tft_cfg, pl_trainer_kwargs=pl_trainer_kwargs)\n",
    "    tft_model.fit(\n",
    "        series=[s[:T_tr] for s in series_list],\n",
    "        val_series=[s[T_tr:T_va] for s in series_list],\n",
    "        past_covariates=[c[:T_tr] for c in past_cov_list],\n",
    "        val_past_covariates=[c[T_tr:T_va] for c in past_cov_list],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 4) 歷史一步（val+test）預測（scaled）\n",
    "    yhat_val_list, yhat_test_list = [], []\n",
    "    for i in range(N):\n",
    "        preds = tft_model.historical_forecasts(\n",
    "            series=series_list[i],\n",
    "            past_covariates=past_cov_list[i],\n",
    "            start=T_tr, forecast_horizon=1, retrain=False, verbose=False\n",
    "        ).values()  # (Tval+Ttest,)\n",
    "        yhat_val_list.append(preds[:val_len])\n",
    "        yhat_test_list.append(preds[val_len:val_len+test_len])\n",
    "    yhat_val_sc = np.stack(yhat_val_list, 1)\n",
    "    yhat_test_sc = np.stack(yhat_test_list, 1)\n",
    "\n",
    "    # 5) 反標準化（demo 以原尺度評估）\n",
    "    def _t2d(x):\n",
    "        if isinstance(x, np.ndarray): x = torch.from_numpy(x)\n",
    "        if x.ndim == 3: x = x.squeeze(-1)\n",
    "        return x.float()\n",
    "\n",
    "    val_y_den_t     = _t2d(denormalize_predictions(val_y.squeeze(-1),  preproc))\n",
    "    test_y_den_t    = _t2d(denormalize_predictions(test_y.squeeze(-1), preproc))\n",
    "    yhat_val_den_t  = _t2d(denormalize_predictions(torch.from_numpy(yhat_val_sc).float(),  preproc))\n",
    "    yhat_test_den_t = _t2d(denormalize_predictions(torch.from_numpy(yhat_test_sc).float(), preproc))\n",
    "\n",
    "    return {\n",
    "        \"tft_model\": tft_model,\n",
    "        \"preprocessor\": preproc,\n",
    "        \"split_idx\": {\"T_tr\": T_tr, \"T_va\": T_va, \"val_len\": val_len, \"test_len\": test_len},\n",
    "        \"N\": N, \"F\": F,\n",
    "        \"val_y_den_t\": val_y_den_t, \"test_y_den_t\": test_y_den_t,\n",
    "        \"yhat_val_den_t\": yhat_val_den_t, \"yhat_test_den_t\": yhat_test_den_t,\n",
    "        \"val_y_sc\": val_y.squeeze(-1).cpu().numpy(),\n",
    "        \"test_y_sc\": test_y.squeeze(-1).cpu().numpy(),\n",
    "        \"yhat_val_sc\": yhat_val_sc, \"yhat_test_sc\": yhat_test_sc,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_spatial_adapter_from_demo(\n",
    "    *,\n",
    "    tft_model,\n",
    "    device,\n",
    "    n_locations: int,\n",
    "    latent_dim: int,\n",
    "    num_features: int,          # 必傳給 TFTWrapper\n",
    "    train_loader,\n",
    "    val_cont: torch.Tensor,\n",
    "    val_y: torch.Tensor,\n",
    "    locs,\n",
    "    adapter_cfg,                # ← 參數改名\n",
    "    tau1: float,\n",
    "    tau2: float,\n",
    "    writer=None,\n",
    "):\n",
    "    # 防呆：確保是 dataclass，而不是 tuple/dict\n",
    "    from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapterConfig\n",
    "    assert isinstance(adapter_cfg, SpatialNeuralAdapterConfig), f\"adapter_cfg type error: {type(adapter_cfg)}\"\n",
    "\n",
    "    trend_backbone = TFTWrapper(\n",
    "        tft_model=tft_model,\n",
    "        num_locations=n_locations,\n",
    "        num_features=num_features,\n",
    "    )\n",
    "    basis = SpatialBasisLearner(n_locations, latent_dim).to(device)\n",
    "\n",
    "    adapter = SpatialNeuralAdapter(\n",
    "        trend_backbone, basis,\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_cont.to(device),\n",
    "        val_y=val_y.to(device),\n",
    "        locs=locs,\n",
    "        config=adapter_cfg,   # 這裡用改名後的 adapter_cfg\n",
    "        device=device,\n",
    "        writer=writer,\n",
    "        tau1=tau1,\n",
    "        tau2=tau2,\n",
    "    )\n",
    "    return adapter\n",
    "\n",
    "\n",
    "def train_unregularized_adapter(\n",
    "    adapter: SpatialNeuralAdapter,\n",
    "    pretrain_epochs: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DEMO 風格: tau1=tau2=0 的 adapter 訓練流程。\n",
    "    回傳：{\"adapter\": adapter}\n",
    "    \"\"\"\n",
    "    adapter.pretrain_trend(epochs=pretrain_epochs)\n",
    "    adapter.init_basis_dense()\n",
    "    adapter.run()\n",
    "    return {\"adapter\": adapter}\n",
    "\n",
    "\n",
    "def train_regularized_adapter_with_optuna(\n",
    "    build_adapter_fn,                             # closure: (tau1, tau2) -> SpatialNeuralAdapter\n",
    "    val_y_den_t: torch.Tensor,\n",
    "    predict_val_den_fn,                           # closure: (adapter) -> torch.Tensor denorm predictions on val\n",
    "    n_trials: int = 30,\n",
    "    study_name: str = \"TFT_spatial_adapter_reg\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DEMO 風格：Optuna 搜 tau1、tau2，指標在 **denormalized** 空間計算。\n",
    "    \"\"\"\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "        tau1 = trial.suggest_float(\"tau1\", 1e-4, 1e8, log=True)\n",
    "        tau2 = trial.suggest_float(\"tau2\", 1e-4, 1e8, log=True)\n",
    "        adapter = build_adapter_fn(tau1, tau2)\n",
    "        adapter.pretrain_trend(epochs=3)\n",
    "        adapter.init_basis_dense()\n",
    "        adapter.run()\n",
    "\n",
    "        y_val_pred_den = predict_val_den_fn(adapter)\n",
    "        rmse, mae, r2 = compute_metrics(val_y_den_t, y_val_pred_den)\n",
    "\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\",  r2)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name, direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=MedianPruner(n_warmup_steps=5),\n",
    "        load_if_exists=False,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    best = study.best_trial\n",
    "    return {\n",
    "        \"tau1\": best.params[\"tau1\"],\n",
    "        \"tau2\": best.params[\"tau2\"],\n",
    "        \"rmse\": best.user_attrs[\"rmse\"],\n",
    "        \"mae\":  best.user_attrs[\"mae\"],\n",
    "        \"r2\":   best.user_attrs[\"r2\"],\n",
    "        \"best_trial\": best.number,\n",
    "        \"study\": study,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_adapter_on_test_from_demo(\n",
    "    adapter: SpatialNeuralAdapter,\n",
    "    denorm_true_test: torch.Tensor,\n",
    "    predict_test_den_fn,          # closure: (adapter) -> torch.Tensor denorm predictions on test\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    DEMO 風格：在 test 集上用 adapter 推論，回傳 (rmse, mae, r2)（原尺度）。\n",
    "    \"\"\"\n",
    "    y_test_pred_den = predict_test_den_fn(adapter)\n",
    "    rmse, mae, r2 = compute_metrics(denorm_true_test, y_test_pred_den)\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_experiment(dataset_seed: int, n_trials: int = 30):\n",
    "    log_root = Path(\"TFT_runs\") / f\"TFT_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ----- 1) Data -----\n",
    "    catf, conf, tgts = generate_time_synthetic_data(\n",
    "        locs=locs,\n",
    "        n_time_steps=CFG[\"n_time_steps\"],\n",
    "        noise_std=CFG[\"noise_std\"],\n",
    "        eigenvalue=CFG[\"eigenvalue\"],\n",
    "        eta_rho=0.8,\n",
    "        f_rho=0.6,\n",
    "        global_mean=50.0,\n",
    "        feature_noise_std=0.1,\n",
    "        non_linear_strength=0.2,\n",
    "        seed=dataset_seed,\n",
    "    )\n",
    "    train_ds, val_ds, test_ds, preproc = prepare_all_with_scaling(\n",
    "        cat_features=catf, cont_features=conf, targets=tgts,\n",
    "        train_ratio=SPLIT_CONFIG[\"train_ratio\"], val_ratio=SPLIT_CONFIG[\"val_ratio\"],\n",
    "        feature_scaler_type=\"standard\", target_scaler_type=\"standard\", fit_on_train_only=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds, batch_size=SPATIAL_CONFIG.training.batch_size, shuffle=True)\n",
    "    _, val_X,  val_y  = val_ds.tensors\n",
    "    _, test_X, test_y = test_ds.tensors\n",
    "\n",
    "    # ----- 2) TFT baseline -----\n",
    "    tft_pack = build_tft_and_data(\n",
    "        cat_features=catf, cont_features=conf, targets=tgts,\n",
    "        split_cfg=SPLIT_CONFIG, tft_cfg=TFT_CONFIG, pl_trainer_kwargs=PL_TRAINER_KWARGS,\n",
    "    )\n",
    "    tft_model        = tft_pack[\"tft_model\"]\n",
    "    val_y_den_t      = tft_pack[\"val_y_den_t\"]\n",
    "    test_y_den_t     = tft_pack[\"test_y_den_t\"]\n",
    "    yhat_val_den_t   = tft_pack[\"yhat_val_den_t\"]\n",
    "    yhat_test_den_t  = tft_pack[\"yhat_test_den_t\"]\n",
    "\n",
    "    rmse_tft, mae_tft, r2_tft = compute_metrics(val_y_den_t,  yhat_val_den_t)\n",
    "    rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(test_y_den_t, yhat_test_den_t)\n",
    "\n",
    "    # ----- 3) Unregularized adapter -----\n",
    "    writer_boot = SummaryWriter(log_dir=log_root / \"bootstrap\")\n",
    "    adapter_unreg = build_spatial_adapter_from_demo(\n",
    "        tft_model=tft_model,\n",
    "        device=DEVICE,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "        num_features=val_X.shape[-1],\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_X,\n",
    "        val_y=val_y,\n",
    "        locs=locs,\n",
    "        adapter_cfg=SPATIAL_CONFIG,   # ← 用 adapter_cfg\n",
    "        tau1=0.0, tau2=0.0,\n",
    "        writer=writer_boot,\n",
    "    )\n",
    "    train_unregularized_adapter(adapter_unreg, pretrain_epochs=5)\n",
    "    writer_boot.close()\n",
    "\n",
    "    def predict_val_den_fn(adapter):\n",
    "        y_pred_sc = adapter.predict(val_X.to(DEVICE), val_y.to(DEVICE))\n",
    "        if y_pred_sc.ndim == 3: y_pred_sc = y_pred_sc.squeeze(-1)\n",
    "        y_pred_den = denormalize_predictions(y_pred_sc, preproc)\n",
    "        if isinstance(y_pred_den, np.ndarray): y_pred_den = torch.from_numpy(y_pred_den).float()\n",
    "        return y_pred_den\n",
    "\n",
    "    def predict_test_den_fn(adapter):\n",
    "        y_pred_sc = adapter.predict(test_X.to(DEVICE), test_y.to(DEVICE))\n",
    "        if y_pred_sc.ndim == 3: y_pred_sc = y_pred_sc.squeeze(-1)\n",
    "        y_pred_den = denormalize_predictions(y_pred_sc, preproc)\n",
    "        if isinstance(y_pred_den, np.ndarray): y_pred_den = torch.from_numpy(y_pred_den).float()\n",
    "        return y_pred_den\n",
    "\n",
    "    y_unreg_val_den_t  = predict_val_den_fn(adapter_unreg)\n",
    "    y_unreg_test_den_t = predict_test_den_fn(adapter_unreg)\n",
    "    rmse_unreg, mae_unreg, r2_unreg = compute_metrics(val_y_den_t,  y_unreg_val_den_t)\n",
    "    rmse_unreg_test, mae_unreg_test, r2_unreg_test = compute_metrics(test_y_den_t, y_unreg_test_den_t)\n",
    "\n",
    "    # ----- 4) Regularized (Optuna) -----\n",
    "    def build_adapter_fn(tau1: float, tau2: float):\n",
    "        writer = SummaryWriter(log_dir=log_root / f\"trial_tau1_{tau1:.3g}_tau2_{tau2:.3g}\")\n",
    "        adapter = build_spatial_adapter_from_demo(\n",
    "            tft_model=tft_model,\n",
    "            device=DEVICE,\n",
    "            n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "            latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "            num_features=val_X.shape[-1],\n",
    "            train_loader=train_loader,\n",
    "            val_cont=val_X,\n",
    "            val_y=val_y,\n",
    "            locs=locs,\n",
    "            adapter_cfg=SPATIAL_CONFIG,   # ← 一律 adapter_cfg\n",
    "            tau1=tau1, tau2=tau2,\n",
    "            writer=writer,\n",
    "        )\n",
    "        adapter._tmp_writer = writer\n",
    "        return adapter\n",
    "\n",
    "    def predict_val_den_for_optuna(adapter):\n",
    "        y_pred = predict_val_den_fn(adapter)\n",
    "        if hasattr(adapter, \"_tmp_writer\") and adapter._tmp_writer is not None:\n",
    "            adapter._tmp_writer.close()\n",
    "            adapter._tmp_writer = None\n",
    "        return y_pred\n",
    "\n",
    "    reg_search = train_regularized_adapter_with_optuna(\n",
    "        build_adapter_fn=build_adapter_fn,\n",
    "        val_y_den_t=val_y_den_t,\n",
    "        predict_val_den_fn=predict_val_den_for_optuna,\n",
    "        n_trials=n_trials,\n",
    "        study_name=f\"TFT_spatial_adapter_reg_ds{dataset_seed}\",\n",
    "    )\n",
    "    tau1_opt, tau2_opt = reg_search[\"tau1\"], reg_search[\"tau2\"]\n",
    "    rmse_opt, mae_opt, r2_opt = reg_search[\"rmse\"], reg_search[\"mae\"], reg_search[\"r2\"]\n",
    "    best_no = reg_search[\"best_trial\"]\n",
    "\n",
    "    # Best adapter → retrain → test\n",
    "    writer_best = SummaryWriter(log_dir=log_root / f\"best_tau1_{tau1_opt:.3g}_tau2_{tau2_opt:.3g}\")\n",
    "    adapter_best = build_spatial_adapter_from_demo(\n",
    "        tft_model=tft_model,\n",
    "        device=DEVICE,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "        num_features=val_X.shape[-1],\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_X,\n",
    "        val_y=val_y,\n",
    "        locs=locs,\n",
    "        adapter_cfg=SPATIAL_CONFIG,   # ← 修正：你原本這裡傳成 config=...\n",
    "        tau1=tau1_opt, tau2=tau2_opt,\n",
    "        writer=writer_best,\n",
    "    )\n",
    "    train_unregularized_adapter(adapter_best, pretrain_epochs=5)\n",
    "    writer_best.close()\n",
    "\n",
    "    y_reg_test_den_t = predict_test_den_fn(adapter_best)\n",
    "    rmse_reg_test, mae_reg_test, r2_reg_test = compute_metrics(test_y_den_t, y_reg_test_den_t)\n",
    "\n",
    "    # ----- 5) CSV -----\n",
    "    csv_path = Path(\"metrics_summary_TFT.csv\")\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\"seed\",\"model\",\"trial\",\"tau1\",\"tau2\",\"rmse_val\",\"mae_val\",\"r2_val\",\"rmse_test\",\"mae_test\",\"r2_test\"])\n",
    "        w.writerow([dataset_seed,\"TFT\",\"\", \"\", \"\", f\"{rmse_tft:.6f}\",f\"{mae_tft:.6f}\",f\"{r2_tft:.6f}\",\n",
    "                    f\"{rmse_tft_test:.6f}\",f\"{mae_tft_test:.6f}\",f\"{r2_tft_test:.6f}\"])\n",
    "        w.writerow([dataset_seed,\"Unreg\",\"\", \"0\",\"0\", f\"{rmse_unreg:.6f}\",f\"{mae_unreg:.6f}\",f\"{r2_unreg:.6f}\",\n",
    "                    f\"{rmse_unreg_test:.6f}\",f\"{mae_unreg_test:.6f}\",f\"{r2_unreg_test:.6f}\"])\n",
    "        w.writerow([dataset_seed,\"Reg\", best_no, f\"{tau1_opt:.6g}\",f\"{tau2_opt:.6g}\",\n",
    "                    f\"{rmse_opt:.6f}\",f\"{mae_opt:.6f}\",f\"{r2_opt:.6f}\",\n",
    "                    f\"{rmse_reg_test:.6f}\",f\"{mae_reg_test:.6f}\",f\"{r2_reg_test:.6f}\"])\n",
    "\n",
    "    print(\n",
    "        f\"Dataset {dataset_seed}: \"\n",
    "        f\"TFT {rmse_tft:.3f} | Unreg {rmse_unreg:.3f} | \"\n",
    "        f\"Reg {rmse_opt:.3f} (test {rmse_reg_test:.3f})\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"TFT\":   {\"rmse_val\": rmse_tft, \"rmse_test\": rmse_tft_test, \"r2_val\": r2_tft, \"r2_test\": r2_tft_test},\n",
    "        \"unreg\": {\"rmse_val\": rmse_unreg, \"rmse_test\": rmse_unreg_test, \"r2_val\": r2_unreg, \"r2_test\": r2_unreg_test},\n",
    "        \"reg\":   {\"rmse_val\": rmse_opt, \"rmse_test\": rmse_reg_test, \"r2_val\": r2_opt, \"r2_test\": r2_reg_test,\n",
    "                  \"tau1\": tau1_opt, \"tau2\": tau2_opt},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6642f",
   "metadata": {},
   "source": [
    "## 6. Run Full Experiment Suite\n",
    "before test epoch change to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ababa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for seed in range(EXPERIMENT_TRIALS_CONFIG['seed_range_start'], EXPERIMENT_TRIALS_CONFIG['seed_range_end']):\n",
    "    print(f\"\\nStarting experiment for seed {seed}\")\n",
    "    results = run_one_experiment(seed, n_trials=EXPERIMENT_TRIALS_CONFIG['n_trials_per_seed'])\n",
    "    all_results.append(results)\n",
    "    cache.clear()\n",
    "    clear_gpu_memory()\n",
    "    print(f\"✅ Completed seed {seed}\")\n",
    "\n",
    "print(\"\\n🎉 All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0388ee",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization\n",
    "\n",
    "還未完全改完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_df = pd.read_csv(\"metrics_summary_TFT.csv\")\n",
    "print(\"📊 Results Summary:\")\n",
    "print(results_df.groupby('model')[['rmse_val', 'rmse_test', 'r2_val', 'r2_test']].mean())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_val', ax=axes[0,0])\n",
    "axes[0,0].set_title('Validation RMSE')\n",
    "axes[0,0].set_ylabel('RMSE')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_test', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test RMSE')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "sns.boxplot(data=results_df, x='model', y='r2_val', ax=axes[1,0])\n",
    "axes[1,0].set_title('Validation R²')\n",
    "axes[1,0].set_ylabel('R²')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='r2_test', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test R²')\n",
    "axes[1,1].set_ylabel('R²')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best hyperparameters for regularized model\n",
    "reg_results = results_df[results_df['model'] == 'Reg']\n",
    "print(\"\\n🔧 Best Hyperparameters for Regularized Model:\")\n",
    "print(reg_results[['tau1', 'tau2', 'rmse_val', 'rmse_test']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison summary (TFT baseline)\n",
    "print(\"=== Performance Comparison Summary (TFT as baseline) ===\")\n",
    "\n",
    "# Means by model\n",
    "tft_mean_rmse   = results_df[results_df['model'] == 'TFT']['rmse_test'].mean()\n",
    "unreg_mean_rmse = results_df[results_df['model'] == 'Unreg']['rmse_test'].mean()\n",
    "reg_mean_rmse   = results_df[results_df['model'] == 'Reg']['rmse_test'].mean()\n",
    "\n",
    "print(f\"TFT (baseline)  - Mean Test RMSE: {tft_mean_rmse:.4f}\")\n",
    "print(f\"Unregularized   - Mean Test RMSE: {unreg_mean_rmse:.4f} \"\n",
    "      f\"({(1 - unreg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement vs TFT)\")\n",
    "print(f\"Regularized     - Mean Test RMSE: {reg_mean_rmse:.4f} \"\n",
    "      f\"({(1 - reg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement vs TFT)\")\n",
    "\n",
    "# Statistical significance test (paired by seed): TFT vs Regularized\n",
    "from scipy import stats\n",
    "\n",
    "pivot = results_df.pivot_table(index='seed', columns='model', values='rmse_test', aggfunc='mean')\n",
    "paired = pivot.dropna(subset=['TFT', 'Reg'])  # keep only seeds that have both\n",
    "t_stat, p_value = stats.ttest_rel(paired['TFT'].values, paired['Reg'].values)\n",
    "\n",
    "print(f\"\\nStatistical Test (TFT vs Regularized):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant improvement: {'Yes' if p_value < 0.05 else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
