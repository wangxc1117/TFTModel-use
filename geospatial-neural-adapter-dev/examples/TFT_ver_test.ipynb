{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFT vs. Spatial Adapter Comparison with Tuning Parameter Selection\n",
    "\n",
    "This notebook implements a comprehensive comparison between:\n",
    "1. **TFT** - Linear baseline (no spatial term)\n",
    "2. **Unregularized Spatial Adapter** - Neural spatial model without regularization\n",
    "3. **Regularized Spatial Adapter** - Neural spatial model with optimized tau1, tau2 parameters\n",
    "\n",
    "The experiment uses Optuna for hyperparameter optimization and evaluates performance across multiple random seeds.\n",
    "\n",
    "my work: ols 換成 TFT 然後執行模擬\n",
    "\n",
    "epoch 30 ->10\n",
    "'n_time_steps' 1024 -> 512\n",
    "'n_locations': 512 -> 256 \n",
    "GPU n_trials_per_seed 20 -> 10 \n",
    "n_dataset_seeds 10 -> 5\n",
    "完整實驗待跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345997df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "\n",
    "from geospatial_neural_adapter.cpp_extensions import estimate_covariance\n",
    "from geospatial_neural_adapter.utils.experiment import log_covariance_and_basis\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    ModelCache,\n",
    "    clear_gpu_memory,\n",
    "    create_experiment_config,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "from geospatial_neural_adapter.models.spatial_basis_learner import SpatialBasisLearner\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapter\n",
    "from geospatial_neural_adapter.models.trend_model import TrendModel\n",
    "from geospatial_neural_adapter.models.wrapper_examples.tft_wrapper import TFTWrapper\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import (\n",
    "    prepare_all_with_scaling,\n",
    "    denormalize_predictions,\n",
    ")\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "from geospatial_neural_adapter.models.pretrained_trend_model import (\n",
    "    create_pretrained_trend_model,\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Imports successful (TFT backbone enabled; all OLS utilities removed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f86c36",
   "metadata": {},
   "source": [
    "## 1. Parameter Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_time_steps': 512,  #1024\n",
    "    'n_locations': 256,  #512\n",
    "    'noise_std': 4.0,\n",
    "    'eigenvalue': 16.0,\n",
    "    'latent_dim': 1,\n",
    "    'ckpt_dir': \"admm_bcd_ckpts\",\n",
    "}\n",
    "\n",
    "SPLIT_CONFIG = {\n",
    "    \"train_ratio\": 0.70,\n",
    "    \"val_ratio\": 0.15,   \n",
    "}\n",
    "\n",
    "TFT_CONFIG = {\n",
    "    \"input_chunk_length\": 48,\n",
    "    \"output_chunk_length\": 1,\n",
    "    \"hidden_size\": 64,\n",
    "    \"lstm_layers\": 1,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"dropout\": 0.10,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 10,  #30\n",
    "    \"random_state\": 42,\n",
    "    \"add_relative_index\": True,\n",
    "}\n",
    "\n",
    "PL_TRAINER_KWARGS = (\n",
    "    {\"accelerator\": \"gpu\", \"devices\": 1,\n",
    "     \"logger\": True,                 \n",
    "     \"enable_progress_bar\": True,   \n",
    "     \"enable_model_summary\": False,\n",
    "     \"num_sanity_val_steps\": 0}\n",
    "    if torch.cuda.is_available()\n",
    "    else {\"accelerator\": \"cpu\", \"devices\": 1,\n",
    "          \"logger\": True,\n",
    "          \"enable_progress_bar\": True,\n",
    "          \"enable_model_summary\": False,\n",
    "          \"num_sanity_val_steps\": 0}\n",
    ")\n",
    "\n",
    "# Spatial Neural Adapter Configuration using dataclasses\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import (\n",
    "    SpatialNeuralAdapterConfig, ADMMConfig, TrainingConfig, BasisConfig\n",
    ")\n",
    "\n",
    "# ADMM Configuration\n",
    "admm_config = ADMMConfig(\n",
    "    rho=1.0,            \n",
    "    dual_momentum=0.2,  \n",
    "    max_iters=3000,     \n",
    "    min_outer=20,       \n",
    "    tol=1e-4,           \n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    lr_mu=1e-2,           \n",
    "    batch_size=64,        \n",
    "    pretrain_epochs=5,    \n",
    "    use_mixed_precision=False,\n",
    ")\n",
    "\n",
    "# Basis Configuration\n",
    "basis_config = BasisConfig(\n",
    "    phi_every=5,        \n",
    "    phi_freeze=200,     \n",
    "    matrix_reg=1e-6,    \n",
    "    irl1_max_iters=10,  \n",
    "    irl1_eps=1e-6,      \n",
    "    irl1_tol=5e-4,      \n",
    ")\n",
    "\n",
    "# Complete Spatial Neural Adapter Configuration\n",
    "SPATIAL_CONFIG = SpatialNeuralAdapterConfig(\n",
    "    admm=admm_config,\n",
    "    training=training_config,\n",
    "    basis=basis_config\n",
    ")\n",
    "\n",
    "# Legacy config dict for backward compatibility (if needed)\n",
    "CFG = SPATIAL_CONFIG.to_dict()\n",
    "CFG.update(EXPERIMENT_CONFIG)\n",
    "CFG.update({\n",
    "    \"split\": SPLIT_CONFIG,\n",
    "    \"tft\": TFT_CONFIG,\n",
    "})\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "torch.manual_seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "Path(EXPERIMENT_CONFIG[\"ckpt_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info['device'] == 'cuda':\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Experiment Configuration ===\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Split Configuration ===\")\n",
    "for k, v in SPLIT_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== TFT Configuration (Baseline) ===\")\n",
    "for k, v in TFT_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Spatial Neural Adapter Configuration ===\")\n",
    "SPATIAL_CONFIG.log_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a32565",
   "metadata": {},
   "source": [
    "## 2. Initialize Utilities\n",
    " 目前減少跑的次數 確認完之後要跑完整的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment configuration\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=10 if torch.cuda.is_available() else 50,\n",
    "    n_dataset_seeds=5,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=6,\n",
    ")\n",
    "\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "print(\"Utilities initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777490a",
   "metadata": {},
   "source": [
    "## 3. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19da385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with meaningful correlations\n",
    "print(\"Generating correlated synthetic data...\")\n",
    "\n",
    "locs = np.linspace(-3, 3, CFG[\"n_locations\"])\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locs,\n",
    "    n_time_steps=CFG[\"n_time_steps\"],\n",
    "    noise_std=CFG[\"noise_std\"],\n",
    "    eigenvalue=CFG[\"eigenvalue\"],\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=CFG[\"seed\"]\n",
    ")\n",
    "\n",
    "# Prepare datasets with scaling\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=SPLIT_CONFIG[\"train_ratio\"],\n",
    "    val_ratio=SPLIT_CONFIG[\"val_ratio\"],\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG[\"tft\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Extract tensors (scaled)\n",
    "_, train_X, train_y = train_dataset.tensors\n",
    "_, val_X,   val_y   = val_dataset.tensors\n",
    "_, test_X,  test_y  = test_dataset.tensors\n",
    "\n",
    "if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "\n",
    "y_all_scaled = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T_full, N)\n",
    "x_all_scaled = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T_full, N, F)\n",
    "\n",
    "T_full = y_all_scaled.shape[0]\n",
    "N      = y_all_scaled.shape[1]\n",
    "F      = x_all_scaled.shape[2]\n",
    "\n",
    "train_T = int(T_full * SPLIT_CONFIG[\"train_ratio\"])\n",
    "val_T   = int(T_full * (SPLIT_CONFIG[\"train_ratio\"] + SPLIT_CONFIG[\"val_ratio\"]))\n",
    "test_T  = T_full\n",
    "\n",
    "series_all   = TimeSeries.from_values(y_all_scaled)      # shape (T_full, N)\n",
    "series_train = series_all[:train_T]\n",
    "series_val   = series_all[train_T:val_T]\n",
    "series_test  = series_all[val_T:]\n",
    "\n",
    "y_true_future = y_all_scaled[train_T:]   # (T_val+T_test, N)\n",
    "\n",
    "\n",
    "p_dim = train_X.shape[-1]\n",
    "print(f\"Data shapes (cont_features, targets): {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")\n",
    "print(f\"Feature dimension: {p_dim}\")\n",
    "print(f\"T_full={T_full}, N={N}, F={F} | splits: train={train_T}, val={val_T-train_T}, test={test_T-val_T}\")\n",
    "print(\"Scaled series prepared for TFT (Darts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize data characteristics\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Plot 1: Target distribution\n",
    "# axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "# axes[0, 0].set_title('Target Distribution')\n",
    "# axes[0, 0].set_xlabel('Target Value')\n",
    "# axes[0, 0].set_ylabel('Frequency')\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 2: Spatial pattern at first time step\n",
    "# axes[0, 1].plot(locs, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "# axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "# axes[0, 1].set_xlabel('Location')\n",
    "# axes[0, 1].set_ylabel('Target Value')\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 3: Temporal pattern at middle location\n",
    "# time_steps = np.arange(len(targets))\n",
    "# axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "# axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "# axes[1, 0].set_xlabel('Time Step')\n",
    "# axes[1, 0].set_ylabel('Target Value')\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 4: Feature correlations\n",
    "# feature_corrs = []\n",
    "# for i in range(cont_features.shape[-1]):\n",
    "#     corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "#     feature_corrs.append(corr)\n",
    "\n",
    "# axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "# axes[1, 1].set_title('Feature-Target Correlations')\n",
    "# axes[1, 1].set_xlabel('Feature Index')\n",
    "# axes[1, 1].set_ylabel('Correlation')\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 多-seed 本地粗調（不改全域） ===\n",
    "def multi_seed_tft_tuning(\n",
    "    dataset_seed_list=(1,2,3),\n",
    "    split_cfg={\"train_ratio\": 0.70, \"val_ratio\": 0.15},\n",
    "    sim_base={\"n_time_steps\": 512, \"n_locations\": 256, \"noise_std\": 4.0, \"eigenvalue\": 16.0,\n",
    "              \"eta_rho\": 0.8, \"f_rho\": 0.6, \"global_mean\": 50.0,\n",
    "              \"feature_noise_std\": 0.1, \"non_linear_strength\": 0.2},\n",
    "    candidates=None\n",
    "):\n",
    "    import numpy as np, torch, pytorch_lightning as pl\n",
    "    from typing import Dict, Any, List, Tuple\n",
    "    from darts import TimeSeries\n",
    "    from darts.models import TFTModel\n",
    "    from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "    from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling, denormalize_predictions\n",
    "\n",
    "    # ==== Helpers: shape & metrics ====\n",
    "    def _to_tensor_2d(x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x)\n",
    "        if x.ndim == 3 and x.shape[-1] == 1:  # (T, N, 1) -> (T, N)\n",
    "            x = x.squeeze(-1)\n",
    "        return x.float()\n",
    "\n",
    "    def _ensure_TN(x, N_expected):\n",
    "        # 統一為 (T, N)；若收到 (N, T) 就轉置\n",
    "        if x.ndim != 2:\n",
    "            raise ValueError(f\"Expect 2D tensor, got shape {tuple(x.shape)}\")\n",
    "        T, N = x.shape\n",
    "        if N != N_expected and T == N_expected:\n",
    "            return x.T.contiguous()\n",
    "        return x\n",
    "\n",
    "    def _denorm_to_TN(x, preproc, N_expected):\n",
    "        # denorm -> tensor -> squeeze -> ensure (T, N)\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().cpu().numpy()\n",
    "        y_den = denormalize_predictions(x, preproc)        # 可能回 np.ndarray 或 torch.Tensor\n",
    "        y_den = _to_tensor_2d(y_den)                       # -> torch.Tensor (T,N) or (N,T)\n",
    "        y_den = _ensure_TN(y_den, N_expected)              # -> (T, N)\n",
    "        return y_den\n",
    "\n",
    "    def _metrics(true_den, pred_den):\n",
    "        eps = 1e-8\n",
    "        mse  = torch.mean((pred_den-true_den)**2); rmse  = torch.sqrt(mse).item()\n",
    "        rmspe = torch.sqrt(torch.mean(((pred_den-true_den)/(true_den.abs()+eps))**2)).item()\n",
    "        ss_res = torch.sum((true_den-pred_den)**2)\n",
    "        ss_tot = torch.sum((true_den - torch.mean(true_den,0,True))**2)\n",
    "        r2 = 1.0 - (ss_res/(ss_tot+eps)).item()\n",
    "        return rmse, rmspe, r2\n",
    "\n",
    "    if candidates is None:\n",
    "        candidates = [\n",
    "            {\"name\":\"A_len24_h64\",\n",
    "             \"input_chunk_length\":24, \"output_chunk_length\":1,\n",
    "             \"hidden_size\":64, \"lstm_layers\":1, \"num_attention_heads\":4,\n",
    "             \"dropout\":0.10, \"batch_size\":64, \"n_epochs\":20,\n",
    "             \"add_relative_index\":True, \"random_state\":42,\n",
    "             # 可選： \"gradient_clip_val\": 0.5,\n",
    "             # 可選： \"early_stopping\":{\"monitor\":\"val_loss\",\"patience\":3,\"min_delta\":0.0},\n",
    "            },\n",
    "            {\"name\":\"B_len48_h64\",\n",
    "             \"input_chunk_length\":48, \"output_chunk_length\":1,\n",
    "             \"hidden_size\":64, \"lstm_layers\":1, \"num_attention_heads\":4,\n",
    "             \"dropout\":0.10, \"batch_size\":64, \"n_epochs\":20,\n",
    "             \"add_relative_index\":True, \"random_state\":42,\n",
    "            },\n",
    "            {\"name\":\"C_len72_h64\",\n",
    "             \"input_chunk_length\":72, \"output_chunk_length\":1,\n",
    "             \"hidden_size\":64, \"lstm_layers\":1, \"num_attention_heads\":4,\n",
    "             \"dropout\":0.10, \"batch_size\":64, \"n_epochs\":20,\n",
    "             \"add_relative_index\":True, \"random_state\":42,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    # ==== make series ====\n",
    "    def _mk_series(train_y, val_y, test_y, train_X, val_X, test_X):\n",
    "        if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "        if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "        if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "        y_all = torch.cat([train_y, val_y, test_y], 0).squeeze(-1).cpu().numpy()\n",
    "        x_all = torch.cat([train_X, val_X, test_X], 0).cpu().numpy()\n",
    "        T_full, N, F = y_all.shape[0], y_all.shape[1], x_all.shape[2]\n",
    "        T_tr = int(T_full*split_cfg[\"train_ratio\"])\n",
    "        T_va = int(T_full*(split_cfg[\"train_ratio\"]+split_cfg[\"val_ratio\"]))\n",
    "        val_len, test_len = T_va - T_tr, T_full - T_va\n",
    "        series_list, past_list = [], []\n",
    "        for i in range(N):\n",
    "            series_list.append(TimeSeries.from_values(y_all[:, i]))\n",
    "            past_list.append(TimeSeries.from_values(x_all[:, i, :]))\n",
    "        return series_list, past_list, T_tr, T_va, val_len, test_len, N\n",
    "\n",
    "    # ==== Lightning Trainer kwargs（不依賴外部） ====\n",
    "    trainer_kwargs = ({\"accelerator\":\"gpu\",\"devices\":1,\"logger\":False,\"enable_progress_bar\":False,\n",
    "                       \"enable_model_summary\":False,\"num_sanity_val_steps\":0}\n",
    "                      if torch.cuda.is_available()\n",
    "                      else {\"accelerator\":\"cpu\",\"devices\":1,\"logger\":False,\"enable_progress_bar\":False,\n",
    "                            \"enable_model_summary\":False,\"num_sanity_val_steps\":0})\n",
    "\n",
    "    from statistics import mean, median, pstdev\n",
    "    summary = {c[\"name\"]: {\"seed_metrics\": []} for c in candidates}\n",
    "\n",
    "    for ds_seed in dataset_seed_list:\n",
    "        # 生成該 seed 的資料\n",
    "        locs = np.linspace(-3, 3, sim_base[\"n_locations\"])\n",
    "        catf, conf, tgts = generate_time_synthetic_data(\n",
    "            locs=locs, n_time_steps=sim_base[\"n_time_steps\"],\n",
    "            noise_std=sim_base[\"noise_std\"], eigenvalue=sim_base[\"eigenvalue\"],\n",
    "            eta_rho=sim_base[\"eta_rho\"], f_rho=sim_base[\"f_rho\"],\n",
    "            global_mean=sim_base[\"global_mean\"], feature_noise_std=sim_base[\"feature_noise_std\"],\n",
    "            non_linear_strength=sim_base[\"non_linear_strength\"], seed=ds_seed\n",
    "        )\n",
    "        train_ds, val_ds, test_ds, preproc = prepare_all_with_scaling(\n",
    "            cat_features=catf, cont_features=conf, targets=tgts,\n",
    "            train_ratio=split_cfg[\"train_ratio\"], val_ratio=split_cfg[\"val_ratio\"],\n",
    "            feature_scaler_type=\"standard\", target_scaler_type=\"standard\",\n",
    "            fit_on_train_only=True\n",
    "        )\n",
    "        _, train_X, train_y = train_ds.tensors\n",
    "        _, val_X,   val_y   = val_ds.tensors\n",
    "        _, test_X,  test_y  = test_ds.tensors\n",
    "\n",
    "        series_list, past_list, T_tr, T_va, val_len, test_len, N = _mk_series(\n",
    "            train_y, val_y, test_y, train_X, val_X, test_X\n",
    "        )\n",
    "\n",
    "        # 真值（反標準化 → (T,N)）\n",
    "        val_y_den  = _denorm_to_TN(val_y.squeeze(-1),  preproc, N)\n",
    "        test_y_den = _denorm_to_TN(test_y.squeeze(-1), preproc, N)\n",
    "\n",
    "        for cfg in candidates:\n",
    "            pl.seed_everything(cfg.get(\"random_state\", 123))\n",
    "\n",
    "            # EarlyStopping + gradient clipping 放進 trainer kwargs\n",
    "            tk = dict(trainer_kwargs)\n",
    "            if cfg.get(\"early_stopping\"):\n",
    "                from pytorch_lightning.callbacks import EarlyStopping\n",
    "                es = EarlyStopping(monitor=cfg[\"early_stopping\"][\"monitor\"],\n",
    "                                   patience=cfg[\"early_stopping\"][\"patience\"],\n",
    "                                   min_delta=cfg[\"early_stopping\"][\"min_delta\"], mode=\"min\")\n",
    "                tk[\"callbacks\"] = list(tk.get(\"callbacks\", [])) + [es]\n",
    "            if cfg.get(\"gradient_clip_val\") is not None:\n",
    "                tk[\"gradient_clip_val\"] = cfg[\"gradient_clip_val\"]\n",
    "\n",
    "            # 建立模型\n",
    "            model = TFTModel(\n",
    "                input_chunk_length=cfg[\"input_chunk_length\"],\n",
    "                output_chunk_length=cfg[\"output_chunk_length\"],\n",
    "                hidden_size=cfg[\"hidden_size\"], lstm_layers=cfg[\"lstm_layers\"],\n",
    "                num_attention_heads=cfg[\"num_attention_heads\"],\n",
    "                dropout=cfg[\"dropout\"], batch_size=cfg[\"batch_size\"],\n",
    "                n_epochs=cfg[\"n_epochs\"], add_relative_index=cfg[\"add_relative_index\"],\n",
    "                random_state=cfg[\"random_state\"],\n",
    "                # 若需要可加：optimizer_kwargs=cfg.get(\"optimizer_kwargs\"),\n",
    "                pl_trainer_kwargs=tk,\n",
    "            )\n",
    "\n",
    "            # 拆分序列與 past covariates\n",
    "            series_train = [s[:T_tr] for s in series_list]\n",
    "            series_val   = [s[T_tr:T_va] for s in series_list]\n",
    "            past_train   = [c[:T_tr] for c in past_list]\n",
    "            past_val     = [c[T_tr:T_va] for c in past_list]\n",
    "\n",
    "            # 訓練\n",
    "            model.fit(series=series_train, val_series=series_val,\n",
    "                      past_covariates=past_train, val_past_covariates=past_val, verbose=True)\n",
    "\n",
    "            # 滾動 one-step 預測（val + test）\n",
    "            import numpy as _np\n",
    "            yval, ytest = [], []\n",
    "            for i in range(N):\n",
    "                preds = model.historical_forecasts(\n",
    "                    series=series_list[i], past_covariates=past_list[i],\n",
    "                    start=T_tr, forecast_horizon=1, retrain=False, verbose=False\n",
    "                ).values()\n",
    "                yval.append(preds[:val_len])\n",
    "                ytest.append(preds[val_len:val_len+test_len])\n",
    "            yval = _np.stack(yval, 1)   # (T_val,  N)\n",
    "            ytest = _np.stack(ytest, 1) # (T_test, N)\n",
    "\n",
    "            # 反標準化 → (T,N)\n",
    "            yval_den  = _denorm_to_TN(yval,  preproc, N)\n",
    "            ytest_den = _denorm_to_TN(ytest, preproc, N)\n",
    "\n",
    "            # 保險檢查\n",
    "            assert val_y_den.shape == yval_den.shape\n",
    "            assert test_y_den.shape == ytest_den.shape\n",
    "\n",
    "            # 指標\n",
    "            rmse_v, rmspe_v, r2_v   = _metrics(val_y_den,  yval_den)\n",
    "            rmse_t, rmspe_t, r2_tst = _metrics(test_y_den, ytest_den)\n",
    "\n",
    "            summary[cfg[\"name\"]][\"seed_metrics\"].append({\n",
    "                \"seed\": ds_seed, \"rmse_val\": rmse_v, \"rmspe_val\": rmspe_v, \"r2_val\": r2_v,\n",
    "                \"rmse_test\": rmse_t, \"rmspe_test\": rmspe_t, \"r2_test\": r2_tst\n",
    "            })\n",
    "            print(f\"[{cfg['name']} | seed={ds_seed}] \"\n",
    "                  f\"val RMSE={rmse_v:.4f}, R²={r2_v:.4f} | \"\n",
    "                  f\"test RMSE={rmse_t:.4f}, R²={r2_tst:.4f}\")\n",
    "\n",
    "    # ==== 聚合各組表現（以 test RMSE 為主、std 為穩定性、同時回報 R²>0 比例）====\n",
    "    print(\"\\n=== Multi-seed Summary ===\")\n",
    "    ranking = []\n",
    "    for name, rec in summary.items():\n",
    "        mets = rec[\"seed_metrics\"]\n",
    "        mr = [m[\"rmse_test\"] for m in mets]\n",
    "        rr = [m[\"r2_test\"] for m in mets]\n",
    "        from statistics import mean, median, pstdev\n",
    "        entry = {\n",
    "            \"name\": name,\n",
    "            \"mean_rmse_test\": mean(mr),\n",
    "            \"median_rmse_test\": median(mr),\n",
    "            \"std_rmse_test\": pstdev(mr) if len(mr) > 1 else 0.0,\n",
    "            \"r2_pos_ratio\": sum(1 for x in rr if x > 0.0) / len(rr),\n",
    "            \"detail\": mets\n",
    "        }\n",
    "        ranking.append(entry)\n",
    "        print(f\"- {name}: mean RMSE={entry['mean_rmse_test']:.4f} | \"\n",
    "              f\"median RMSE={entry['median_rmse_test']:.4f} | \"\n",
    "              f\"std={entry['std_rmse_test']:.4f} | \"\n",
    "              f\"R²>0 ratio={entry['r2_pos_ratio']:.2f}\")\n",
    "\n",
    "    ranking.sort(key=lambda x: (x[\"mean_rmse_test\"], x[\"std_rmse_test\"]))\n",
    "    best = ranking[0]\n",
    "    print(\"\\n>>> 建議採用（以 mean test RMSE 最小、std 次要）：\", best[\"name\"])\n",
    "    print(\"   平均RMSE={:.4f} | 中位數RMSE={:.4f} | std={:.4f} | R²>0比例={:.2f}\".format(\n",
    "        best[\"mean_rmse_test\"], best[\"median_rmse_test\"], best[\"std_rmse_test\"], best[\"r2_pos_ratio\"]\n",
    "    ))\n",
    "    chosen = next(c for c in candidates if c[\"name\"] == best[\"name\"])\n",
    "    suggest_TFT_CONFIG = {\n",
    "        \"input_chunk_length\": chosen[\"input_chunk_length\"],\n",
    "        \"output_chunk_length\": chosen[\"output_chunk_length\"],\n",
    "        \"hidden_size\": chosen[\"hidden_size\"],\n",
    "        \"lstm_layers\": chosen[\"lstm_layers\"],\n",
    "        \"num_attention_heads\": chosen[\"num_attention_heads\"],\n",
    "        \"dropout\": chosen[\"dropout\"],\n",
    "        \"batch_size\": chosen[\"batch_size\"],\n",
    "        \"n_epochs\": chosen[\"n_epochs\"],\n",
    "        \"add_relative_index\": chosen[\"add_relative_index\"],\n",
    "        \"random_state\": chosen[\"random_state\"],\n",
    "    }\n",
    "    print(\"\\n請手動回填到全域 TFT_CONFIG（核心欄位）：\")\n",
    "    print(suggest_TFT_CONFIG)\n",
    "\n",
    "# 範例呼叫（你可改 seed 列表與候選組合）\n",
    "multi_seed_tft_tuning(dataset_seed_list=(1,2,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fb6b7",
   "metadata": {},
   "source": [
    "## 4. Baseline Implementation\n",
    "在Main Experiment Function 做了訓練與測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== TFT Baseline (demo style; no spatial term, no future covariates) ====\n",
    "print(\"Training TFT baseline (demo-only, no spatial)...\")\n",
    "\n",
    "pl.seed_everything(CFG[\"tft\"][\"random_state\"])\n",
    "\n",
    "# 1) Per-location target & past covariates（不建 future covariates）\n",
    "series_list, past_cov_list = [], []\n",
    "for i in range(N):\n",
    "    s_i   = TimeSeries.from_values(y_all_scaled[:, i])       # target (T,) -> TS\n",
    "    cov_i = TimeSeries.from_values(x_all_scaled[:, i, :])    # cont features (T, F) -> TS\n",
    "    series_list.append(s_i)\n",
    "    past_cov_list.append(cov_i)\n",
    "\n",
    "# 2) Split（train/val 對齊）\n",
    "series_train_list = [s[:train_T]      for s in series_list]\n",
    "series_val_list   = [s[train_T:val_T] for s in series_list]\n",
    "past_cov_train    = [c[:train_T]      for c in past_cov_list]\n",
    "val_past_covs     = [c[train_T:val_T] for c in past_cov_list]\n",
    "\n",
    "# 3) Train TFT（只傳 past；驗證也傳 val_past_covariates）\n",
    "tft = TFTModel(\n",
    "    **TFT_CONFIG,                      # 需含 add_relative_index=True 或等價 encoders\n",
    "    pl_trainer_kwargs=PL_TRAINER_KWARGS\n",
    ")\n",
    "tft.fit(\n",
    "    series=series_train_list,\n",
    "    val_series=series_val_list,\n",
    "    past_covariates=past_cov_train,\n",
    "    val_past_covariates=val_past_covs,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4) One-step rolling forecast via historical_forecasts（只用 past_covariates）\n",
    "val_len  = val_y.shape[0]\n",
    "test_len = test_y.shape[0]\n",
    "\n",
    "yhat_val_list, yhat_test_list = [], []\n",
    "for i in range(N):\n",
    "    yhat_i = tft.historical_forecasts(\n",
    "        series=series_list[i],\n",
    "        past_covariates=past_cov_list[i],\n",
    "        start=train_T,\n",
    "        forecast_horizon=1,\n",
    "        retrain=False,\n",
    "        verbose=False\n",
    "    ).values()  # (Tval+Ttest,)\n",
    "    yhat_val_list.append(yhat_i[:val_len])\n",
    "    yhat_test_list.append(yhat_i[val_len:val_len + test_len])\n",
    "\n",
    "# 回到 (T, N)（scaled space）\n",
    "yhat_val_sc  = np.stack(yhat_val_list,  axis=1)\n",
    "yhat_test_sc = np.stack(yhat_test_list, axis=1)\n",
    "\n",
    "# 5) 指標在原始尺度計算（denorm → torch.Tensor 2D）\n",
    "def to_tensor_2d(x):\n",
    "    if isinstance(x, np.ndarray): x = torch.from_numpy(x)\n",
    "    if x.ndim == 3: x = x.squeeze(-1)\n",
    "    return x.float()\n",
    "\n",
    "y_val_den_t     = to_tensor_2d(denormalize_predictions(val_y.squeeze(-1),  preprocessor))               # (Tval, N)\n",
    "y_test_den_t    = to_tensor_2d(denormalize_predictions(test_y.squeeze(-1), preprocessor))               # (Ttest, N)\n",
    "yhat_val_den_t  = to_tensor_2d(denormalize_predictions(torch.from_numpy(yhat_val_sc).float(),  preprocessor))\n",
    "yhat_test_den_t = to_tensor_2d(denormalize_predictions(torch.from_numpy(yhat_test_sc).float(), preprocessor))\n",
    "\n",
    "rmse_tft_val,  mae_tft_val,  r2_tft_val  = compute_metrics(y_val_den_t,  yhat_val_den_t)\n",
    "rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(y_test_den_t, yhat_test_den_t)\n",
    "\n",
    "print(f\"TFT Validation - RMSE: {rmse_tft_val:.4f}, R²: {r2_tft_val:.4f}\")\n",
    "print(f\"TFT Test       - RMSE: {rmse_tft_test:.4f}, R²: {r2_tft_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285ba2",
   "metadata": {},
   "source": [
    "## 5. Main Experiment Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc94b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mk_series_per_station(\n",
    "    train_y, val_y, test_y, train_X, val_X, test_X, split_cfg\n",
    ") -> Tuple[List[TimeSeries], List[TimeSeries], List[TimeSeries], int, int, int, int, int]:\n",
    "    \"\"\"\n",
    "    (T, N, F) / (T, N[,1]) 串回完整時間，轉成每站一條 TimeSeries + covariates\n",
    "    retrrn: series_list, past_cov_list, future_cov_list, T_tr, T_va, val_len, test_len, N, F\n",
    "    \"\"\"\n",
    "    if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "    if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "    if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "\n",
    "    y_all_scaled = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T_full, N)\n",
    "    x_all_scaled = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T_full, N, F)\n",
    "\n",
    "    T_full = y_all_scaled.shape[0]\n",
    "    N      = y_all_scaled.shape[1]\n",
    "    F      = x_all_scaled.shape[2]\n",
    "\n",
    "    T_tr = int(T_full * split_cfg[\"train_ratio\"])\n",
    "    T_va = int(T_full * (split_cfg[\"train_ratio\"] + split_cfg[\"val_ratio\"]))\n",
    "    val_len  = T_va - T_tr\n",
    "    test_len = T_full - T_va\n",
    "\n",
    "    series_list, past_cov_list, future_cov_list = [], [], []\n",
    "    for i in range(N):\n",
    "        s_i   = TimeSeries.from_values(y_all_scaled[:, i])    # (T,)\n",
    "        cov_i = TimeSeries.from_values(x_all_scaled[:, i, :]) # (T, F)\n",
    "        series_list.append(s_i)\n",
    "        past_cov_list.append(cov_i)\n",
    "        future_cov_list.append(cov_i)  \n",
    "\n",
    "    return series_list, past_cov_list, future_cov_list, T_tr, T_va, val_len, test_len, N, F\n",
    "\n",
    "\n",
    "def build_tft_and_data(\n",
    "    *,\n",
    "    cat_features: np.ndarray,          \n",
    "    cont_features: np.ndarray,         \n",
    "    targets: np.ndarray,               \n",
    "    split_cfg: Dict[str, Any],         \n",
    "    tft_cfg: Dict[str, Any],           \n",
    "    pl_trainer_kwargs: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    # 1) scaling\n",
    "    train_ds, val_ds, test_ds, preproc = prepare_all_with_scaling(\n",
    "        cat_features=cat_features,\n",
    "        cont_features=cont_features,\n",
    "        targets=targets,\n",
    "        train_ratio=split_cfg[\"train_ratio\"],\n",
    "        val_ratio=split_cfg[\"val_ratio\"],\n",
    "        feature_scaler_type=\"standard\",\n",
    "        target_scaler_type=\"standard\",\n",
    "        fit_on_train_only=True,\n",
    "    )\n",
    "    _, train_X, train_y = train_ds.tensors\n",
    "    _, val_X,   val_y   = val_ds.tensors\n",
    "    _, test_X,  test_y  = test_ds.tensors\n",
    "\n",
    "    (series_list, past_cov_list, _future_cov_list,\n",
    "     T_tr, T_va, val_len, test_len, N, F) = _mk_series_per_station(\n",
    "        train_y, val_y, test_y, train_X, val_X, test_X, split_cfg\n",
    "    )\n",
    "\n",
    "    pl.seed_everything(tft_cfg.get(\"random_state\", 42))\n",
    "    tft_model = TFTModel(**tft_cfg, pl_trainer_kwargs=pl_trainer_kwargs)\n",
    "    tft_model.fit(\n",
    "        series=[s[:T_tr] for s in series_list],\n",
    "        val_series=[s[T_tr:T_va] for s in series_list],\n",
    "        past_covariates=[c[:T_tr] for c in past_cov_list],\n",
    "        val_past_covariates=[c[T_tr:T_va] for c in past_cov_list],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    yhat_val_list, yhat_test_list = [], []\n",
    "    for i in range(N):\n",
    "        preds = tft_model.historical_forecasts(\n",
    "            series=series_list[i],\n",
    "            past_covariates=past_cov_list[i],\n",
    "            start=T_tr, forecast_horizon=1, retrain=False, verbose=False\n",
    "        ).values()  # (Tval+Ttest,)\n",
    "        yhat_val_list.append(preds[:val_len])\n",
    "        yhat_test_list.append(preds[val_len:val_len+test_len])\n",
    "    yhat_val_sc = np.stack(yhat_val_list, 1)\n",
    "    yhat_test_sc = np.stack(yhat_test_list, 1)\n",
    "\n",
    "    def _t2d(x):\n",
    "        if isinstance(x, np.ndarray): x = torch.from_numpy(x)\n",
    "        if x.ndim == 3: x = x.squeeze(-1)\n",
    "        return x.float()\n",
    "\n",
    "    val_y_den_t     = _t2d(denormalize_predictions(val_y.squeeze(-1),  preproc))\n",
    "    test_y_den_t    = _t2d(denormalize_predictions(test_y.squeeze(-1), preproc))\n",
    "    yhat_val_den_t  = _t2d(denormalize_predictions(torch.from_numpy(yhat_val_sc).float(),  preproc))\n",
    "    yhat_test_den_t = _t2d(denormalize_predictions(torch.from_numpy(yhat_test_sc).float(), preproc))\n",
    "\n",
    "    return {\n",
    "        \"tft_model\": tft_model,\n",
    "        \"preprocessor\": preproc,\n",
    "        \"split_idx\": {\"T_tr\": T_tr, \"T_va\": T_va, \"val_len\": val_len, \"test_len\": test_len},\n",
    "        \"N\": N, \"F\": F,\n",
    "        \"val_y_den_t\": val_y_den_t, \"test_y_den_t\": test_y_den_t,\n",
    "        \"yhat_val_den_t\": yhat_val_den_t, \"yhat_test_den_t\": yhat_test_den_t,\n",
    "        \"val_y_sc\": val_y.squeeze(-1).cpu().numpy(),\n",
    "        \"test_y_sc\": test_y.squeeze(-1).cpu().numpy(),\n",
    "        \"yhat_val_sc\": yhat_val_sc, \"yhat_test_sc\": yhat_test_sc,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_spatial_adapter_from_demo(\n",
    "    *,\n",
    "    tft_model,\n",
    "    device,\n",
    "    n_locations: int,\n",
    "    latent_dim: int,\n",
    "    num_features: int,      \n",
    "    train_loader,\n",
    "    val_cont: torch.Tensor,\n",
    "    val_y: torch.Tensor,\n",
    "    locs,\n",
    "    adapter_cfg,            \n",
    "    tau1: float,\n",
    "    tau2: float,\n",
    "    writer=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    TFTWrapper -> create_pretrained_trend_model(含 residual head) -> SpatialNeuralAdapter\n",
    "    \"\"\"\n",
    "    tft_wrapper = TFTWrapper(\n",
    "        tft_model=tft_model,\n",
    "        num_locations=n_locations,\n",
    "        num_features=num_features,\n",
    "    )\n",
    "\n",
    "    tft_trend_model = create_pretrained_trend_model(\n",
    "        pretrained_model=tft_wrapper,\n",
    "        input_shape=(None, n_locations, num_features),  # (B, N, F)\n",
    "        output_shape=(None, n_locations),               # (B, N)\n",
    "        model_type=\"custom\",\n",
    "        freeze_backbone=True,       \n",
    "        add_residual_head=True,     \n",
    "        residual_hidden_dim=64,     \n",
    "        dropout_rate=0.1,\n",
    "    )\n",
    "\n",
    "    basis = SpatialBasisLearner(n_locations, latent_dim).to(device)\n",
    "\n",
    "    adapter = SpatialNeuralAdapter(\n",
    "        trend=tft_trend_model,\n",
    "        basis=basis,\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_cont.to(device),\n",
    "        val_y=val_y.to(device),\n",
    "        locs=locs,\n",
    "        config=adapter_cfg,   \n",
    "        device=device,\n",
    "        writer=writer,\n",
    "        tau1=tau1,\n",
    "        tau2=tau2,\n",
    "    )\n",
    "    return adapter\n",
    "\n",
    "\n",
    "def train_unregularized_adapter(\n",
    "    adapter: SpatialNeuralAdapter,\n",
    "    pretrain_epochs: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    adapter.pretrain_trend(epochs=pretrain_epochs)\n",
    "    adapter.init_basis_dense()\n",
    "    adapter.run()\n",
    "    return {\"adapter\": adapter}\n",
    "\n",
    "\n",
    "def train_regularized_adapter_with_optuna(\n",
    "    build_adapter_fn,                             # closure: (tau1, tau2) -> SpatialNeuralAdapter\n",
    "    val_y_den_t: torch.Tensor,\n",
    "    predict_val_den_fn,                           # closure: (adapter) -> torch.Tensor denorm predictions on val\n",
    "    n_trials: int = 30,\n",
    "    study_name: str = \"TFT_spatial_adapter_reg\",\n",
    ") -> Dict[str, Any]:\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "        tau1 = trial.suggest_float(\"tau1\", 1e-4, 1e8, log=True)\n",
    "        tau2 = trial.suggest_float(\"tau2\", 1e-4, 1e8, log=True)\n",
    "        adapter = build_adapter_fn(tau1, tau2)\n",
    "        adapter.pretrain_trend(epochs=3)\n",
    "        adapter.init_basis_dense()\n",
    "        adapter.run()\n",
    "\n",
    "        y_val_pred_den = predict_val_den_fn(adapter)\n",
    "        rmse, mae, r2 = compute_metrics(val_y_den_t, y_val_pred_den)\n",
    "\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\",  r2)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name, direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=MedianPruner(n_warmup_steps=5),\n",
    "        load_if_exists=False,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    best = study.best_trial\n",
    "    return {\n",
    "        \"tau1\": best.params[\"tau1\"],\n",
    "        \"tau2\": best.params[\"tau2\"],\n",
    "        \"rmse\": best.user_attrs[\"rmse\"],\n",
    "        \"mae\":  best.user_attrs[\"mae\"],\n",
    "        \"r2\":   best.user_attrs[\"r2\"],\n",
    "        \"best_trial\": best.number,\n",
    "        \"study\": study,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_adapter_on_test_from_demo(\n",
    "    adapter: SpatialNeuralAdapter,\n",
    "    denorm_true_test: torch.Tensor,\n",
    "    predict_test_den_fn,          \n",
    ") -> Dict[str, float]:\n",
    "\n",
    "    y_test_pred_den = predict_test_den_fn(adapter)\n",
    "    rmse, mae, r2 = compute_metrics(denorm_true_test, y_test_pred_den)\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_experiment(dataset_seed: int, n_trials: int = 30):\n",
    "    log_root = Path(\"TFT_runs\") / f\"TFT_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Data \n",
    "    catf, conf, tgts = generate_time_synthetic_data(\n",
    "        locs=locs,\n",
    "        n_time_steps=CFG[\"n_time_steps\"],\n",
    "        noise_std=CFG[\"noise_std\"],\n",
    "        eigenvalue=CFG[\"eigenvalue\"],\n",
    "        eta_rho=0.8,\n",
    "        f_rho=0.6,\n",
    "        global_mean=50.0,\n",
    "        feature_noise_std=0.1,\n",
    "        non_linear_strength=0.2,\n",
    "        seed=dataset_seed,\n",
    "    )\n",
    "    train_ds, val_ds, test_ds, preproc = prepare_all_with_scaling(\n",
    "        cat_features=catf, cont_features=conf, targets=tgts,\n",
    "        train_ratio=SPLIT_CONFIG[\"train_ratio\"], val_ratio=SPLIT_CONFIG[\"val_ratio\"],\n",
    "        feature_scaler_type=\"standard\", target_scaler_type=\"standard\", fit_on_train_only=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds, batch_size=SPATIAL_CONFIG.training.batch_size, shuffle=True)\n",
    "    _, val_X,  val_y  = val_ds.tensors\n",
    "    _, test_X, test_y = test_ds.tensors\n",
    "\n",
    "    # TFT baseline\n",
    "    tft_pack = build_tft_and_data(\n",
    "        cat_features=catf, cont_features=conf, targets=tgts,\n",
    "        split_cfg=SPLIT_CONFIG, tft_cfg=TFT_CONFIG, pl_trainer_kwargs=PL_TRAINER_KWARGS,\n",
    "    )\n",
    "    tft_model        = tft_pack[\"tft_model\"]\n",
    "    val_y_den_t      = tft_pack[\"val_y_den_t\"]\n",
    "    test_y_den_t     = tft_pack[\"test_y_den_t\"]\n",
    "    yhat_val_den_t   = tft_pack[\"yhat_val_den_t\"]\n",
    "    yhat_test_den_t  = tft_pack[\"yhat_test_den_t\"]\n",
    "\n",
    "    rmse_tft, mae_tft, r2_tft = compute_metrics(val_y_den_t,  yhat_val_den_t)\n",
    "    rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(test_y_den_t, yhat_test_den_t)\n",
    "\n",
    "    # Unregularized adapter \n",
    "    writer_boot = SummaryWriter(log_dir=log_root / \"bootstrap\")\n",
    "    adapter_unreg = build_spatial_adapter_from_demo(\n",
    "        tft_model=tft_model,\n",
    "        device=DEVICE,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "        num_features=val_X.shape[-1],\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_X,\n",
    "        val_y=val_y,\n",
    "        locs=locs,\n",
    "        adapter_cfg=SPATIAL_CONFIG,   \n",
    "        tau1=0.0, tau2=0.0,\n",
    "        writer=writer_boot,\n",
    "    )\n",
    "    train_unregularized_adapter(adapter_unreg, pretrain_epochs=5)\n",
    "    writer_boot.close()\n",
    "\n",
    "    def predict_val_den_fn(adapter):\n",
    "        with torch.no_grad():\n",
    "            y_pred_sc = adapter.predict(val_X.to(DEVICE), val_y.to(DEVICE))  \n",
    "            if y_pred_sc.ndim == 3:\n",
    "                y_pred_sc = y_pred_sc.squeeze(-1)\n",
    "            # -> CPU numpy\n",
    "            y_pred_sc_np = y_pred_sc.detach().cpu().numpy()\n",
    "            y_pred_den = denormalize_predictions(y_pred_sc_np, preproc)       \n",
    "            if isinstance(y_pred_den, np.ndarray):\n",
    "                y_pred_den = torch.from_numpy(y_pred_den).float()\n",
    "            return y_pred_den  \n",
    "\n",
    "    def predict_test_den_fn(adapter):\n",
    "        with torch.no_grad():\n",
    "            y_pred_sc = adapter.predict(test_X.to(DEVICE), test_y.to(DEVICE)) \n",
    "            if y_pred_sc.ndim == 3:\n",
    "                y_pred_sc = y_pred_sc.squeeze(-1)\n",
    "            # -> CPU numpy\n",
    "            y_pred_sc_np = y_pred_sc.detach().cpu().numpy()\n",
    "            y_pred_den = denormalize_predictions(y_pred_sc_np, preproc)\n",
    "            if isinstance(y_pred_den, np.ndarray):\n",
    "                y_pred_den = torch.from_numpy(y_pred_den).float()\n",
    "            return y_pred_den  \n",
    "        \n",
    "    y_unreg_val_den_t  = predict_val_den_fn(adapter_unreg)\n",
    "    y_unreg_test_den_t = predict_test_den_fn(adapter_unreg)\n",
    "    rmse_unreg, mae_unreg, r2_unreg = compute_metrics(val_y_den_t,  y_unreg_val_den_t)\n",
    "    rmse_unreg_test, mae_unreg_test, r2_unreg_test = compute_metrics(test_y_den_t, y_unreg_test_den_t)\n",
    "\n",
    "    # Regularized (Optuna) \n",
    "    def build_adapter_fn(tau1: float, tau2: float):\n",
    "        writer = SummaryWriter(log_dir=log_root / f\"trial_tau1_{tau1:.3g}_tau2_{tau2:.3g}\")\n",
    "        adapter = build_spatial_adapter_from_demo(\n",
    "            tft_model=tft_model,\n",
    "            device=DEVICE,\n",
    "            n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "            latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "            num_features=val_X.shape[-1],\n",
    "            train_loader=train_loader,\n",
    "            val_cont=val_X,\n",
    "            val_y=val_y,\n",
    "            locs=locs,\n",
    "            adapter_cfg=SPATIAL_CONFIG,  \n",
    "            tau1=tau1, tau2=tau2,\n",
    "            writer=writer,\n",
    "        )\n",
    "        adapter._tmp_writer = writer\n",
    "        return adapter\n",
    "\n",
    "    def predict_val_den_for_optuna(adapter):\n",
    "        y_pred = predict_val_den_fn(adapter)\n",
    "        if hasattr(adapter, \"_tmp_writer\") and adapter._tmp_writer is not None:\n",
    "            adapter._tmp_writer.close()\n",
    "            adapter._tmp_writer = None\n",
    "        return y_pred\n",
    "\n",
    "    reg_search = train_regularized_adapter_with_optuna(\n",
    "        build_adapter_fn=build_adapter_fn,\n",
    "        val_y_den_t=val_y_den_t,\n",
    "        predict_val_den_fn=predict_val_den_for_optuna,\n",
    "        n_trials=n_trials,\n",
    "        study_name=f\"TFT_spatial_adapter_reg_ds{dataset_seed}\",\n",
    "    )\n",
    "    tau1_opt, tau2_opt = reg_search[\"tau1\"], reg_search[\"tau2\"]\n",
    "    rmse_opt, mae_opt, r2_opt = reg_search[\"rmse\"], reg_search[\"mae\"], reg_search[\"r2\"]\n",
    "    best_no = reg_search[\"best_trial\"]\n",
    "\n",
    "    # Best adapter → retrain → test\n",
    "    writer_best = SummaryWriter(log_dir=log_root / f\"best_tau1_{tau1_opt:.3g}_tau2_{tau2_opt:.3g}\")\n",
    "    adapter_best = build_spatial_adapter_from_demo(\n",
    "        tft_model=tft_model,\n",
    "        device=DEVICE,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "        num_features=val_X.shape[-1],\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_X,\n",
    "        val_y=val_y,\n",
    "        locs=locs,\n",
    "        adapter_cfg=SPATIAL_CONFIG,   \n",
    "        tau1=tau1_opt, tau2=tau2_opt,\n",
    "        writer=writer_best,\n",
    "    )\n",
    "    train_unregularized_adapter(adapter_best, pretrain_epochs=5)\n",
    "    writer_best.close()\n",
    "\n",
    "    y_reg_test_den_t = predict_test_den_fn(adapter_best)\n",
    "    rmse_reg_test, mae_reg_test, r2_reg_test = compute_metrics(test_y_den_t, y_reg_test_den_t)\n",
    "\n",
    "    # write\n",
    "    csv_path = Path(\"metrics_summary_TFT.csv\")\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\"seed\",\"model\",\"trial\",\"tau1\",\"tau2\",\"rmse_val\",\"mae_val\",\"r2_val\",\"rmse_test\",\"mae_test\",\"r2_test\"])\n",
    "        w.writerow([dataset_seed,\"TFT\",\"\", \"\", \"\", f\"{rmse_tft:.6f}\",f\"{mae_tft:.6f}\",f\"{r2_tft:.6f}\",\n",
    "                    f\"{rmse_tft_test:.6f}\",f\"{mae_tft_test:.6f}\",f\"{r2_tft_test:.6f}\"])\n",
    "        w.writerow([dataset_seed,\"Unreg\",\"\", \"0\",\"0\", f\"{rmse_unreg:.6f}\",f\"{mae_unreg:.6f}\",f\"{r2_unreg:.6f}\",\n",
    "                    f\"{rmse_unreg_test:.6f}\",f\"{mae_unreg_test:.6f}\",f\"{r2_unreg_test:.6f}\"])\n",
    "        w.writerow([dataset_seed,\"Reg\", best_no, f\"{tau1_opt:.6g}\",f\"{tau2_opt:.6g}\",\n",
    "                    f\"{rmse_opt:.6f}\",f\"{mae_opt:.6f}\",f\"{r2_opt:.6f}\",\n",
    "                    f\"{rmse_reg_test:.6f}\",f\"{mae_reg_test:.6f}\",f\"{r2_reg_test:.6f}\"])\n",
    "\n",
    "    print(\n",
    "        f\"Dataset {dataset_seed}: \"\n",
    "        f\"TFT {rmse_tft:.3f} | Unreg {rmse_unreg:.3f} | \"\n",
    "        f\"Reg {rmse_opt:.3f} (test {rmse_reg_test:.3f})\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"TFT\":   {\"rmse_val\": rmse_tft, \"rmse_test\": rmse_tft_test, \"r2_val\": r2_tft, \"r2_test\": r2_tft_test},\n",
    "        \"unreg\": {\"rmse_val\": rmse_unreg, \"rmse_test\": rmse_unreg_test, \"r2_val\": r2_unreg, \"r2_test\": r2_unreg_test},\n",
    "        \"reg\":   {\"rmse_val\": rmse_opt, \"rmse_test\": rmse_reg_test, \"r2_val\": r2_opt, \"r2_test\": r2_reg_test,\n",
    "                  \"tau1\": tau1_opt, \"tau2\": tau2_opt},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6642f",
   "metadata": {},
   "source": [
    "## 6. Run Full Experiment Suite\n",
    "before test epoch change to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ababa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for seed in range(EXPERIMENT_TRIALS_CONFIG['seed_range_start'], EXPERIMENT_TRIALS_CONFIG['seed_range_end']):\n",
    "    print(f\"\\nStarting experiment for seed {seed}\")\n",
    "    results = run_one_experiment(seed, n_trials=EXPERIMENT_TRIALS_CONFIG['n_trials_per_seed'])\n",
    "    all_results.append(results)\n",
    "    clear_gpu_memory()\n",
    "    print(f\"✅ Completed seed {seed}\")\n",
    "\n",
    "print(\"\\n🎉 All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0388ee",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_df = pd.read_csv(\"metrics_summary_TFT.csv\")\n",
    "print(\"📊 Results Summary:\")\n",
    "print(results_df.groupby('model')[['rmse_val', 'rmse_test', 'r2_val', 'r2_test']].mean())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_val', ax=axes[0,0])\n",
    "axes[0,0].set_title('Validation RMSE')\n",
    "axes[0,0].set_ylabel('RMSE')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_test', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test RMSE')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "sns.boxplot(data=results_df, x='model', y='r2_val', ax=axes[1,0])\n",
    "axes[1,0].set_title('Validation R²')\n",
    "axes[1,0].set_ylabel('R²')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='r2_test', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test R²')\n",
    "axes[1,1].set_ylabel('R²')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best hyperparameters for regularized model\n",
    "reg_results = results_df[results_df['model'] == 'Reg']\n",
    "print(\"\\n🔧 Best Hyperparameters for Regularized Model:\")\n",
    "print(reg_results[['tau1', 'tau2', 'rmse_val', 'rmse_test']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison summary (TFT baseline)\n",
    "print(\"=== Performance Comparison Summary (TFT as baseline) ===\")\n",
    "\n",
    "# Means by model\n",
    "tft_mean_rmse   = results_df[results_df['model'] == 'TFT']['rmse_test'].mean()\n",
    "unreg_mean_rmse = results_df[results_df['model'] == 'Unreg']['rmse_test'].mean()\n",
    "reg_mean_rmse   = results_df[results_df['model'] == 'Reg']['rmse_test'].mean()\n",
    "\n",
    "print(f\"TFT (baseline)  - Mean Test RMSE: {tft_mean_rmse:.4f}\")\n",
    "print(f\"Unregularized   - Mean Test RMSE: {unreg_mean_rmse:.4f} \"\n",
    "      f\"({(1 - unreg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement vs TFT)\")\n",
    "print(f\"Regularized     - Mean Test RMSE: {reg_mean_rmse:.4f} \"\n",
    "      f\"({(1 - reg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement vs TFT)\")\n",
    "\n",
    "# Statistical significance test (paired by seed): TFT vs Regularized\n",
    "from scipy import stats\n",
    "\n",
    "pivot = results_df.pivot_table(index='seed', columns='model', values='rmse_test', aggfunc='mean')\n",
    "paired = pivot.dropna(subset=['TFT', 'Reg'])  # keep only seeds that have both\n",
    "t_stat, p_value = stats.ttest_rel(paired['TFT'].values, paired['Reg'].values)\n",
    "\n",
    "print(f\"\\nStatistical Test (TFT vs Regularized):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.8f}\")\n",
    "print(f\"  Significant improvement: {'Yes' if p_value < 0.05 else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
