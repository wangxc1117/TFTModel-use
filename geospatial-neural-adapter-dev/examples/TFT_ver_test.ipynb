{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS vs. Spatial Adapter Comparison with Tuning Parameter Selection\n",
    "\n",
    "This notebook implements a comprehensive comparison between:\n",
    "1. **OLS (Ordinary Least Squares)** - Linear baseline\n",
    "2. **Unregularized Spatial Adapter** - Neural spatial model without regularization\n",
    "3. **Regularized Spatial Adapter** - Neural spatial model with optimized tau1, tau2 parameters\n",
    "\n",
    "The experiment uses Optuna for hyperparameter optimization and evaluates performance across multiple random seeds.\n",
    "\n",
    "my work: ols 換成 TFT 然後執行模擬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345997df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "from geospatial_neural_adapter.cpp_extensions import estimate_covariance\n",
    "from geospatial_neural_adapter.utils.experiment import log_covariance_and_basis\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    ModelCache,\n",
    "    clear_gpu_memory,\n",
    "    compute_ols_coefficients,\n",
    "    create_experiment_config,\n",
    "    create_fresh_models,\n",
    "    predict_ols,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "from geospatial_neural_adapter.models.spatial_basis_learner import SpatialBasisLearner\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapter\n",
    "from geospatial_neural_adapter.models.trend_model import TrendModel\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling, denormalize_predictions\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f86c36",
   "metadata": {},
   "source": [
    "## 1. Parameter Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_time_steps': 1024,\n",
    "    'n_locations': 512,\n",
    "    'noise_std': 4.0,\n",
    "    'eigenvalue': 16.0,\n",
    "    'latent_dim': 1,\n",
    "    'ckpt_dir': \"admm_bcd_ckpts\",\n",
    "}\n",
    "\n",
    "# Spatial Neural Adapter Configuration using dataclasses\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import (\n",
    "    SpatialNeuralAdapterConfig, ADMMConfig, TrainingConfig, BasisConfig\n",
    ")\n",
    "\n",
    "# ADMM Configuration\n",
    "admm_config = ADMMConfig(\n",
    "    rho=1.0,  # Base ADMM penalty parameter\n",
    "    dual_momentum=0.2,  # Dual variable momentum\n",
    "    max_iters=3000,  # Maximum ADMM iterations\n",
    "    min_outer=20,  # Minimum outer iterations before convergence check\n",
    "    tol=1e-4,  # Convergence tolerance\n",
    ")\n",
    "\n",
    "# Training Configuration\n",
    "training_config = TrainingConfig(\n",
    "    lr_mu=1e-2,  # Learning rate for trend parameters\n",
    "    batch_size=64,  # Batch size for theta step\n",
    "    pretrain_epochs=5,  # Default pretraining epochs\n",
    "    use_mixed_precision=False,  # Whether to use mixed precision\n",
    ")\n",
    "\n",
    "# Basis Configuration\n",
    "basis_config = BasisConfig(\n",
    "    phi_every=5,  # Update basis every N iterations\n",
    "    phi_freeze=200,  # Stop updating basis after N iterations\n",
    "    matrix_reg=1e-6,  # Matrix regularization for basis update\n",
    "    irl1_max_iters=10,  # IRL₁ maximum iterations\n",
    "    irl1_eps=1e-6,  # IRL₁ epsilon\n",
    "    irl1_tol=5e-4,  # IRL₁ inner tolerance\n",
    ")\n",
    "\n",
    "# Complete Spatial Neural Adapter Configuration\n",
    "SPATIAL_CONFIG = SpatialNeuralAdapterConfig(\n",
    "    admm=admm_config,\n",
    "    training=training_config,\n",
    "    basis=basis_config\n",
    ")\n",
    "\n",
    "# Legacy config dict for backward compatibility (if needed)\n",
    "CFG = SPATIAL_CONFIG.to_dict()\n",
    "CFG.update(EXPERIMENT_CONFIG)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "Path(EXPERIMENT_CONFIG[\"ckpt_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info['device'] == 'cuda':\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Experiment Configuration ===\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Spatial Neural Adapter Configuration ===\")\n",
    "SPATIAL_CONFIG.log_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a32565",
   "metadata": {},
   "source": [
    "## 2. Initialize Utilities\n",
    " 目前減少跑的次數 確認完之後要跑完整的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model cache for hyperparameter optimization\n",
    "cache = ModelCache()\n",
    "\n",
    "# Create experiment configuration\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=5 if torch.cuda.is_available() else 50,\n",
    "    n_dataset_seeds=2,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=3,\n",
    ")\n",
    "\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "print(\"Utilities initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777490a",
   "metadata": {},
   "source": [
    "## 3. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19da385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with meaningful correlations\n",
    "print(\"Generating correlated synthetic data...\")\n",
    "\n",
    "locs = np.linspace(-3, 3, CFG[\"n_locations\"])\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locs,\n",
    "    n_time_steps=CFG[\"n_time_steps\"],\n",
    "    noise_std=CFG[\"noise_std\"],\n",
    "    eigenvalue=CFG[\"eigenvalue\"],\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=CFG[\"seed\"]\n",
    ")\n",
    "\n",
    "# Prepare datasets with scaling\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Extract tensors\n",
    "_, train_X, train_y = train_dataset.tensors\n",
    "_, val_X, val_y = val_dataset.tensors\n",
    "_, test_X, test_y = test_dataset.tensors\n",
    "\n",
    "p_dim = train_X.shape[-1]\n",
    "\n",
    "print(f\"Data shapes: {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")\n",
    "print(f\"Feature dimension: {p_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Target distribution\n",
    "axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Target Distribution')\n",
    "axes[0, 0].set_xlabel('Target Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Spatial pattern at first time step\n",
    "axes[0, 1].plot(locs, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "axes[0, 1].set_xlabel('Location')\n",
    "axes[0, 1].set_ylabel('Target Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Temporal pattern at middle location\n",
    "time_steps = np.arange(len(targets))\n",
    "axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "axes[1, 0].set_xlabel('Time Step')\n",
    "axes[1, 0].set_ylabel('Target Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature correlations\n",
    "feature_corrs = []\n",
    "for i in range(cont_features.shape[-1]):\n",
    "    corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "    feature_corrs.append(corr)\n",
    "\n",
    "axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Feature-Target Correlations')\n",
    "axes[1, 1].set_xlabel('Feature Index')\n",
    "axes[1, 1].set_ylabel('Correlation')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fb6b7",
   "metadata": {},
   "source": [
    "## 4. OLS Baseline Implementation\n",
    " OLS 改成 TFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f360a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_multivar_target_timeseries(y_np):\n",
    "    import numpy as np, pandas as pd\n",
    "    from darts import TimeSeries\n",
    "    if not isinstance(y_np, np.ndarray):\n",
    "        y_np = y_np.detach().cpu().numpy()\n",
    "    y_np = y_np.astype(np.float32)\n",
    "    T, N = y_np.shape\n",
    "    df = pd.DataFrame(\n",
    "        y_np,\n",
    "        index=pd.RangeIndex(start=0, stop=T),\n",
    "        columns=[f\"loc_{i}\" for i in range(N)]\n",
    "    ).astype(np.float32)\n",
    "    return TimeSeries.from_dataframe(df, value_cols=list(df.columns), fill_missing_dates=False)\n",
    "\n",
    "def to_past_cov_timeseries(x_np, locations_np=None):\n",
    "    import numpy as np, pandas as pd\n",
    "    from darts import TimeSeries\n",
    "\n",
    "    if not isinstance(x_np, np.ndarray):\n",
    "        x_np = x_np.detach().cpu().numpy()\n",
    "    x_np = x_np.astype(np.float32)\n",
    "    T, N, p = x_np.shape\n",
    "\n",
    "    flat = x_np.reshape(T, N * p)  # (T, N*p)\n",
    "    cols = [f\"cov_{j}_loc_{i}\" for i in range(N) for j in range(p)]\n",
    "\n",
    "    if locations_np is not None:\n",
    "        if not isinstance(locations_np, np.ndarray):\n",
    "            locations_np = np.asarray(locations_np)\n",
    "        loc_rep = np.tile(locations_np.reshape(1, -1), (T, 1)).astype(np.float32)  # (T, N)\n",
    "        flat   = np.concatenate([flat, loc_rep], axis=1)                            # (T, N*p + N)\n",
    "        cols   = cols + [f\"spatial_loc_{i}\" for i in range(loc_rep.shape[1])]\n",
    "\n",
    "    df = pd.DataFrame(flat, index=pd.RangeIndex(start=0, stop=T), columns=cols).astype(np.float32)\n",
    "    return TimeSeries.from_dataframe(df, value_cols=list(df.columns), fill_missing_dates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_tft_baseline_from_scaled_tensors(\n",
    "    train_cont, train_targets,\n",
    "    val_cont,   val_targets,\n",
    "    test_cont,  test_targets,\n",
    "    locations=None,\n",
    "    seed: int = 1,\n",
    "    use_past_covariates: bool = True,\n",
    "    input_chunk_length: int = 96,\n",
    "    output_chunk_length: int = 1,\n",
    "    n_epochs: int = 30,\n",
    "    batch_size: int = 64,\n",
    "    hidden_size: int = 64,\n",
    "    lstm_layers: int = 1,\n",
    "    num_attention_heads: int = 4,\n",
    "    dropout: float = 0.1,\n",
    "):\n",
    "    import numpy as np, pandas as pd, torch\n",
    "    from darts import TimeSeries\n",
    "    from darts.models import TFTModel\n",
    "\n",
    "    # reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # ---- numpy float32 ----\n",
    "    tr_x = train_cont.detach().cpu().numpy().astype(np.float32)\n",
    "    va_x = val_cont.detach().cpu().numpy().astype(np.float32)\n",
    "    te_x = test_cont.detach().cpu().numpy().astype(np.float32)\n",
    "    tr_y = train_targets.detach().cpu().numpy().astype(np.float32)\n",
    "    va_y = val_targets.detach().cpu().numpy().astype(np.float32)\n",
    "    te_y = test_targets.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # concat for slicing\n",
    "    X_all = np.concatenate([tr_x, va_x, te_x], axis=0)  # (T_all, N, p)\n",
    "    Y_all = np.concatenate([tr_y, va_y, te_y], axis=0)  # (T_all, N)\n",
    "    T_tr, T_va, T_te = tr_x.shape[0], va_x.shape[0], te_x.shape[0]\n",
    "    T_all = T_tr + T_va + T_te\n",
    "\n",
    "    # ---- TimeSeries (RangeIndex, float32) ----\n",
    "    ts_target_all = to_multivar_target_timeseries(Y_all)\n",
    "    ts_pastcov_all = to_past_cov_timeseries(X_all, locations_np=locations) if use_past_covariates else None\n",
    "\n",
    "    ts_train = ts_target_all[:T_tr]\n",
    "    ts_val   = ts_target_all[T_tr:T_tr+T_va]\n",
    "    ts_test  = ts_target_all[T_tr+T_va:T_tr+T_va+T_te]\n",
    "\n",
    "    cov_train = cov_val = cov_hist_for_test = None\n",
    "    if use_past_covariates:\n",
    "        cov_train = ts_pastcov_all[:T_tr]\n",
    "        cov_val   = ts_pastcov_all[T_tr:T_tr+T_va]\n",
    "        cov_hist_for_test = ts_pastcov_all[:T_tr+T_va]\n",
    "\n",
    "    # ---- 自製 future covariates（相對索引；float32；單變數）----\n",
    "    rel = (np.arange(T_all, dtype=np.float32).reshape(-1, 1)) / max(T_all - 1, 1)\n",
    "    df_rel = pd.DataFrame(rel, index=pd.RangeIndex(start=0, stop=T_all), columns=[\"rel_idx\"]).astype(np.float32)\n",
    "    futcov_all = TimeSeries.from_dataframe(df_rel, value_cols=[\"rel_idx\"], fill_missing_dates=False)\n",
    "    futcov_train = futcov_all[:T_tr]\n",
    "    futcov_for_val  = futcov_all            # 覆蓋到預測窗即可\n",
    "    futcov_for_test = futcov_all\n",
    "\n",
    "    # ---- 建立 TFT（不用 add_relative_index，改用我們的 future_covariates）----\n",
    "    model = TFTModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=output_chunk_length,\n",
    "        hidden_size=hidden_size,\n",
    "        lstm_layers=lstm_layers,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "        dropout=dropout,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=n_epochs,\n",
    "        random_state=seed,\n",
    "        force_reset=True,\n",
    "        add_relative_index=False,    # 關閉；我們自己提供 future_covariates\n",
    "        likelihood=None,\n",
    "        save_checkpoints=False,\n",
    "        pl_trainer_kwargs={\n",
    "            \"accelerator\": \"auto\",\n",
    "            \"devices\": \"auto\",\n",
    "            \"precision\": \"32-true\",  # 強制 32-bit，避免 Double/Float 衝突\n",
    "            \"enable_progress_bar\": True,\n",
    "            \"enable_model_summary\": False,\n",
    "            \"deterministic\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # ---- 訓練（只用 train）----\n",
    "    model.fit(\n",
    "        series=ts_train,\n",
    "        past_covariates=cov_train,\n",
    "        future_covariates=futcov_train,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # ---- 驗證（從 train 末端往後 len(val)）----\n",
    "    pred_val = model.predict(\n",
    "        n=len(ts_val),\n",
    "        series=ts_train,\n",
    "        past_covariates=cov_val,\n",
    "        future_covariates=futcov_for_val,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # ---- 測試（歷史 = train + val）----\n",
    "    hist_for_test = ts_train.append(ts_val)\n",
    "    pred_test = model.predict(\n",
    "        n=len(ts_test),\n",
    "        series=hist_for_test,\n",
    "        past_covariates=cov_hist_for_test,\n",
    "        future_covariates=futcov_for_test,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # ---- 指標 ----\n",
    "    def _rmse(a, b): return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "    def _mae(a, b):  return float(np.mean(np.abs(a - b)))\n",
    "    def _r2(a, b):\n",
    "        num = np.sum((a - b) ** 2); den = np.sum((a - a.mean()) ** 2)\n",
    "        return float(1 - num / den) if den > 0 else float(\"nan\")\n",
    "\n",
    "    y_val_true  = np.squeeze(ts_val.values())\n",
    "    y_val_pred  = np.squeeze(pred_val.values())\n",
    "    y_test_true = np.squeeze(ts_test.values())\n",
    "    y_test_pred = np.squeeze(pred_test.values())\n",
    "\n",
    "    return {\n",
    "        \"tft_val_rmse\":  _rmse(y_val_true,  y_val_pred),\n",
    "        \"tft_val_mae\":   _mae(y_val_true,   y_val_pred),\n",
    "        \"tft_val_r2\":    _r2(y_val_true,    y_val_pred),\n",
    "        \"tft_test_rmse\": _rmse(y_test_true, y_test_pred),\n",
    "        \"tft_test_mae\":  _mae(y_test_true,  y_test_pred),\n",
    "        \"tft_test_r2\":   _r2(y_test_true,   y_test_pred),\n",
    "        \"tft_pred_val\":  pred_val,\n",
    "        \"tft_pred_test\": pred_test,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285ba2",
   "metadata": {},
   "source": [
    "## 5. Main Experiment Function\n",
    "存檔從seed改為TFT_seed, \n",
    " 呼叫 OLS 的函式改成呼叫 TFT 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main: 完整的 run_one_experiment()（TFT 版；其餘流程維持原樣） ===\n",
    "def run_one_experiment(dataset_seed: int, n_trials: int = 30):\n",
    "    \"\"\"Run a complete experiment for one dataset seed (TFT baseline, no OLS).\"\"\"\n",
    "    log_root = Path(\"runs\") / f\"TFT_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Generate data for this seed\n",
    "    cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "        locs=locs,\n",
    "        n_time_steps=CFG[\"n_time_steps\"],\n",
    "        noise_std=CFG[\"noise_std\"],\n",
    "        eigenvalue=CFG[\"eigenvalue\"],\n",
    "        eta_rho=0.8,\n",
    "        f_rho=0.6,\n",
    "        global_mean=50.0,\n",
    "        feature_noise_std=0.1,\n",
    "        non_linear_strength=0.2,\n",
    "        seed=dataset_seed\n",
    "    )\n",
    "    train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "        cat_features=cat_features,\n",
    "        cont_features=cont_features,\n",
    "        targets=targets,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        feature_scaler_type=\"standard\",\n",
    "        target_scaler_type=\"standard\",\n",
    "        fit_on_train_only=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=SPATIAL_CONFIG.training.batch_size, shuffle=True)\n",
    "    _, train_X, train_y = train_dataset.tensors\n",
    "    _, val_X, val_y = val_dataset.tensors\n",
    "    _, test_X, test_y = test_dataset.tensors\n",
    "\n",
    "    # 🔧 統一成 float32，避免 Double/Float 衝突\n",
    "    train_X = train_X.to(torch.float32)\n",
    "    val_X   = val_X.to(torch.float32)\n",
    "    test_X  = test_X.to(torch.float32)\n",
    "    train_y = train_y.to(torch.float32)\n",
    "    val_y   = val_y.to(torch.float32)\n",
    "    test_y  = test_y.to(torch.float32)\n",
    "\n",
    "    p_dim = train_X.shape[-1]\n",
    "\n",
    "    # === TFT baseline（取代 OLS） ============================================\n",
    "    tft_res = train_eval_tft_baseline_from_scaled_tensors(\n",
    "        train_cont=train_X, train_targets=train_y,\n",
    "        val_cont=val_X,     val_targets=val_y,\n",
    "        test_cont=test_X,   test_targets=test_y,\n",
    "        locations=locs,     # 若沒有位置特徵可傳 None\n",
    "        seed=dataset_seed,\n",
    "        use_past_covariates=True,  # 與 tft_spatial_adapter_demo 對齊\n",
    "        input_chunk_length=96,\n",
    "        output_chunk_length=1,\n",
    "        n_epochs=30,\n",
    "        batch_size=64,\n",
    "        hidden_size=64,\n",
    "        lstm_layers=1,\n",
    "        num_attention_heads=4,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "\n",
    "    # Clear cache between datasets\n",
    "    cache.clear()\n",
    "\n",
    "    # === Bootstrap tau1=tau2=0 (unregularized) ===============================\n",
    "    clear_gpu_memory()\n",
    "    # 注意：此版不傳 w_ols/b_ols；若你的 create_fresh_models 仍要求，請先把簽名改掉或設為可為 None\n",
    "    boot_trend, boot_basis = create_fresh_models(\n",
    "        device=DEVICE,\n",
    "        p_dim=p_dim,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "    )\n",
    "    boot_writer = SummaryWriter(log_dir=log_root / \"bootstrap\")\n",
    "    boot = SpatialNeuralAdapter(\n",
    "        boot_trend,\n",
    "        boot_basis,\n",
    "        train_loader,\n",
    "        val_cont=val_X.to(DEVICE),\n",
    "        val_y=val_y.to(DEVICE),\n",
    "        locs=locs,\n",
    "        config=SPATIAL_CONFIG,\n",
    "        device=DEVICE,\n",
    "        writer=boot_writer,\n",
    "        tau1=0.0,\n",
    "        tau2=0.0,\n",
    "    )\n",
    "    boot.pretrain_trend(epochs=5)\n",
    "    boot.init_basis_dense()\n",
    "    boot.run()\n",
    "    cache.store(0.0, 0.0, boot_trend.state_dict(), boot_basis.state_dict())\n",
    "    boot_writer.close()\n",
    "\n",
    "    # Get unregularized predictions\n",
    "    y_boot_val = boot.predict(val_X.to(DEVICE), val_y.to(DEVICE))\n",
    "    rmse_boot, mae_boot, r2_boot = compute_metrics(val_y.to(DEVICE), y_boot_val)\n",
    "    y_boot_test = boot.predict(test_X.to(DEVICE), test_y.to(DEVICE))\n",
    "    rmse_boot_test, mae_boot_test, r2_boot_test = compute_metrics(test_y.to(DEVICE), y_boot_test)\n",
    "\n",
    "    # Clean up bootstrap models\n",
    "    del boot_trend, boot_basis, boot\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # === Optuna objective for hyperparameter optimization ====================\n",
    "    def objective(trial):\n",
    "        dev = DEVICE\n",
    "        tau1 = trial.suggest_float(\"tau1\", 1e-4, 1e8, log=True)\n",
    "        tau2 = trial.suggest_float(\"tau2\", 1e-4, 1e8, log=True)\n",
    "\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        trend, basis = create_fresh_models(\n",
    "            device=dev,\n",
    "            p_dim=p_dim,\n",
    "            n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "            latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "        )\n",
    "        cache.load_nearest(trend, basis, tau1, tau2)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=log_root / f\"trial_{trial.number:03d}\")\n",
    "        trainer = SpatialNeuralAdapter(\n",
    "            trend,\n",
    "            basis,\n",
    "            train_loader,\n",
    "            val_cont=val_X.to(dev),\n",
    "            val_y=val_y.to(dev),\n",
    "            locs=locs,\n",
    "            config=SPATIAL_CONFIG,\n",
    "            device=dev,\n",
    "            writer=writer,\n",
    "            tau1=tau1,\n",
    "            tau2=tau2,\n",
    "        )\n",
    "        trainer.pretrain_trend(epochs=3)\n",
    "        trainer.init_basis_dense()\n",
    "        trainer.run()\n",
    "\n",
    "        y_pred = trainer.predict(val_X.to(dev), val_y.to(dev))\n",
    "        rmse, mae, r2 = compute_metrics(val_y.to(dev), y_pred)\n",
    "\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\", r2)\n",
    "\n",
    "        writer.close()\n",
    "        cache.store(tau1, tau2, trend.state_dict(), basis.state_dict())\n",
    "\n",
    "        del trend, basis, trainer, y_pred\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    # Run Optuna optimization\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"spatial_adapter_ds{dataset_seed}\",\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=MedianPruner(n_warmup_steps=5),\n",
    "        load_if_exists=False,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    # Get best results\n",
    "    best = study.best_trial\n",
    "    rmse_opt = best.user_attrs[\"rmse\"]\n",
    "    mae_opt  = best.user_attrs[\"mae\"]\n",
    "    r2_opt   = best.user_attrs[\"r2\"]\n",
    "    tau1_opt = best.params[\"tau1\"]\n",
    "    tau2_opt = best.params[\"tau2\"]\n",
    "    best_no  = best.number\n",
    "    \n",
    "    # Test best model\n",
    "    dev_best = DEVICE\n",
    "    trend_best, basis_best = create_fresh_models(\n",
    "        device=dev_best,\n",
    "        p_dim=p_dim,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "    )\n",
    "    sd_t, sd_b = cache.cache[(tau1_opt, tau2_opt)]\n",
    "    trend_best.load_state_dict(sd_t)\n",
    "    basis_best.load_state_dict(sd_b)\n",
    "\n",
    "    trend_best.eval()\n",
    "    basis_best.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test = test_X.to(dev_best)\n",
    "        y_test = test_y.to(dev_best)\n",
    "        y_trend = trend_best(X_test)\n",
    "        residual = y_test - y_trend\n",
    "        y_basis = (residual @ basis_best.basis) @ basis_best.basis.T\n",
    "        y_reg_test = y_trend + y_basis\n",
    "    rmse_test, mae_test, r2_test = compute_metrics(y_test, y_reg_test)\n",
    "\n",
    "    # Write results（CSV 標頭保持不變）\n",
    "    csv_path = Path(\"metrics_summary.csv\")\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        import csv\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\n",
    "                \"seed\", \"model\", \"trial\", \"tau1\", \"tau2\",\n",
    "                \"rmse_val\", \"mae_val\", \"r2_val\",\n",
    "                \"rmse_test\", \"mae_test\", \"r2_test\"\n",
    "            ])\n",
    "\n",
    "        # TFT baseline\n",
    "        w.writerow([\n",
    "            dataset_seed, \"TFT\", \"\", \"\", \"\",\n",
    "            f\"{tft_res['tft_val_rmse']:.6f}\",  f\"{tft_res['tft_val_mae']:.6f}\",  f\"{tft_res['tft_val_r2']:.6f}\",\n",
    "            f\"{tft_res['tft_test_rmse']:.6f}\", f\"{tft_res['tft_test_mae']:.6f}\", f\"{tft_res['tft_test_r2']:.6f}\"\n",
    "        ])\n",
    "        # Unregularized\n",
    "        w.writerow([\n",
    "            dataset_seed, \"Unreg\", \"\", \"0\", \"0\",\n",
    "            f\"{rmse_boot:.6f}\", f\"{mae_boot:.6f}\", f\"{r2_boot:.6f}\",\n",
    "            f\"{rmse_boot_test:.6f}\", f\"{mae_boot_test:.6f}\", f\"{r2_boot_test:.6f}\"\n",
    "        ])\n",
    "        # Regularized（best trial）\n",
    "        w.writerow([\n",
    "            dataset_seed, \"Reg\", best_no, f\"{tau1_opt:.6g}\", f\"{tau2_opt:.6g}\",\n",
    "            f\"{rmse_opt:.6f}\", f\"{mae_opt:.6f}\", f\"{r2_opt:.6f}\",\n",
    "            f\"{rmse_test:.6f}\", f\"{mae_test:.6f}\", f\"{r2_test:.6f}\"\n",
    "        ])\n",
    "\n",
    "    print(\n",
    "        f\"Dataset {dataset_seed}:  \"\n",
    "        f\"TFT RMSE={tft_res['tft_val_rmse']:.3f} | \"\n",
    "        f\"Unreg RMSE={rmse_boot:.3f} | \"\n",
    "        f\"Reg RMSE={rmse_opt:.3f} (test {rmse_test:.3f})\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'tft':   {'rmse_val': tft_res['tft_val_rmse'],  'rmse_test': tft_res['tft_test_rmse'],  'r2_val': tft_res['tft_val_r2'],  'r2_test': tft_res['tft_test_r2']},\n",
    "        'unreg': {'rmse_val': rmse_boot,                'rmse_test': rmse_boot_test,            'r2_val': r2_boot,                'r2_test': r2_boot_test},\n",
    "        'reg':   {'rmse_val': rmse_opt,                 'rmse_test': rmse_test,                 'r2_val': r2_opt,                 'r2_test': r2_test,\n",
    "                  'tau1': tau1_opt, 'tau2': tau2_opt}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6642f",
   "metadata": {},
   "source": [
    "## 6. Run Full Experiment Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ababa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for seed in range(EXPERIMENT_TRIALS_CONFIG['seed_range_start'], EXPERIMENT_TRIALS_CONFIG['seed_range_end']):\n",
    "    print(f\"\\nStarting experiment for seed {seed}\")\n",
    "    results = run_one_experiment(seed, n_trials=EXPERIMENT_TRIALS_CONFIG['n_trials_per_seed'])\n",
    "    all_results.append(results)\n",
    "    # Clear cache between seeds to free memory\n",
    "    cache.clear()\n",
    "    clear_gpu_memory()\n",
    "    print(f\"✅ Completed seed {seed}\")\n",
    "\n",
    "print(\"\\n🎉 All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0388ee",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization\n",
    "\n",
    "OSL 還未完全改完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_df = pd.read_csv(\"metrics_summary.csv\")\n",
    "print(\"📊 Results Summary:\")\n",
    "print(results_df.groupby('model')[['rmse_val', 'rmse_test', 'r2_val', 'r2_test']].mean())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_val', ax=axes[0,0])\n",
    "axes[0,0].set_title('Validation RMSE')\n",
    "axes[0,0].set_ylabel('RMSE')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_test', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test RMSE')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "sns.boxplot(data=results_df, x='model', y='r2_val', ax=axes[1,0])\n",
    "axes[1,0].set_title('Validation R²')\n",
    "axes[1,0].set_ylabel('R²')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='r2_test', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test R²')\n",
    "axes[1,1].set_ylabel('R²')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best hyperparameters for regularized model\n",
    "reg_results = results_df[results_df['model'] == 'Reg']\n",
    "print(\"\\n🔧 Best Hyperparameters for Regularized Model:\")\n",
    "print(reg_results[['tau1', 'tau2', 'rmse_val', 'rmse_test']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison summary\n",
    "print(\"=== Performance Comparison Summary ===\")\n",
    "\n",
    "# Calculate improvements\n",
    "ols_mean_rmse = results_df[results_df['model'] == 'OLS']['rmse_test'].mean()\n",
    "unreg_mean_rmse = results_df[results_df['model'] == 'Unreg']['rmse_test'].mean()\n",
    "reg_mean_rmse = results_df[results_df['model'] == 'Reg']['rmse_test'].mean()\n",
    "\n",
    "print(f\"OLS (baseline) - Mean Test RMSE: {ols_mean_rmse:.4f}\")\n",
    "print(f\"Unregularized - Mean Test RMSE: {unreg_mean_rmse:.4f} ({(1 - unreg_mean_rmse/ols_mean_rmse)*100:.1f}% improvement)\")\n",
    "print(f\"Regularized - Mean Test RMSE: {reg_mean_rmse:.4f} ({(1 - reg_mean_rmse/ols_mean_rmse)*100:.1f}% improvement)\")\n",
    "\n",
    "# Statistical significance test\n",
    "from scipy import stats\n",
    "ols_scores = results_df[results_df['model'] == 'OLS']['rmse_test'].values\n",
    "reg_scores = results_df[results_df['model'] == 'Reg']['rmse_test'].values\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(ols_scores, reg_scores)\n",
    "print(f\"\\nStatistical Test (OLS vs Regularized):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant improvement: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
