{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS vs. Spatial Adapter Comparison with Tuning Parameter Selection\n",
    "\n",
    "This notebook implements a comprehensive comparison between:\n",
    "1. **OLS (Ordinary Least Squares)** - Linear baseline\n",
    "2. **Unregularized Spatial Adapter** - Neural spatial model without regularization\n",
    "3. **Regularized Spatial Adapter** - Neural spatial model with optimized tau1, tau2 parameters\n",
    "\n",
    "The experiment uses Optuna for hyperparameter optimization and evaluates performance across multiple random seeds.\n",
    "\n",
    "my work: ols 換成 TFT 然後執行模擬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345997df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangxc17/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wangxc17/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded spatial_utils from: /home/wangxc17/work/TFTModel-use/geospatial-neural-adapter-dev/geospatial_neural_adapter/cpp_extensions/spatial_utils.so\n",
      "✅ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "# NEW: Darts TFT backbone\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from geospatial_neural_adapter.cpp_extensions import estimate_covariance\n",
    "from geospatial_neural_adapter.utils.experiment import log_covariance_and_basis\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    ModelCache,\n",
    "    clear_gpu_memory,\n",
    "    create_experiment_config,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "from geospatial_neural_adapter.models.spatial_basis_learner import SpatialBasisLearner\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapter\n",
    "\n",
    "# NEW: 用 TFT 當 backbone 的包裝\n",
    "from geospatial_neural_adapter.models.pretrained_trend_model import create_pretrained_trend_model\n",
    "from geospatial_neural_adapter.models.wrapper_examples.tft_wrapper import TFTWrapper\n",
    "\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling, denormalize_predictions\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f86c36",
   "metadata": {},
   "source": [
    "## 1. Parameter Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4dfeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 18:01:01,829 - spatial_neural_adapter - INFO - SpatialNeuralAdapterConfig:\n",
      "2025-08-11 18:01:01,829 - spatial_neural_adapter - INFO -   ADMM Config:\n",
      "2025-08-11 18:01:01,830 - spatial_neural_adapter - INFO -     rho: 1.0\n",
      "2025-08-11 18:01:01,830 - spatial_neural_adapter - INFO -     dual_momentum: 0.2\n",
      "2025-08-11 18:01:01,830 - spatial_neural_adapter - INFO -     max_iters: 3000\n",
      "2025-08-11 18:01:01,831 - spatial_neural_adapter - INFO -     min_outer: 20\n",
      "2025-08-11 18:01:01,831 - spatial_neural_adapter - INFO -     tol: 0.0001\n",
      "2025-08-11 18:01:01,832 - spatial_neural_adapter - INFO -   Training Config:\n",
      "2025-08-11 18:01:01,832 - spatial_neural_adapter - INFO -     lr_mu: 0.01\n",
      "2025-08-11 18:01:01,833 - spatial_neural_adapter - INFO -     batch_size: 64\n",
      "2025-08-11 18:01:01,833 - spatial_neural_adapter - INFO -     pretrain_epochs: 5\n",
      "2025-08-11 18:01:01,834 - spatial_neural_adapter - INFO -     use_mixed_precision: False\n",
      "2025-08-11 18:01:01,835 - spatial_neural_adapter - INFO -   Basis Config:\n",
      "2025-08-11 18:01:01,835 - spatial_neural_adapter - INFO -     phi_every: 5\n",
      "2025-08-11 18:01:01,835 - spatial_neural_adapter - INFO -     phi_freeze: 200\n",
      "2025-08-11 18:01:01,836 - spatial_neural_adapter - INFO -     matrix_reg: 1e-06\n",
      "2025-08-11 18:01:01,836 - spatial_neural_adapter - INFO -     irl1_max_iters: 10\n",
      "2025-08-11 18:01:01,836 - spatial_neural_adapter - INFO -     irl1_eps: 1e-06\n",
      "2025-08-11 18:01:01,837 - spatial_neural_adapter - INFO -     irl1_tol: 0.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   Memory: 8.6 GB\n",
      "\n",
      "=== Experiment Configuration ===\n",
      "seed: 42\n",
      "n_time_steps: 1024\n",
      "n_locations: 512\n",
      "noise_std: 4.0\n",
      "eigenvalue: 16.0\n",
      "latent_dim: 1\n",
      "ckpt_dir: admm_bcd_ckpts\n",
      "\n",
      "=== Spatial Neural Adapter Configuration ===\n"
     ]
    }
   ],
   "source": [
    "# Experiment Configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_time_steps': 1024,\n",
    "    'n_locations': 512,\n",
    "    'noise_std': 4.0,\n",
    "    'eigenvalue': 16.0,\n",
    "    'latent_dim': 1,\n",
    "    'ckpt_dir': \"admm_bcd_ckpts\",\n",
    "}\n",
    "\n",
    "TFT_CONFIG = {\n",
    "    # 長度\n",
    "    'tft_input_chunk_length': None,  # None → 依 T_train 自動算\n",
    "    'tft_output_chunk_length': 1,\n",
    "    'tft_output_chunk_shift': 0,\n",
    "\n",
    "    # 模型規模\n",
    "    'tft_hidden_size': 16, #32         \n",
    "    'tft_lstm_layers': 1,            \n",
    "    'tft_num_attention_heads': 2,    \n",
    "    'tft_full_attention': False,\n",
    "    'tft_feed_forward': 'GatedResidualNetwork',\n",
    "    'tft_hidden_continuous_size': 16,             \n",
    "\n",
    "    # 訓練/優化\n",
    "    'tft_batch_size': 16, # 64\n",
    "    'tft_n_epochs': 5, #20\n",
    "    'tft_optimizer_kwargs': {'lr': 1e-3, 'weight_decay': 1e-4},\n",
    "}\n",
    "\n",
    "# Spatial Neural Adapter Configuration using dataclasses\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import (\n",
    "    SpatialNeuralAdapterConfig, ADMMConfig, TrainingConfig, BasisConfig\n",
    ")\n",
    "\n",
    "# ADMM Configuration\n",
    "admm_config = ADMMConfig(\n",
    "    rho=1.0,\n",
    "    dual_momentum=0.2,\n",
    "    max_iters=3000,\n",
    "    min_outer=20,\n",
    "    tol=1e-4,\n",
    ")\n",
    "\n",
    "# Training Configuration\n",
    "training_config = TrainingConfig(\n",
    "    lr_mu=1e-2,\n",
    "    batch_size=64,\n",
    "    pretrain_epochs=5,\n",
    "    use_mixed_precision=False,\n",
    ")\n",
    "\n",
    "# Basis Configuration\n",
    "basis_config = BasisConfig(\n",
    "    phi_every=5,\n",
    "    phi_freeze=200,\n",
    "    matrix_reg=1e-6,\n",
    "    irl1_max_iters=10,\n",
    "    irl1_eps=1e-6,\n",
    "    irl1_tol=5e-4,\n",
    ")\n",
    "\n",
    "# Complete Spatial Neural Adapter Configuration\n",
    "SPATIAL_CONFIG = SpatialNeuralAdapterConfig(\n",
    "    admm=admm_config,\n",
    "    training=training_config,\n",
    "    basis=basis_config\n",
    ")\n",
    "\n",
    "# Legacy dict for convenience\n",
    "CFG: Dict[str, Any] = SPATIAL_CONFIG.to_dict()\n",
    "CFG.update(EXPERIMENT_CONFIG)\n",
    "CFG.update(TFT_CONFIG) \n",
    "\n",
    "# Set random seed & paths\n",
    "torch.manual_seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "Path(EXPERIMENT_CONFIG[\"ckpt_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info['device'] == 'cuda':\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Experiment Configuration ===\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Spatial Neural Adapter Configuration ===\")\n",
    "SPATIAL_CONFIG.log_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a32565",
   "metadata": {},
   "source": [
    "## 2. Initialize Utilities\n",
    " 目前減少跑的次數 確認完之後要跑完整的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c16d3a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Configuration:\n",
      "  Trials per seed: 5\n",
      "  Dataset seeds: 1 to 2\n",
      "  Total experiments: 10\n",
      "  Device: GPU\n",
      "Utilities initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model cache for hyperparameter optimization\n",
    "cache = ModelCache()\n",
    "\n",
    "# Create experiment configuration\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=5 if torch.cuda.is_available() else 5,\n",
    "    n_dataset_seeds=2,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=3,\n",
    ")\n",
    "\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "print(\"Utilities initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777490a",
   "metadata": {},
   "source": [
    "## 3. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19da385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating correlated synthetic data...\n",
      "Data shapes: cont=(1024, 512, 3), targets=(1024, 512)\n",
      "Original targets - Mean: 50.95, Std: 4.22\n",
      "Original targets - Range: 31.31 to 72.22\n",
      "N = 512 | F = 3 | T = 1024\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data with meaningful correlations\n",
    "print(\"Generating correlated synthetic data...\")\n",
    "\n",
    "locs = np.linspace(-3, 3, CFG[\"n_locations\"]).astype(np.float32)\n",
    "\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locs,\n",
    "    n_time_steps=CFG[\"n_time_steps\"],\n",
    "    noise_std=CFG[\"noise_std\"],\n",
    "    eigenvalue=CFG[\"eigenvalue\"],\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=CFG[\"seed\"],\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True,\n",
    ")\n",
    "\n",
    "# 建 DataLoader：用 SPATIAL_CONFIG.training.batch_size 做保險\n",
    "bs = int(CFG.get(\"batch_size\", SPATIAL_CONFIG.training.batch_size))\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "# Extract tensors\n",
    "_, train_X, train_y = train_dataset.tensors\n",
    "_, val_X,   val_y   = val_dataset.tensors\n",
    "_, test_X,  test_y  = test_dataset.tensors\n",
    "\n",
    "p_dim  = train_X.shape[-1]\n",
    "T_full = cont_features.shape[0]\n",
    "N      = cont_features.shape[1]\n",
    "F      = p_dim\n",
    "\n",
    "print(f\"Data shapes: cont={cont_features.shape}, targets={targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")\n",
    "print(f\"N = {N} | F = {F} | T = {T_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb14f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize data characteristics\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Plot 1: Target distribution\n",
    "# axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "# axes[0, 0].set_title('Target Distribution')\n",
    "# axes[0, 0].set_xlabel('Target Value')\n",
    "# axes[0, 0].set_ylabel('Frequency')\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 2: Spatial pattern at first time step\n",
    "# axes[0, 1].plot(locs, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "# axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "# axes[0, 1].set_xlabel('Location')\n",
    "# axes[0, 1].set_ylabel('Target Value')\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 3: Temporal pattern at middle location\n",
    "# time_steps = np.arange(len(targets))\n",
    "# axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "# axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "# axes[1, 0].set_xlabel('Time Step')\n",
    "# axes[1, 0].set_ylabel('Target Value')\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 4: Feature correlations\n",
    "# feature_corrs = []\n",
    "# for i in range(cont_features.shape[-1]):\n",
    "#     corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "#     feature_corrs.append(corr)\n",
    "\n",
    "# axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "# axes[1, 1].set_title('Feature-Target Correlations')\n",
    "# axes[1, 1].set_xlabel('Feature Index')\n",
    "# axes[1, 1].set_ylabel('Correlation')\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fb6b7",
   "metadata": {},
   "source": [
    "## 4. Baseline Implementation\n",
    " OLS 改成 TFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016d8ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TFT baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                              | Type                             | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0      | train\n",
      "1  | val_metrics                       | MetricCollection                 | 0      | train\n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0      | train\n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0      | train\n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 3.7 M  | train\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 1.2 K  | train\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K  | train\n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | lstm_encoder                      | LSTM                             | 2.2 K  | train\n",
      "11 | lstm_decoder                      | LSTM                             | 2.2 K  | train\n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576    | train\n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K  | train\n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 808    | train\n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576    | train\n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K  | train\n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576    | train\n",
      "18 | output_layer                      | Linear                           | 147 K  | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.477    Total estimated model params size (MB)\n",
      "28305     Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 43/43 [01:56<00:00,  0.37it/s, train_loss=4.130]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 43/43 [01:56<00:00,  0.37it/s, train_loss=4.130]\n",
      "TFT Validation - RMSE: 1.1373, R²: -0.2951\n",
      "TFT Test       - RMSE: 1.1822, R²: -0.3789\n"
     ]
    }
   ],
   "source": [
    "# Compute true spatial basis for comparison\n",
    "phi_true = np.exp(-(locs**2))[:, None]\n",
    "phi_true /= np.linalg.norm(phi_true)\n",
    "sigma_true_spatial = CFG[\"eigenvalue\"] * (phi_true @ phi_true.T)\n",
    "\n",
    "print(\"Training TFT baseline...\")\n",
    "\n",
    "# === 用「經過 scaling 的張量」拼回全長，確保與後續 tft_trend(val_X) 尺度一致 ===\n",
    "# train/val/test 的切分是沿時間切，所以直接按時間串接即可\n",
    "y_all = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T, N)\n",
    "x_all = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T, N, F)\n",
    "\n",
    "T_full = x_all.shape[0]\n",
    "N      = x_all.shape[1]\n",
    "F      = x_all.shape[2]\n",
    "\n",
    "# Build pandas DataFrames for Darts\n",
    "time_index = pd.RangeIndex(start=0, stop=T_full, step=1)\n",
    "target_df  = pd.DataFrame(\n",
    "    y_all,  # 已經是 (T, N)\n",
    "    index=time_index,\n",
    "    columns=[f\"y_loc_{i}\" for i in range(N)]\n",
    ")\n",
    "\n",
    "cont_np       = x_all.reshape(T_full, N * F)\n",
    "locs_expanded = np.tile(locs.astype(np.float32), (T_full, 1))\n",
    "cov_df = pd.DataFrame(\n",
    "    np.concatenate([cont_np, locs_expanded], axis=1),\n",
    "    index=time_index,\n",
    "    columns=[f\"x{j}_loc_{i}\" for i in range(N) for j in range(F)] + [f\"loc_{i}\" for i in range(N)]\n",
    ")\n",
    "\n",
    "# Create TimeSeries\n",
    "target_ts    = TimeSeries.from_dataframe(target_df,  fill_missing_dates=False)\n",
    "covariate_ts = TimeSeries.from_dataframe(cov_df,     fill_missing_dates=False)\n",
    "\n",
    "# Train TFT（T_train 用 train_X 長度）\n",
    "T_train = len(train_X)\n",
    "input_chunk = CFG.get('tft_input_chunk_length') or min(32, max(8, T_train // 4))\n",
    "tft_model = TFTModel(\n",
    "    input_chunk_length=input_chunk,\n",
    "    output_chunk_length=int(CFG['tft_output_chunk_length']),\n",
    "    hidden_size=int(CFG['tft_hidden_size']),\n",
    "    lstm_layers=int(CFG['tft_lstm_layers']),\n",
    "    num_attention_heads=int(CFG['tft_num_attention_heads']),\n",
    "    full_attention=bool(CFG['tft_full_attention']),\n",
    "    hidden_continuous_size=int(CFG['tft_hidden_continuous_size']),\n",
    "    batch_size=int(CFG['tft_batch_size']),\n",
    "    n_epochs=int(CFG['tft_n_epochs']),\n",
    "    optimizer_kwargs=CFG['tft_optimizer_kwargs'],\n",
    "    add_relative_index=True,\n",
    "    force_reset=True,\n",
    "    save_checkpoints=False,\n",
    "    random_state=int(CFG['seed']),\n",
    "    dropout = 0.1\n",
    ")\n",
    "tft_model.fit(\n",
    "    series=target_ts[:T_train],\n",
    "    past_covariates=covariate_ts[:T_train],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Wrap TFT as trend model for Spatial Adapter\n",
    "tft_wrapper = TFTWrapper(tft_model=tft_model, num_locations=N, num_features=F)\n",
    "tft_trend = create_pretrained_trend_model(\n",
    "    pretrained_model=tft_wrapper,\n",
    "    input_shape=(None, N, F),   # ← 輸入：T x N x F\n",
    "    output_shape=(None, N, 1),  # ← 輸出：T x N x 1\n",
    ")\n",
    "tft_trend=tft_trend.to(DEVICE)\n",
    "# Residual eigen-basis from validation residuals\n",
    "with torch.no_grad():\n",
    "    y_tft_val  = tft_trend(val_X.to(DEVICE))\n",
    "    y_tft_test = tft_trend(test_X.to(DEVICE))\n",
    "\n",
    "# ✅ 壓成 2D 再做共變異\n",
    "residuals_val = (val_y.to(DEVICE) - y_tft_val).squeeze(-1)  # (T_val, N)\n",
    "covariance_matrix = residuals_val.T @ residuals_val         # (N, N)\n",
    "K = CFG[\"latent_dim\"]\n",
    "eigenvectors = torch.linalg.eigh(covariance_matrix).eigenvectors[:, -K:]  # top-K\n",
    "\n",
    "tft_basis = SpatialBasisLearner(CFG[\"n_locations\"], K).to(DEVICE)\n",
    "tft_basis.basis.data.copy_(eigenvectors)\n",
    "\n",
    "# TFT metrics\n",
    "rmse_tft_val,  mae_tft_val,  r2_tft_val  = compute_metrics(val_y.to(DEVICE),  y_tft_val)\n",
    "rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(test_y.to(DEVICE), y_tft_test)\n",
    "\n",
    "print(f\"TFT Validation - RMSE: {rmse_tft_val:.4f}, R²: {r2_tft_val:.4f}\")\n",
    "print(f\"TFT Test       - RMSE: {rmse_tft_test:.4f}, R²: {r2_tft_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285ba2",
   "metadata": {},
   "source": [
    "## 5. Main Experiment Function\n",
    "存檔從seed改為TFT_seed, \n",
    " 呼叫 OLS 的函式改成呼叫 TFT 函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d03e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_experiment(dataset_seed: int, n_trials: int = 30):\n",
    "    \"\"\"Run a complete experiment for one dataset seed (TFT baseline + Spatial Adapter).\"\"\"\n",
    "    import copy  # 為了 deepcopy trend/basis\n",
    "\n",
    "    log_root = Path(\"runs\") / f\"TFT_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Generate data for this seed ----\n",
    "    cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "        locs=locs,\n",
    "        n_time_steps=CFG[\"n_time_steps\"],\n",
    "        noise_std=CFG[\"noise_std\"],\n",
    "        eigenvalue=CFG[\"eigenvalue\"],\n",
    "        eta_rho=0.8,\n",
    "        f_rho=0.6,\n",
    "        global_mean=50.0,\n",
    "        feature_noise_std=0.1,\n",
    "        non_linear_strength=0.2,\n",
    "        seed=dataset_seed\n",
    "    )\n",
    "\n",
    "    # ---- Prepare datasets with scaling (沿用你原本 0.7/0.15 切分) ----\n",
    "    train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "        cat_features=cat_features,\n",
    "        cont_features=cont_features,\n",
    "        targets=targets,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        feature_scaler_type=\"standard\",\n",
    "        target_scaler_type=\"standard\",\n",
    "        fit_on_train_only=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=SPATIAL_CONFIG.training.batch_size, shuffle=True)\n",
    "\n",
    "    _, train_X, train_y = train_dataset.tensors\n",
    "    _, val_X,   val_y   = val_dataset.tensors\n",
    "    _, test_X,  test_y  = test_dataset.tensors\n",
    "\n",
    "    # ---- Shapes ----\n",
    "    T_full = cont_features.shape[0]\n",
    "    N      = cont_features.shape[1]\n",
    "    F      = train_X.shape[-1]\n",
    "\n",
    "    # ---- TFT Baseline（取代 OLS）----\n",
    "    print(\"Training TFT baseline...\")\n",
    "    time_index = pd.RangeIndex(start=0, stop=T_full, step=1)\n",
    "\n",
    "    target_df = pd.DataFrame(\n",
    "        targets.reshape(T_full, N),\n",
    "        index=time_index,\n",
    "        columns=[f\"y_loc_{i}\" for i in range(N)]\n",
    "    )\n",
    "    cont_np        = cont_features.reshape(T_full, N * F)\n",
    "    locs_expanded  = np.tile(locs.astype(np.float32), (T_full, 1))\n",
    "    cov_df = pd.DataFrame(\n",
    "        np.concatenate([cont_np, locs_expanded], axis=1),\n",
    "        index=time_index,\n",
    "        columns=[f\"x{j}_loc_{i}\" for i in range(N) for j in range(F)] + [f\"loc_{i}\" for i in range(N)]\n",
    "    )\n",
    "\n",
    "    target_ts    = TimeSeries.from_dataframe(target_df, fill_missing_dates=False)\n",
    "    covariate_ts = TimeSeries.from_dataframe(cov_df,   fill_missing_dates=False)\n",
    "\n",
    "    T_train = len(train_X)\n",
    "    input_chunk = CFG.get('tft_input_chunk_length') or min(64, max(8, T_train // 4))\n",
    "    tft_model = TFTModel(\n",
    "        input_chunk_length=input_chunk,\n",
    "        output_chunk_length=int(CFG.get('tft_output_chunk_length', 1)),\n",
    "        hidden_size=int(CFG.get('tft_hidden_size', 64)),\n",
    "        n_epochs=int(CFG.get('tft_n_epochs', 30)),\n",
    "        dropout=float(CFG.get('tft_dropout', 0.1)),\n",
    "        batch_size=int(CFG.get('tft_batch_size', CFG.get('batch_size', 64))),\n",
    "        random_state=int(CFG.get('seed', 42)),\n",
    "        add_relative_index=True,\n",
    "        force_reset=True,\n",
    "        save_checkpoints=False,\n",
    "    )\n",
    "    tft_model.fit(\n",
    "        series=target_ts[:T_train],\n",
    "        past_covariates=covariate_ts[:T_train],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Wrap TFT as trend model for Spatial Adapter\n",
    "    tft_wrapper = TFTWrapper(tft_model=tft_model, num_locations=N, num_features=F)\n",
    "\n",
    "    trend_base = create_pretrained_trend_model(\n",
    "        pretrained_model=tft_wrapper,\n",
    "        input_shape=(None, N, F),   # 時間×地點×特徵\n",
    "        output_shape=(None, N, 1),  # 時間×地點×1\n",
    "    )\n",
    "    trend_base = trend_base.to(DEVICE)  # ← 這裡再搬到裝置\n",
    "\n",
    "    # Baseline metrics\n",
    "    with torch.no_grad():\n",
    "        y_tft_val  = trend_base(val_X.to(DEVICE))\n",
    "        y_tft_test = trend_base(test_X.to(DEVICE))\n",
    "    rmse_tft,  mae_tft,  r2_tft    = compute_metrics(val_y.to(DEVICE),  y_tft_val)\n",
    "    rmse_tft_t, mae_tft_t, r2_tft_t = compute_metrics(test_y.to(DEVICE), y_tft_test)\n",
    "\n",
    "    # ---- 用 TFT 驗證殘差初始化空間基底（top-K eigenvectors）----\n",
    "    with torch.no_grad():\n",
    "        resid_val = val_y.to(DEVICE) - y_tft_val\n",
    "    covM = resid_val.T @ resid_val\n",
    "    K = CFG[\"latent_dim\"]\n",
    "    eigvecs = torch.linalg.eigh(covM).eigenvectors[:, -K:]  # top-K\n",
    "\n",
    "    basis_init = SpatialBasisLearner(N, K).to(DEVICE)\n",
    "    basis_init.basis.data.copy_(eigvecs)\n",
    "\n",
    "    # ---- 清空 cache，準備 bootstrap ----\n",
    "    cache.clear()\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # ---- Bootstrap: tau1=tau2=0（Unregularized）----\n",
    "    boot_trend = copy.deepcopy(trend_base).to(DEVICE)\n",
    "    boot_basis = SpatialBasisLearner(N, K).to(DEVICE)\n",
    "    boot_basis.basis.data.copy_(basis_init.basis.data)\n",
    "\n",
    "    boot_writer = SummaryWriter(log_dir=log_root / \"bootstrap\")\n",
    "    boot = SpatialNeuralAdapter(\n",
    "        boot_trend,\n",
    "        boot_basis,\n",
    "        train_loader,\n",
    "        val_cont=val_X.to(DEVICE),\n",
    "        val_y=val_y.to(DEVICE),\n",
    "        locs=locs,\n",
    "        config=SPATIAL_CONFIG,\n",
    "        device=DEVICE,\n",
    "        writer=boot_writer,\n",
    "        tau1=0.0,\n",
    "        tau2=0.0,\n",
    "    )\n",
    "    boot.pretrain_trend(epochs=5)\n",
    "    boot.init_basis_dense()\n",
    "    boot.run()\n",
    "    cache.store(0.0, 0.0, boot_trend.state_dict(), boot_basis.state_dict())\n",
    "    boot_writer.close()\n",
    "\n",
    "    # Unreg predictions\n",
    "    y_boot_val  = boot.predict(val_X.to(DEVICE),  val_y.to(DEVICE))\n",
    "    rmse_boot,  mae_boot,  r2_boot  = compute_metrics(val_y.to(DEVICE),  y_boot_val)\n",
    "    y_boot_test = boot.predict(test_X.to(DEVICE), test_y.to(DEVICE))\n",
    "    rmse_boot_t, mae_boot_t, r2_boot_t = compute_metrics(test_y.to(DEVICE), y_boot_test)\n",
    "\n",
    "    # Clean up bootstrap models\n",
    "    del boot_trend, boot_basis, boot\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # ---- Optuna objective（搜尋 tau1, tau2）----\n",
    "    def objective(trial):\n",
    "        dev  = DEVICE\n",
    "        tau1 = trial.suggest_float(\"tau1\", 1e-4, 1e8, log=True)\n",
    "        tau2 = trial.suggest_float(\"tau2\", 1e-4, 1e8, log=True)\n",
    "\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        trend = copy.deepcopy(trend_base).to(dev)\n",
    "        basis = SpatialBasisLearner(N, K).to(dev)\n",
    "        basis.basis.data.copy_(basis_init.basis.data)\n",
    "\n",
    "        # 若 cache 內有鄰近的起點，載入\n",
    "        cache.load_nearest(trend, basis, tau1, tau2)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=log_root / f\"trial_{trial.number:03d}\")\n",
    "        trainer = SpatialNeuralAdapter(\n",
    "            trend,\n",
    "            basis,\n",
    "            train_loader,\n",
    "            val_cont=val_X.to(dev),\n",
    "            val_y=val_y.to(dev),\n",
    "            locs=locs,\n",
    "            config=SPATIAL_CONFIG,\n",
    "            device=dev,\n",
    "            writer=writer,\n",
    "            tau1=tau1,\n",
    "            tau2=tau2,\n",
    "        )\n",
    "        trainer.pretrain_trend(epochs=3)\n",
    "        trainer.init_basis_dense()\n",
    "        trainer.run()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = trainer.predict(val_X.to(dev), val_y.to(dev))\n",
    "        rmse, mae, r2 = compute_metrics(val_y.to(dev), y_pred)\n",
    "\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\", r2)\n",
    "\n",
    "        writer.close()\n",
    "        cache.store(tau1, tau2, trend.state_dict(), basis.state_dict())\n",
    "\n",
    "        del trend, basis, trainer, y_pred\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"spatial_adapter_ds{dataset_seed}\",\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=MedianPruner(n_warmup_steps=5),\n",
    "        load_if_exists=False,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    # ---- Best results ----\n",
    "    best     = study.best_trial\n",
    "    rmse_opt = best.user_attrs[\"rmse\"]\n",
    "    mae_opt  = best.user_attrs[\"mae\"]\n",
    "    r2_opt   = best.user_attrs[\"r2\"]\n",
    "    tau1_opt = best.params[\"tau1\"]\n",
    "    tau2_opt = best.params[\"tau2\"]\n",
    "    best_no  = best.number\n",
    "\n",
    "    # ---- Test best model（用 cache 中的最佳權重還原 trend/basis，做簡單投影評估）----\n",
    "    dev_best = DEVICE\n",
    "    trend_best = copy.deepcopy(trend_base).to(dev_best)\n",
    "    basis_best = SpatialBasisLearner(N, K).to(dev_best)\n",
    "    # 從 cache 還原（若不存在會 KeyError；正常來說已 store）\n",
    "    sd_t, sd_b = cache.cache[(tau1_opt, tau2_opt)]\n",
    "    trend_best.load_state_dict(sd_t)\n",
    "    basis_best.load_state_dict(sd_b)\n",
    "\n",
    "    trend_best.eval()\n",
    "    basis_best.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test = test_X.to(dev_best)\n",
    "        y_test = test_y.to(dev_best)\n",
    "        y_trend = trend_best(X_test)\n",
    "        # 將殘差投影到基底子空間，再加回趨勢（與你原本做法一致）\n",
    "        residual = y_test - y_trend\n",
    "        y_basis  = (residual @ basis_best.basis) @ basis_best.basis.T\n",
    "        y_reg_test = y_trend + y_basis\n",
    "    rmse_test, mae_test, r2_test = compute_metrics(y_test, y_reg_test)\n",
    "\n",
    "    # ---- Write results to CSV ----\n",
    "    csv_path = Path(\"metrics_summary.csv\")\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\n",
    "                \"seed\", \"model\", \"trial\", \"tau1\", \"tau2\",\n",
    "                \"rmse_val\", \"mae_val\", \"r2_val\",\n",
    "                \"rmse_test\", \"mae_test\", \"r2_test\"\n",
    "            ])\n",
    "\n",
    "        # Baseline（TFT）\n",
    "        w.writerow([\n",
    "            dataset_seed, \"TFT\", \"\", \"\", \"\",\n",
    "            f\"{rmse_tft:.6f}\", f\"{mae_tft:.6f}\", f\"{r2_tft:.6f}\",\n",
    "            f\"{rmse_tft_t:.6f}\", f\"{mae_tft_t:.6f}\", f\"{r2_tft_t:.6f}\"\n",
    "        ])\n",
    "        # Unregularized\n",
    "        w.writerow([\n",
    "            dataset_seed, \"Unreg\", \"\", \"0\", \"0\",\n",
    "            f\"{rmse_boot:.6f}\", f\"{mae_boot:.6f}\", f\"{r2_boot:.6f}\",\n",
    "            f\"{rmse_boot_t:.6f}\", f\"{mae_boot_t:.6f}\", f\"{r2_boot_t:.6f}\"\n",
    "        ])\n",
    "        # Regularized\n",
    "        w.writerow([\n",
    "            dataset_seed, \"Reg\", best_no, f\"{tau1_opt:.6g}\", f\"{tau2_opt:.6g}\",\n",
    "            f\"{rmse_opt:.6f}\", f\"{mae_opt:.6f}\", f\"{r2_opt:.6f}\",\n",
    "            f\"{rmse_test:.6f}\", f\"{mae_test:.6f}\", f\"{r2_test:.6f}\"\n",
    "        ])\n",
    "\n",
    "    print(\n",
    "        f\"Dataset {dataset_seed}:  \"\n",
    "        f\"TFT RMSE={rmse_tft:.3f} | \"\n",
    "        f\"Unreg RMSE={rmse_boot:.3f} | \"\n",
    "        f\"Reg RMSE={rmse_opt:.3f} (test {rmse_test:.3f})\"\n",
    "    )\n",
    "\n",
    "    # 為了相容外部程式若仍取 'ols' key，這裡沿用 'ols'，內容是 TFT 指標\n",
    "    return {\n",
    "        'ols':   {'rmse_val': rmse_tft,  'rmse_test': rmse_tft_t,  'r2_val': r2_tft,  'r2_test': r2_tft_t},\n",
    "        'unreg': {'rmse_val': rmse_boot, 'rmse_test': rmse_boot_t, 'r2_val': r2_boot, 'r2_test': r2_boot_t},\n",
    "        'reg':   {'rmse_val': rmse_opt,  'rmse_test': rmse_test,   'r2_val': r2_opt,  'r2_test': r2_test,\n",
    "                  'tau1': tau1_opt, 'tau2': tau2_opt}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6642f",
   "metadata": {},
   "source": [
    "## 6. Run Full Experiment Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ababa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting experiment for seed 1\n",
      "Training TFT baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                              | Type                             | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0      | train\n",
      "1  | val_metrics                       | MetricCollection                 | 0      | train\n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0      | train\n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0      | train\n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 1.8 M  | train\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 528    | train\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K  | train\n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | lstm_encoder                      | LSTM                             | 2.2 K  | train\n",
      "11 | lstm_decoder                      | LSTM                             | 2.2 K  | train\n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576    | train\n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K  | train\n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 676    | train\n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576    | train\n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K  | train\n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576    | train\n",
      "18 | output_layer                      | Linear                           | 147 K  | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.771     Total estimated model params size (MB)\n",
      "38557     Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  95%|█████████▌| 39/41 [02:56<00:09,  0.22it/s, train_loss=407.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:344\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1328\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1305\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1306\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1326\u001b[39m \n\u001b[32m   1327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/torch/optim/adam.py:226\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03mhook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:137\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._zero_grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_zero_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:226\u001b[39m, in \u001b[36m_AutomaticOptimization._make_zero_grad_fn.<locals>.zero_grad_fn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28mself\u001b[39m._on_before_zero_grad(optimizer)\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_zero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:304\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_zero_grad\u001b[39m\u001b[34m(self, batch_idx, optimizer)\u001b[39m\n\u001b[32m    303\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_zero_grad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.zero_grad.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1351\u001b[39m, in \u001b[36mLightningModule.optimizer_zero_grad\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer)\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Override this method to change the default behaviour of ``optimizer.zero_grad()``.\u001b[39;00m\n\u001b[32m   1332\u001b[39m \n\u001b[32m   1333\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1349\u001b[39m \n\u001b[32m   1350\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1351\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/torch/_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/torch/optim/optimizer.py:1032\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m     p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EXPERIMENT_TRIALS_CONFIG[\u001b[33m'\u001b[39m\u001b[33mseed_range_start\u001b[39m\u001b[33m'\u001b[39m], EXPERIMENT_TRIALS_CONFIG[\u001b[33m'\u001b[39m\u001b[33mseed_range_end\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting experiment for seed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     results = \u001b[43mrun_one_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEXPERIMENT_TRIALS_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_trials_per_seed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     all_results.append(results)\n\u001b[32m      6\u001b[39m     cache.clear()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mrun_one_experiment\u001b[39m\u001b[34m(dataset_seed, n_trials)\u001b[39m\n\u001b[32m     65\u001b[39m input_chunk = CFG.get(\u001b[33m'\u001b[39m\u001b[33mtft_input_chunk_length\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[32m64\u001b[39m, \u001b[38;5;28mmax\u001b[39m(\u001b[32m8\u001b[39m, T_train // \u001b[32m4\u001b[39m))\n\u001b[32m     66\u001b[39m tft_model = TFTModel(\n\u001b[32m     67\u001b[39m     input_chunk_length=input_chunk,\n\u001b[32m     68\u001b[39m     output_chunk_length=\u001b[38;5;28mint\u001b[39m(CFG.get(\u001b[33m'\u001b[39m\u001b[33mtft_output_chunk_length\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     save_checkpoints=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     77\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mtft_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_ts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_covariates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcovariate_ts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     82\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Wrap TFT as trend model for Spatial Adapter\u001b[39;00m\n\u001b[32m     85\u001b[39m tft_wrapper = TFTWrapper(tft_model=tft_model, num_locations=N, num_features=F)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/darts/utils/torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/darts/models/forecasting/torch_forecasting_model.py:944\u001b[39m, in \u001b[36mTorchForecastingModel.fit\u001b[39m\u001b[34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, dataloader_kwargs, sample_weight, val_sample_weight, stride)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;28msuper\u001b[39m().fit(\n\u001b[32m    940\u001b[39m     series=seq2series(series),\n\u001b[32m    941\u001b[39m     past_covariates=seq2series(past_covariates),\n\u001b[32m    942\u001b[39m     future_covariates=seq2series(future_covariates),\n\u001b[32m    943\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/darts/utils/torch.py:94\u001b[39m, in \u001b[36mrandom_method.<locals>.decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[32m     93\u001b[39m     manual_seed(random_instance.randint(\u001b[32m0\u001b[39m, high=MAX_TORCH_SEED_VALUE))\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecorated\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/darts/models/forecasting/torch_forecasting_model.py:1127\u001b[39m, in \u001b[36mTorchForecastingModel.fit_from_dataset\u001b[39m\u001b[34m(self, train_dataset, val_dataset, trainer, verbose, epochs, dataloader_kwargs)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_from_dataset\u001b[39m(\n\u001b[32m   1076\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1082\u001b[39m     dataloader_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1083\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mTorchForecastingModel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1084\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[33;03m    Train the model with a specific :class:`darts.utils.data.TorchTrainingDataset` instance.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m \u001b[33;03m        Fitted model.\u001b[39;00m\n\u001b[32m   1126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/darts/models/forecasting/torch_forecasting_model.py:1316\u001b[39m, in \u001b[36mTorchForecastingModel._train\u001b[39m\u001b[34m(self, trainer, model, train_loader, val_loader)\u001b[39m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28mself\u001b[39m.load_ckpt_path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._requires_training:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m   1323\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/geospatial-neural-adapter/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for seed in range(EXPERIMENT_TRIALS_CONFIG['seed_range_start'], EXPERIMENT_TRIALS_CONFIG['seed_range_end']):\n",
    "    print(f\"\\nStarting experiment for seed {seed}\")\n",
    "    results = run_one_experiment(seed, n_trials=EXPERIMENT_TRIALS_CONFIG['n_trials_per_seed'])\n",
    "    all_results.append(results)\n",
    "    cache.clear()\n",
    "    clear_gpu_memory()\n",
    "    print(f\"✅ Completed seed {seed}\")\n",
    "\n",
    "print(\"\\n🎉 All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0388ee",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization\n",
    "\n",
    "還未完全改完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_df = pd.read_csv(\"metrics_summary.csv\")\n",
    "print(\"📊 Results Summary (mean across seeds):\")\n",
    "print(results_df.groupby('model')[['rmse_val', 'rmse_test', 'r2_val', 'r2_test']].mean())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_val', ax=axes[0,0])\n",
    "axes[0,0].set_title('Validation RMSE'); axes[0,0].set_ylabel('RMSE'); axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_test', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test RMSE'); axes[0,1].set_ylabel('RMSE'); axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "sns.boxplot(data=results_df, x='model', y='r2_val', ax=axes[1,0])\n",
    "axes[1,0].set_title('Validation R²'); axes[1,0].set_ylabel('R²'); axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='r2_test', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test R²'); axes[1,1].set_ylabel('R²'); axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Show best hyperparameters for regularized model\n",
    "reg_results = results_df[results_df['model'] == 'Reg']\n",
    "print(\"\\n🔧 Best Hyperparameters for Regularized Model (first 10 rows):\")\n",
    "print(reg_results[['tau1', 'tau2', 'rmse_val', 'rmse_test']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison summary\n",
    "print(\"=== Performance Comparison Summary ===\")\n",
    "tft_mean_rmse   = results_df[results_df['model'] == 'TFT']['rmse_test'].mean()\n",
    "unreg_mean_rmse = results_df[results_df['model'] == 'Unreg']['rmse_test'].mean()\n",
    "reg_mean_rmse   = results_df[results_df['model'] == 'Reg']['rmse_test'].mean()\n",
    "\n",
    "print(f\"TFT (baseline) - Mean Test RMSE: {tft_mean_rmse:.4f}\")\n",
    "print(f\"Unregularized  - Mean Test RMSE: {unreg_mean_rmse:.4f} ({(1 - unreg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement)\")\n",
    "print(f\"Regularized    - Mean Test RMSE: {reg_mean_rmse:.4f} ({(1 - reg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement)\")\n",
    "\n",
    "from scipy import stats\n",
    "wide = results_df.pivot_table(index='seed', columns='model', values='rmse_test', aggfunc='mean')\n",
    "paired = wide[['TFT', 'Reg']].dropna()\n",
    "t_stat, p_value = stats.ttest_rel(paired['TFT'], paired['Reg'])\n",
    "\n",
    "print(f\"\\nStatistical Test (TFT vs Regularized): t={t_stat:.4f}, p={p_value:.4f}, \"\n",
    "      f\"significant: {'Yes' if p_value < 0.05 else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
