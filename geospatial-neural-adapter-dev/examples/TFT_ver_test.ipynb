{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS vs. Spatial Adapter Comparison with Tuning Parameter Selection\n",
    "\n",
    "This notebook implements a comprehensive comparison between:\n",
    "1. **TFT** - Linear baseline (no spatial term)\n",
    "2. **Unregularized Spatial Adapter** - Neural spatial model without regularization\n",
    "3. **Regularized Spatial Adapter** - Neural spatial model with optimized tau1, tau2 parameters\n",
    "\n",
    "The experiment uses Optuna for hyperparameter optimization and evaluates performance across multiple random seeds.\n",
    "\n",
    "my work: ols æ›æˆ TFT ç„¶å¾ŒåŸ·è¡Œæ¨¡æ“¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345997df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "# ==== New: Darts / TFT as the backbone (replaces OLS) ====\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "\n",
    "from geospatial_neural_adapter.cpp_extensions import estimate_covariance\n",
    "from geospatial_neural_adapter.utils.experiment import log_covariance_and_basis\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    ModelCache,\n",
    "    clear_gpu_memory,\n",
    "    create_experiment_config,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "from geospatial_neural_adapter.models.spatial_basis_learner import SpatialBasisLearner\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapter\n",
    "from geospatial_neural_adapter.models.trend_model import TrendModel\n",
    "from geospatial_neural_adapter.models.wrapper_examples.tft_wrapper import TFTWrapper\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import (\n",
    "    prepare_all_with_scaling,\n",
    "    denormalize_predictions,\n",
    ")\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Imports successful (TFT backbone enabled; all OLS utilities removed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f86c36",
   "metadata": {},
   "source": [
    "## 1. Parameter Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_time_steps': 1024,\n",
    "    'n_locations': 512,\n",
    "    'noise_std': 4.0,\n",
    "    'eigenvalue': 16.0,\n",
    "    'latent_dim': 1,\n",
    "    'ckpt_dir': \"admm_bcd_ckpts\",\n",
    "}\n",
    "\n",
    "# ==== New: Split & TFT configs (baseline uses TFT, no spatial term) ====\n",
    "SPLIT_CONFIG = {\n",
    "    \"train_ratio\": 0.70,\n",
    "    \"val_ratio\": 0.15,   # å‰©é¤˜å³ç‚º test_ratio\n",
    "}\n",
    "\n",
    "TFT_CONFIG = {\n",
    "    \"input_chunk_length\": 48,\n",
    "    \"output_chunk_length\": 1,\n",
    "    \"hidden_size\": 64,\n",
    "    \"lstm_layers\": 1,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"dropout\": 0.10,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 30,\n",
    "    \"random_state\": 42,\n",
    "    \"add_relative_index\": True,\n",
    "}\n",
    "\n",
    "PL_TRAINER_KWARGS = (\n",
    "    {\"accelerator\": \"gpu\", \"devices\": 1,\n",
    "     \"logger\": True,                 # â† é–‹å•Ÿ logger\n",
    "     \"enable_progress_bar\": True,   # â† ç¹¼çºŒé—œé–‰é€²åº¦æ¢\n",
    "     \"enable_model_summary\": False,\n",
    "     \"num_sanity_val_steps\": 0}\n",
    "    if torch.cuda.is_available()\n",
    "    else {\"accelerator\": \"cpu\", \"devices\": 1,\n",
    "          \"logger\": True,\n",
    "          \"enable_progress_bar\": True,\n",
    "          \"enable_model_summary\": False,\n",
    "          \"num_sanity_val_steps\": 0}\n",
    ")\n",
    "\n",
    "# Spatial Neural Adapter Configuration using dataclasses\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import (\n",
    "    SpatialNeuralAdapterConfig, ADMMConfig, TrainingConfig, BasisConfig\n",
    ")\n",
    "\n",
    "# ADMM Configuration\n",
    "admm_config = ADMMConfig(\n",
    "    rho=1.0,            # Base ADMM penalty parameter\n",
    "    dual_momentum=0.2,  # Dual variable momentum\n",
    "    max_iters=3000,     # Maximum ADMM iterations\n",
    "    min_outer=20,       # Minimum outer iterations before convergence check\n",
    "    tol=1e-4,           # Convergence tolerance\n",
    ")\n",
    "\n",
    "# Training Configuration\n",
    "# æé†’ï¼šé€™è£¡çš„ lr_mu åŸå…ˆæ˜¯çµ¦ OLS/ç·šæ€§è¶¨å‹¢ç”¨ï¼›æ”¹ç‚º TFT å¾Œï¼Œè¶¨å‹¢å­¸ç¿’æ”¹ç”± TFT æœ¬èº«è™•ç†ã€‚\n",
    "# è‹¥ä½ çš„ SpatialNeuralAdapter å…§éƒ¨ä»åƒè€ƒè©²æ¬„ä½ï¼Œå…ˆä¿ç•™ä»¥ç¶­æŒç›¸å®¹ï¼›å¯¦éš›å­¸ç¿’ç‡ä»¥ TFT_CONFIG ç‚ºæº–ã€‚\n",
    "training_config = TrainingConfig(\n",
    "    lr_mu=1e-2,           # ä¿ç•™ä»¥ç¶­æŒç›¸å®¹ï¼Œå¯¦éš›è¶¨å‹¢ç”± TFT å­¸\n",
    "    batch_size=64,        # Adapter theta step æ‰¹æ¬¡\n",
    "    pretrain_epochs=5,    # Default pretraining epochsï¼ˆè‹¥æµç¨‹ç”¨å¾—åˆ°ï¼‰\n",
    "    use_mixed_precision=False,\n",
    ")\n",
    "\n",
    "# Basis Configuration\n",
    "basis_config = BasisConfig(\n",
    "    phi_every=5,        # Update basis every N iterations\n",
    "    phi_freeze=200,     # Stop updating basis after N iterations\n",
    "    matrix_reg=1e-6,    # Matrix regularization for basis update\n",
    "    irl1_max_iters=10,  # IRLâ‚ maximum iterations\n",
    "    irl1_eps=1e-6,      # IRLâ‚ epsilon\n",
    "    irl1_tol=5e-4,      # IRLâ‚ inner tolerance\n",
    ")\n",
    "\n",
    "# Complete Spatial Neural Adapter Configuration\n",
    "SPATIAL_CONFIG = SpatialNeuralAdapterConfig(\n",
    "    admm=admm_config,\n",
    "    training=training_config,\n",
    "    basis=basis_config\n",
    ")\n",
    "\n",
    "# Legacy config dict for backward compatibility (if needed)\n",
    "CFG = SPATIAL_CONFIG.to_dict()\n",
    "CFG.update(EXPERIMENT_CONFIG)\n",
    "CFG.update({\n",
    "    \"split\": SPLIT_CONFIG,\n",
    "    \"tft\": TFT_CONFIG,\n",
    "})\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "torch.manual_seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "Path(EXPERIMENT_CONFIG[\"ckpt_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info['device'] == 'cuda':\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Experiment Configuration ===\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Split Configuration ===\")\n",
    "for k, v in SPLIT_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== TFT Configuration (Baseline) ===\")\n",
    "for k, v in TFT_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Spatial Neural Adapter Configuration ===\")\n",
    "SPATIAL_CONFIG.log_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a32565",
   "metadata": {},
   "source": [
    "## 2. Initialize Utilities\n",
    " ç›®å‰æ¸›å°‘è·‘çš„æ¬¡æ•¸ ç¢ºèªå®Œä¹‹å¾Œè¦è·‘å®Œæ•´çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model cache for hyperparameter optimization\n",
    "cache = ModelCache()\n",
    "\n",
    "# Create experiment configuration\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=20 if torch.cuda.is_available() else 50,\n",
    "    n_dataset_seeds=10,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=11,\n",
    ")\n",
    "\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "print(\"Utilities initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777490a",
   "metadata": {},
   "source": [
    "## 3. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19da385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with meaningful correlations\n",
    "print(\"Generating correlated synthetic data...\")\n",
    "\n",
    "locs = np.linspace(-3, 3, CFG[\"n_locations\"])\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locs,\n",
    "    n_time_steps=CFG[\"n_time_steps\"],\n",
    "    noise_std=CFG[\"noise_std\"],\n",
    "    eigenvalue=CFG[\"eigenvalue\"],\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=CFG[\"seed\"]\n",
    ")\n",
    "\n",
    "# Prepare datasets with scaling\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=SPLIT_CONFIG[\"train_ratio\"],\n",
    "    val_ratio=SPLIT_CONFIG[\"val_ratio\"],\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "\n",
    "# ï¼ˆä¿ç•™ï¼‰çµ¦ Adapter å…§éƒ¨ batch å¯èƒ½æœƒç”¨åˆ°\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG[\"tft\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Extract tensors (scaled)\n",
    "_, train_X, train_y = train_dataset.tensors\n",
    "_, val_X,   val_y   = val_dataset.tensors\n",
    "_, test_X,  test_y  = test_dataset.tensors\n",
    "\n",
    "if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "\n",
    "# ===== New: å°‡ç¶“é scaling çš„ y ä¸²å›å®Œæ•´æ™‚é–“ï¼Œä¾› Darts/TFT ä½¿ç”¨ =====\n",
    "# åŸæœ¬ OLS æœƒç”¨é€™äº›å¼µé‡ç›´æ¥åšå›æ­¸ï¼›ç¾åœ¨æ”¹æˆçµ„æˆå¤šè®Šé‡ TimeSeries\n",
    "y_all_scaled = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T_full, N)\n",
    "x_all_scaled = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T_full, N, F)\n",
    "\n",
    "T_full = y_all_scaled.shape[0]\n",
    "N      = y_all_scaled.shape[1]\n",
    "F      = x_all_scaled.shape[2]\n",
    "\n",
    "# ä¾æ™‚é–“åˆ‡åˆ†ç´¢å¼•ï¼ˆèˆ‡ä¸Šé¢çš„ ratio ä¸€è‡´ï¼‰\n",
    "train_T = int(T_full * SPLIT_CONFIG[\"train_ratio\"])\n",
    "val_T   = int(T_full * (SPLIT_CONFIG[\"train_ratio\"] + SPLIT_CONFIG[\"val_ratio\"]))\n",
    "test_T  = T_full\n",
    "\n",
    "# å»ºç«‹ Darts å¤šè®Šé‡ TimeSeriesï¼ˆä»¥ y çš„æ­·å²ä½œç‚ºå”¯ä¸€è¼¸å…¥ï¼›è‹¥è¦åŠ å…¥ covariates ä¹‹å¾Œå†æ¥ï¼‰\n",
    "series_all   = TimeSeries.from_values(y_all_scaled)      # shape (T_full, N)\n",
    "series_train = series_all[:train_T]\n",
    "series_val   = series_all[train_T:val_T]\n",
    "series_test  = series_all[val_T:]\n",
    "\n",
    "# ä¿ç•™çµ¦å¾ŒçºŒè©•ä¼°å°é½Š\n",
    "y_true_future = y_all_scaled[train_T:]   # (T_val+T_test, N)\n",
    "\n",
    "#ï¼ˆä¿ç•™ï¼‰åŸæœ¬ä½ å°±æœƒå°çš„è³‡è¨Š\n",
    "p_dim = train_X.shape[-1]\n",
    "print(f\"Data shapes (cont_features, targets): {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")\n",
    "print(f\"Feature dimension: {p_dim}\")\n",
    "print(f\"T_full={T_full}, N={N}, F={F} | splits: train={train_T}, val={val_T-train_T}, test={test_T-val_T}\")\n",
    "print(\"Scaled series prepared for TFT (Darts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize data characteristics\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Plot 1: Target distribution\n",
    "# axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "# axes[0, 0].set_title('Target Distribution')\n",
    "# axes[0, 0].set_xlabel('Target Value')\n",
    "# axes[0, 0].set_ylabel('Frequency')\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 2: Spatial pattern at first time step\n",
    "# axes[0, 1].plot(locs, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "# axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "# axes[0, 1].set_xlabel('Location')\n",
    "# axes[0, 1].set_ylabel('Target Value')\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 3: Temporal pattern at middle location\n",
    "# time_steps = np.arange(len(targets))\n",
    "# axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "# axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "# axes[1, 0].set_xlabel('Time Step')\n",
    "# axes[1, 0].set_ylabel('Target Value')\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 4: Feature correlations\n",
    "# feature_corrs = []\n",
    "# for i in range(cont_features.shape[-1]):\n",
    "#     corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "#     feature_corrs.append(corr)\n",
    "\n",
    "# axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "# axes[1, 1].set_title('Feature-Target Correlations')\n",
    "# axes[1, 1].set_xlabel('Feature Index')\n",
    "# axes[1, 1].set_ylabel('Correlation')\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fb6b7",
   "metadata": {},
   "source": [
    "## 4. Baseline Implementation\n",
    " OLS æ”¹æˆ TFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== TFT Baseline (tft_demo style; no spatial term at all) ====\n",
    "# print(\"Training TFT baseline (demo-only, no spatial)...\")\n",
    "\n",
    "# pl.seed_everything(CFG[\"tft\"][\"random_state\"])\n",
    "\n",
    "# # 1) Per-location target & covariates\n",
    "# series_list, past_cov_list, future_cov_list = [], [], []\n",
    "# for i in range(N):\n",
    "#     s_i   = TimeSeries.from_values(y_all_scaled[:, i])       # target (T,) -> TS\n",
    "#     cov_i = TimeSeries.from_values(x_all_scaled[:, i, :])    # cont features (T, F) -> TS\n",
    "#     series_list.append(s_i)\n",
    "#     past_cov_list.append(cov_i)\n",
    "#     future_cov_list.append(cov_i)  # è‹¥æœªä¾†ä¸å¯å¾—ï¼Œä¹‹å¾ŒæŠŠ future_covariates ç›¸é—œåƒæ•¸ç§»é™¤å³å¯\n",
    "\n",
    "# # 2) Split\n",
    "# series_train_list = [s[:train_T]      for s in series_list]\n",
    "# series_val_list   = [s[train_T:val_T] for s in series_list]\n",
    "# past_cov_train    = [c[:train_T]      for c in past_cov_list]\n",
    "# future_cov_train  = [c[:train_T]      for c in future_cov_list]\n",
    "# val_past_covariates   = [c[train_T:val_T] for c in past_cov_list]\n",
    "# val_future_covariates = [c[train_T:val_T] for c in future_cov_list]\n",
    "\n",
    "# # 3) Train TFT\n",
    "# tft = TFTModel(\n",
    "#     **TFT_CONFIG,                      # è¦å« add_relative_index=True æˆ–ç­‰åƒ¹ encoders\n",
    "#     pl_trainer_kwargs=PL_TRAINER_KWARGS\n",
    "# )\n",
    "# tft.fit(\n",
    "#     series=series_train_list,\n",
    "#     val_series=series_val_list,\n",
    "#     past_covariates=past_cov_train,\n",
    "#     future_covariates=future_cov_train,\n",
    "#     val_past_covariates=val_past_covariates,       \n",
    "#     val_future_covariates=val_future_covariates,   \n",
    "#     verbose=True\n",
    "# )\n",
    "# # 4) One-step rolling forecast via historical_forecasts\n",
    "# val_len  = val_y.shape[0]\n",
    "# test_len = test_y.shape[0]\n",
    "\n",
    "# yhat_val_list, yhat_test_list = [], []\n",
    "# for i in range(N):\n",
    "#     yhat_i = tft.historical_forecasts(\n",
    "#         series=series_list[i],\n",
    "#         past_covariates=past_cov_list[i],\n",
    "#         future_covariates=future_cov_list[i],\n",
    "#         start=train_T,\n",
    "#         forecast_horizon=1,\n",
    "#         retrain=False,\n",
    "#         verbose=False\n",
    "#     ).values()  # (Tval+Ttest,)\n",
    "#     yhat_val_list.append(yhat_i[:val_len])\n",
    "#     yhat_test_list.append(yhat_i[val_len:val_len + test_len])\n",
    "\n",
    "# # back to (T, N) in scaled space\n",
    "# yhat_val  = np.stack(yhat_val_list,  axis=1)\n",
    "# yhat_test = np.stack(yhat_test_list, axis=1)\n",
    "\n",
    "# # 5) Metrics on original scale (no spatial components involved)\n",
    "# y_val_den       = denormalize_predictions(val_y.squeeze(-1),  preprocessor)               # (Tval, N)\n",
    "# y_test_den      = denormalize_predictions(test_y.squeeze(-1), preprocessor)               # (Ttest, N)\n",
    "# yhat_val_den_np = denormalize_predictions(torch.from_numpy(yhat_val).float(),  preprocessor)\n",
    "# yhat_test_den_np= denormalize_predictions(torch.from_numpy(yhat_test).float(), preprocessor)\n",
    "\n",
    "# def to_tensor_2d(x):\n",
    "#     if isinstance(x, np.ndarray): x = torch.from_numpy(x)\n",
    "#     if x.ndim == 3: x = x.squeeze(-1)\n",
    "#     return x.float()\n",
    "\n",
    "# y_val_den_t     = to_tensor_2d(y_val_den)\n",
    "# y_test_den_t    = to_tensor_2d(y_test_den)\n",
    "# yhat_val_den_t  = to_tensor_2d(yhat_val_den_np)\n",
    "# yhat_test_den_t = to_tensor_2d(yhat_test_den_np)\n",
    "\n",
    "# rmse_tft_val,  mae_tft_val,  r2_tft_val  = compute_metrics(y_val_den_t,  yhat_val_den_t)\n",
    "# rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(y_test_den_t, yhat_test_den_t)\n",
    "\n",
    "# print(f\"TFT Validation - RMSE: {rmse_tft_val:.4f}, RÂ²: {r2_tft_val:.4f}\")\n",
    "# print(f\"TFT Test       - RMSE: {rmse_tft_test:.4f}, RÂ²: {r2_tft_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285ba2",
   "metadata": {},
   "source": [
    "## 5. Main Experiment Function\n",
    "å­˜æª”å¾seedæ”¹ç‚ºTFT_seed, \n",
    " å‘¼å« OLS çš„å‡½å¼æ”¹æˆå‘¼å« TFT å‡½å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea211b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mk_series_per_station(\n",
    "    train_y, val_y, test_y, train_X, val_X, test_X, split_cfg\n",
    ") -> Tuple[List[TimeSeries], List[TimeSeries], List[TimeSeries], int, int, int, int, int]:\n",
    "    \"\"\"\n",
    "    å°‡ (T, N, F) / (T, N) çš„å¼µé‡åˆ‡å›å®Œæ•´æ™‚åºï¼Œä¸¦è½‰æˆæ¯ç«™ä¸€æ¢ TimeSeries + covariates(demo é¢¨æ ¼ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "    if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "    if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "\n",
    "    y_all_scaled = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T_full, N)\n",
    "    x_all_scaled = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T_full, N, F)\n",
    "\n",
    "    T_full = y_all_scaled.shape[0]\n",
    "    N      = y_all_scaled.shape[1]\n",
    "    F      = x_all_scaled.shape[2]\n",
    "\n",
    "    T_tr = int(T_full * split_cfg[\"train_ratio\"])\n",
    "    T_va = int(T_full * (split_cfg[\"train_ratio\"] + split_cfg[\"val_ratio\"]))\n",
    "    val_len  = T_va - T_tr\n",
    "    test_len = T_full - T_va\n",
    "\n",
    "    series_list, past_cov_list, future_cov_list = [], [], []\n",
    "    for i in range(N):\n",
    "        s_i   = TimeSeries.from_values(y_all_scaled[:, i])   # (T,)\n",
    "        cov_i = TimeSeries.from_values(x_all_scaled[:, i, :])# (T, F)\n",
    "        series_list.append(s_i)\n",
    "        past_cov_list.append(cov_i)\n",
    "        future_cov_list.append(cov_i)\n",
    "\n",
    "    return series_list, past_cov_list, future_cov_list, T_tr, T_va, val_len, test_len, N, F\n",
    "\n",
    "\n",
    "def build_tft_and_data(\n",
    "    *,\n",
    "    cat_features: np.ndarray,          # shape (T, N, ?)ï¼Œå¯ä¸ä½¿ç”¨\n",
    "    cont_features: np.ndarray,         # shape (T, N, F)\n",
    "    targets: np.ndarray,               # shape (T, N)\n",
    "    split_cfg: Dict[str, Any],         # e.g. {\"train_ratio\": 0.7, \"val_ratio\": 0.15}\n",
    "    tft_cfg: Dict[str, Any],           # TFT_CONFIGï¼ˆéœ€å« add_relative_index æˆ–å·²é—œé–‰ attention éœ€æ±‚ï¼‰\n",
    "    pl_trainer_kwargs: Dict[str, Any], # ä¾‹å¦‚ {\"accelerator\":\"gpu\",\"devices\":1,\"logger\":False,...}\n",
    "    preprocessor=None                  # è‹¥çµ¦ï¼Œæœƒå›å‚³å·²åæ¨™æº–åŒ–çš„ yhatï¼›å¦å‰‡åªå›å‚³ scaled\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    å»ºç«‹ per-location çš„ TimeSeries èˆ‡ past covariatesï¼Œè¨“ç·´å–®ä¸€ TFT æ¨¡å‹ï¼ˆä¸ä½¿ç”¨ future covariatesï¼‰ï¼Œ\n",
    "    ä¸¦ä»¥ historical_forecasts ç”¢ç”Ÿ val/test çš„ä¸€æ­¥æ»¾å‹•é æ¸¬ã€‚\n",
    "\n",
    "    å›å‚³ï¼š\n",
    "      - tft_model : è¨“ç·´å¥½çš„ Darts TFTModel\n",
    "      - yhat_val_sc, yhat_test_sc : ä»¥ scaler ç©ºé–“ (scaled) çš„é æ¸¬ï¼Œshape åˆ†åˆ¥ç‚º (Tval, N), (Ttest, N)\n",
    "      - yhat_val_den, yhat_test_den : è‹¥æä¾› preprocessorï¼Œå‰‡ç‚ºåæ¨™æº–åŒ–å¾Œçš„é æ¸¬ï¼›å¦å‰‡ç‚º None\n",
    "      - T_tr, T_va, val_len, test_len, N : ä¸€äº›æ‹†åˆ†è³‡è¨Š\n",
    "    \"\"\"\n",
    "    # ---- 0) å–å½¢ç‹€ã€è¨ˆç®—åˆ‡åˆ† ----\n",
    "    T_full, N = targets.shape\n",
    "    F = cont_features.shape[2]\n",
    "    train_ratio = float(split_cfg.get(\"train_ratio\", 0.7))\n",
    "    val_ratio   = float(split_cfg.get(\"val_ratio\", 0.15))\n",
    "    T_tr = int(T_full * train_ratio)\n",
    "    T_va = int(T_full * (train_ratio + val_ratio))\n",
    "    val_len  = T_va - T_tr\n",
    "    test_len = T_full - T_va\n",
    "\n",
    "    # ---- 1) çµ„ per-location series & past covariatesï¼ˆä¸å»ºç«‹ future covariatesï¼‰----\n",
    "    series_list: List[TimeSeries] = []\n",
    "    past_cov_list: List[TimeSeries] = []\n",
    "\n",
    "    # è¦æ±‚ï¼štargets, cont_features éƒ½æ˜¯ã€Œæ™‚é–“åœ¨ç¬¬ä¸€ç¶­ã€ï¼Œå³ (T, N, Â·)\n",
    "    for i in range(N):\n",
    "        # ç›®æ¨™ï¼šå–®è®Šæ•¸\n",
    "        s_i = TimeSeries.from_values(targets[:, i])           # (T,)\n",
    "        # past covariatesï¼šå¤šè®Šæ•¸\n",
    "        cov_i = TimeSeries.from_values(cont_features[:, i, :])  # (T, F)\n",
    "        series_list.append(s_i)\n",
    "        past_cov_list.append(cov_i)\n",
    "\n",
    "    # ---- 2) Splitï¼ˆtrain / valï¼‰----\n",
    "    series_train_list = [s[:T_tr]      for s in series_list]\n",
    "    series_val_list   = [s[T_tr:T_va]  for s in series_list]\n",
    "    past_cov_train    = [c[:T_tr]      for c in past_cov_list]\n",
    "    val_past_covs     = [c[T_tr:T_va]  for c in past_cov_list]  # åªçµ¦é©—è­‰ç”¨çš„ past covariates\n",
    "\n",
    "    # ---- 3) å»ºç«‹ä¸¦è¨“ç·´ TFTï¼ˆä¸å‚³ future covariatesï¼‰----\n",
    "    # å»ºè­°ï¼štft_cfg å…§è¦æœ‰ add_relative_index=Trueï¼ˆæˆ–ç­‰åƒ¹ encodersï¼‰ï¼Œé¿å…éœ€è¦ future covariates\n",
    "    pl.seed_everything(tft_cfg.get(\"random_state\", 42))\n",
    "    tft_model = TFTModel(\n",
    "        **tft_cfg,\n",
    "        pl_trainer_kwargs=pl_trainer_kwargs\n",
    "    )\n",
    "    # æ³¨æ„ï¼šval_series èˆ‡ val_past_covariates è¦ä¸€èµ·çµ¦ï¼Œä¸”æ¯å€‹ index çš„ç¶­åº¦ä¸€è‡´\n",
    "    tft_model.fit(\n",
    "        series=series_train_list,\n",
    "        val_series=series_val_list,\n",
    "        past_covariates=past_cov_train,\n",
    "        val_past_covariates=val_past_covs,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # ---- 4) æ­·å²ä¸€æ­¥æ»¾å‹•é æ¸¬ï¼ˆval+test åˆèµ·ä¾†ï¼‰ï¼Œåªç”¨ past covariates ----\n",
    "    yhat_val_list, yhat_test_list = [], []\n",
    "    for i in range(N):\n",
    "        preds_i = tft_model.historical_forecasts(\n",
    "            series=series_list[i],\n",
    "            past_covariates=past_cov_list[i],\n",
    "            start=T_tr,                # å¾é©—è­‰é–‹å§‹é»\n",
    "            forecast_horizon=1,        # ä¸€æ­¥æ»¾å‹•\n",
    "            retrain=False,\n",
    "            verbose=False\n",
    "        ).values()  # shape (Tval+Ttest,)\n",
    "        yhat_val_list.append(preds_i[:val_len])\n",
    "        yhat_test_list.append(preds_i[val_len:val_len + test_len])\n",
    "\n",
    "    # å›åˆ° (T, N) çš„ 2D é™£åˆ—ï¼ˆä»åœ¨ scaled ç©ºé–“ï¼‰\n",
    "    yhat_val_sc  = np.stack(yhat_val_list,  axis=1)  # (Tval,  N)\n",
    "    yhat_test_sc = np.stack(yhat_test_list, axis=1)  # (Ttest, N)\n",
    "\n",
    "    # ---- 5) è‹¥æœ‰ preprocessorï¼Œå‰‡ä¸€ä½µè¼¸å‡ºåæ¨™æº–åŒ–çš„é æ¸¬ ----\n",
    "    def _maybe_denorm(arr_2d: np.ndarray):\n",
    "        if preprocessor is None:\n",
    "            return None\n",
    "        # denormalize_predictions æ”¯æ´ torch.Tensor / np.ndarrayï¼›è¼¸å…¥ shape (T,N) å³å¯\n",
    "        return denormalize_predictions(\n",
    "            torch.from_numpy(arr_2d).float(), preprocessor\n",
    "        )  # -> torch.Tensor or np.ndarrayï¼ˆä¾å¯¦ä½œï¼‰\n",
    "\n",
    "    yhat_val_den  = _maybe_denorm(yhat_val_sc)\n",
    "    yhat_test_den = _maybe_denorm(yhat_test_sc)\n",
    "\n",
    "    return {\n",
    "        \"tft_model\": tft_model,\n",
    "        \"yhat_val_sc\": yhat_val_sc,\n",
    "        \"yhat_test_sc\": yhat_test_sc,\n",
    "        \"yhat_val_den\": yhat_val_den,\n",
    "        \"yhat_test_den\": yhat_test_den,\n",
    "        \"T_tr\": T_tr,\n",
    "        \"T_va\": T_va,\n",
    "        \"val_len\": val_len,\n",
    "        \"test_len\": test_len,\n",
    "        \"N\": N,\n",
    "        \"F\": F,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def build_spatial_adapter_from_demo(\n",
    "    tft_model: TFTModel,\n",
    "    device: torch.device,\n",
    "    n_locations: int,\n",
    "    latent_dim: int,\n",
    "    train_loader,                  # è‹¥ demo çš„ adapter éœ€è¦ DataLoaderï¼ˆtheta/basis æ›´æ–°ï¼‰ï¼Œå°±å‚³é€²ä¾†\n",
    "    val_cont: torch.Tensor,        # é©—è­‰ç”¨ covariates/inputsï¼ˆdemo è‹¥æœ‰ç”¨ï¼‰\n",
    "    val_y: torch.Tensor,           # é©—è­‰çœŸå€¼ï¼ˆscaledï¼‰\n",
    "    locs: np.ndarray,\n",
    "    config,                        # SpatialNeuralAdapterConfig\n",
    "    tau1: float,\n",
    "    tau2: float,\n",
    "    writer=None\n",
    ") -> SpatialNeuralAdapter:\n",
    "    \"\"\"\n",
    "    DEMO é¢¨æ ¼ï¼šç”¨ TFTWrapper æŠŠå·²è¨“ç·´ TFT åŒ…æˆ trend,ç„¶å¾Œå»ºç«‹ SpatialNeuralAdapterã€‚\n",
    "    \"\"\"\n",
    "    # ä¾ demoï¼šTFTWrapper æœƒåœ¨ forward è£¡ç”¨å·²è¨“ç·´å¥½çš„ TFT åšè¶¨å‹¢é æ¸¬ã€‚\n",
    "    trend_backbone = TFTWrapper(tft_model=tft_model)  # è‹¥ demo éœ€è¦å…¶å®ƒåƒæ•¸ï¼ŒæŒ‰ä½ çš„ demo åŠ \n",
    "\n",
    "    basis = SpatialBasisLearner(n_locations, latent_dim).to(device)\n",
    "\n",
    "    adapter = SpatialNeuralAdapter(\n",
    "        trend_backbone, basis,\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_cont.to(device),\n",
    "        val_y=val_y.to(device),\n",
    "        locs=locs,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        writer=writer,\n",
    "        tau1=tau1,\n",
    "        tau2=tau2,\n",
    "    )\n",
    "    return adapter\n",
    "\n",
    "\n",
    "def train_unregularized_adapter(\n",
    "    adapter: SpatialNeuralAdapter,\n",
    "    pretrain_epochs: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DEMO é¢¨æ ¼:tau1=tau2=0 çš„ adapter è¨“ç·´æµç¨‹ã€‚\n",
    "    å›å‚³ï¼š\n",
    "      - \"adapter\": å·²è¨“ç·´å®Œæˆçš„ adapter\n",
    "    \"\"\"\n",
    "    adapter.pretrain_trend(epochs=pretrain_epochs)  # è‹¥ demo æ²’æœ‰ pretrain_trendï¼Œå°±ç”¨å°æ‡‰åç¨±\n",
    "    adapter.init_basis_dense()\n",
    "    adapter.run()\n",
    "    return {\"adapter\": adapter}\n",
    "\n",
    "\n",
    "def train_regularized_adapter_with_optuna(\n",
    "    build_adapter_fn,                             # ä¸€å€‹ closure: (tau1, tau2) -> SpatialNeuralAdapter\n",
    "    val_y_den_t: torch.Tensor,\n",
    "    predict_val_den_fn,                           # closure: (adapter) -> torch.Tensor denorm predictions on val\n",
    "    n_trials: int = 30,\n",
    "    study_name: str = \"TFT_spatial_adapter_reg\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DEMO é¢¨æ ¼ï¼šç”¨ Optuna æœå°‹ tau1ã€tau2,æŒ‡æ¨™åœ¨ **denormalized** ç©ºé–“ä¸Šè¨ˆç®—ã€‚\n",
    "    predict_val_den_fn(adapter) éœ€å›å‚³ denorm çš„ (Tval, N) torch.Tensorã€‚\n",
    "    \"\"\"\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "        tau1 = trial.suggest_float(\"tau1\", 1e-4, 1e8, log=True)\n",
    "        tau2 = trial.suggest_float(\"tau2\", 1e-4, 1e8, log=True)\n",
    "        adapter = build_adapter_fn(tau1, tau2)\n",
    "        adapter.pretrain_trend(epochs=3)\n",
    "        adapter.init_basis_dense()\n",
    "        adapter.run()\n",
    "\n",
    "        # ä¾ demoï¼šç”¨ adapter åš val é æ¸¬ï¼ˆä¸¦è½‰å›åŸå°ºåº¦ï¼‰\n",
    "        y_val_pred_den = predict_val_den_fn(adapter)\n",
    "        rmse, mae, r2 = compute_metrics(val_y_den_t, y_val_pred_den)\n",
    "\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\",  r2)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name, direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=MedianPruner(n_warmup_steps=5),\n",
    "        load_if_exists=False,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    best = study.best_trial\n",
    "    return {\n",
    "        \"tau1\": best.params[\"tau1\"],\n",
    "        \"tau2\": best.params[\"tau2\"],\n",
    "        \"rmse\": best.user_attrs[\"rmse\"],\n",
    "        \"mae\":  best.user_attrs[\"mae\"],\n",
    "        \"r2\":   best.user_attrs[\"r2\"],\n",
    "        \"best_trial\": best.number,\n",
    "        \"study\": study,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_adapter_on_test_from_demo(\n",
    "    adapter: SpatialNeuralAdapter,\n",
    "    denorm_true_test: torch.Tensor,\n",
    "    predict_test_den_fn,          # closure: (adapter) -> torch.Tensor denorm predictions on test\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    DEMO é¢¨æ ¼ï¼šåœ¨ test é›†ä¸Šç”¨ adapter æ¨è«–ï¼Œå›å‚³ (rmse, mae, r2)ï¼ˆåŸå°ºåº¦ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    y_test_pred_den = predict_test_den_fn(adapter)\n",
    "    rmse, mae, r2 = compute_metrics(denorm_true_test, y_test_pred_den)\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_experiment(dataset_seed: int, n_trials: int = 30):\n",
    "    \"\"\"\n",
    "    Run one experiment using *TFT baseline + Spatial Adapter* (demo-style):\n",
    "      1) Train TFT on per-location series (+covariates) â†’ baseline preds (val/test) on original scale\n",
    "      2) Build Spatial Adapter (unregularized), train\n",
    "      3) Optuna to search (tau1, tau2) for regularized adapter\n",
    "      4) Re-train best regularized adapter and evaluate on test\n",
    "      5) Save metrics to metrics_summary_TFT.csv\n",
    "    \"\"\"\n",
    "    log_root = Path(\"TFT_runs\") / f\"TFT_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ----- 1) Data for this seed -----\n",
    "    catf, conf, tgts = generate_time_synthetic_data(\n",
    "        locs=locs,\n",
    "        n_time_steps=CFG[\"n_time_steps\"],\n",
    "        noise_std=CFG[\"noise_std\"],\n",
    "        eigenvalue=CFG[\"eigenvalue\"],\n",
    "        eta_rho=0.8,\n",
    "        f_rho=0.6,\n",
    "        global_mean=50.0,\n",
    "        feature_noise_std=0.1,\n",
    "        non_linear_strength=0.2,\n",
    "        seed=dataset_seed\n",
    "    )\n",
    "\n",
    "    train_ds, val_ds, test_ds, preproc = prepare_all_with_scaling(\n",
    "        cat_features=catf,\n",
    "        cont_features=conf,\n",
    "        targets=tgts,\n",
    "        train_ratio=SPLIT_CONFIG[\"train_ratio\"],\n",
    "        val_ratio=SPLIT_CONFIG[\"val_ratio\"],\n",
    "        feature_scaler_type=\"standard\",\n",
    "        target_scaler_type=\"standard\",\n",
    "        fit_on_train_only=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds, batch_size=SPATIAL_CONFIG.training.batch_size, shuffle=True)\n",
    "\n",
    "    # ----- 2) Demo-style TFT training + baseline (denorm) -----\n",
    "    tft_pack = build_tft_and_data(\n",
    "        cat_features=catf,\n",
    "        cont_features=conf,\n",
    "        targets=tgts,\n",
    "        split_cfg=SPLIT_CONFIG,\n",
    "        tft_cfg=TFT_CONFIG,\n",
    "        pl_trainer_kwargs=PL_TRAINER_KWARGS\n",
    "    )\n",
    "    # æ‹†å›éœ€è¦çš„æ±è¥¿\n",
    "    tft_model       = tft_pack[\"tft_model\"]\n",
    "    val_y_den_t     = tft_pack[\"val_y_den\"]\n",
    "    test_y_den_t    = tft_pack[\"test_y_den\"]\n",
    "    yhat_val_den_t  = tft_pack[\"yhat_val_den\"]\n",
    "    yhat_test_den_t = tft_pack[\"yhat_test_den\"]\n",
    "\n",
    "    # Baseline metrics (TFT)\n",
    "    rmse_tft, mae_tft, r2_tft = compute_metrics(val_y_den_t,  yhat_val_den_t)\n",
    "    rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(test_y_den_t, yhat_test_den_t)\n",
    "\n",
    "    # ä¹Ÿéœ€è¦ scaled çš„é©—è­‰/æ¸¬è©¦ y ä¾› adapter ç”¨\n",
    "    _, val_X,   val_y   = val_ds.tensors\n",
    "    _, test_X,  test_y  = test_ds.tensors\n",
    "\n",
    "    # ----- 3) Unregularized adapter (tau1=tau2=0) -----\n",
    "    adapter_unreg = build_spatial_adapter_from_demo(\n",
    "        tft_model=tft_model,\n",
    "        device=DEVICE,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_X,\n",
    "        val_y=val_y,\n",
    "        locs=locs,\n",
    "        config=SPATIAL_CONFIG,\n",
    "        tau1=0.0,\n",
    "        tau2=0.0,\n",
    "        writer=SummaryWriter(log_dir=log_root / \"bootstrap\")\n",
    "    )\n",
    "    train_unregularized_adapter(adapter_unreg, pretrain_epochs=5)\n",
    "\n",
    "    # demo é¢¨æ ¼ï¼šç”¨ adapter åš val/test æ¨è«–ï¼ˆä¸¦è½‰å›åŸå°ºåº¦ï¼‰\n",
    "    def predict_val_den_fn(adapter):\n",
    "        y_pred_sc = adapter.predict(val_X.to(DEVICE), val_y.to(DEVICE))   # shape (Tval, N, 1) or (Tval, N)\n",
    "        if y_pred_sc.ndim == 3: y_pred_sc = y_pred_sc.squeeze(-1)\n",
    "        y_pred_den = denormalize_predictions(y_pred_sc, preproc)          # (Tval, N)\n",
    "        if isinstance(y_pred_den, np.ndarray):\n",
    "            y_pred_den = torch.from_numpy(y_pred_den).float()\n",
    "        return y_pred_den\n",
    "\n",
    "    def predict_test_den_fn(adapter):\n",
    "        y_pred_sc = adapter.predict(test_X.to(DEVICE), test_y.to(DEVICE)) # shape (Ttest, N, 1) or (Ttest, N)\n",
    "        if y_pred_sc.ndim == 3: y_pred_sc = y_pred_sc.squeeze(-1)\n",
    "        y_pred_den = denormalize_predictions(y_pred_sc, preproc)          # (Ttest, N)\n",
    "        if isinstance(y_pred_den, np.ndarray):\n",
    "            y_pred_den = torch.from_numpy(y_pred_den).float()\n",
    "        return y_pred_den\n",
    "\n",
    "    # Unreg metricsï¼ˆdenormï¼‰\n",
    "    y_unreg_val_den_t  = predict_val_den_fn(adapter_unreg)\n",
    "    y_unreg_test_den_t = predict_test_den_fn(adapter_unreg)\n",
    "    rmse_unreg, mae_unreg, r2_unreg = compute_metrics(val_y_den_t,  y_unreg_val_den_t)\n",
    "    rmse_unreg_test, mae_unreg_test, r2_unreg_test = compute_metrics(test_y_den_t, y_unreg_test_den_t)\n",
    "\n",
    "    # ----- 4) Regularized (Optuna)ï¼šä»¥ demo çš„æµç¨‹æœå°‹ tau1/tau2 -----\n",
    "    def build_adapter_fn(tau1: float, tau2: float):\n",
    "        return build_spatial_adapter_from_demo(\n",
    "            tft_model=tft_model,\n",
    "            device=DEVICE,\n",
    "            n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "            latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "            train_loader=train_loader,\n",
    "            val_cont=val_X,\n",
    "            val_y=val_y,\n",
    "            locs=locs,\n",
    "            config=SPATIAL_CONFIG,\n",
    "            tau1=tau1,\n",
    "            tau2=tau2,\n",
    "            writer=SummaryWriter(log_dir=log_root / f\"trial_tmp_tau1_{tau1:.3g}_tau2_{tau2:.3g}\")\n",
    "        )\n",
    "\n",
    "    reg_search = train_regularized_adapter_with_optuna(\n",
    "        build_adapter_fn=build_adapter_fn,\n",
    "        val_y_den_t=val_y_den_t,\n",
    "        predict_val_den_fn=predict_val_den_fn,\n",
    "        n_trials=n_trials,\n",
    "        study_name=f\"TFT_spatial_adapter_reg_ds{dataset_seed}\"\n",
    "    )\n",
    "    tau1_opt, tau2_opt = reg_search[\"tau1\"], reg_search[\"tau2\"]\n",
    "    rmse_opt, mae_opt, r2_opt = reg_search[\"rmse\"], reg_search[\"mae\"], reg_search[\"r2\"]\n",
    "    best_no = reg_search[\"best_trial\"]\n",
    "\n",
    "    # ç”¨æœ€ä½³åƒæ•¸é‡å»º + è¨“ç·´ï¼Œè©•ä¼° testï¼ˆdenormï¼‰\n",
    "    adapter_best = build_adapter_fn(tau1_opt, tau2_opt)\n",
    "    train_unregularized_adapter(adapter_best, pretrain_epochs=5)   # èˆ‡ search æ™‚åŒä¸€è¨“ç·´ç¨‹åºï¼ˆè‹¥ demo æœ‰å°ˆå±¬ train()ï¼Œç”¨å®ƒï¼‰\n",
    "    y_reg_test_den_t = predict_test_den_fn(adapter_best)\n",
    "    rmse_reg_test, mae_reg_test, r2_reg_test = compute_metrics(test_y_den_t, y_reg_test_den_t)\n",
    "\n",
    "    # ----- 5) å¯« CSVï¼ˆTFT å‰ç¶´ï¼‰ -----\n",
    "    csv_path = Path(\"metrics_summary_TFT.csv\")\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\"seed\",\"model\",\"trial\",\"tau1\",\"tau2\",\"rmse_val\",\"mae_val\",\"r2_val\",\"rmse_test\",\"mae_test\",\"r2_test\"])\n",
    "        w.writerow([dataset_seed,\"TFT\",\"\", \"\", \"\", f\"{rmse_tft:.6f}\",f\"{mae_tft:.6f}\",f\"{r2_tft:.6f}\",\n",
    "                    f\"{rmse_tft_test:.6f}\",f\"{mae_tft_test:.6f}\",f\"{r2_tft_test:.6f}\"])\n",
    "        w.writerow([dataset_seed,\"Unreg\",\"\", \"0\",\"0\", f\"{rmse_unreg:.6f}\",f\"{mae_unreg:.6f}\",f\"{r2_unreg:.6f}\",\n",
    "                    f\"{rmse_unreg_test:.6f}\",f\"{mae_unreg_test:.6f}\",f\"{r2_unreg_test:.6f}\"])\n",
    "        w.writerow([dataset_seed,\"Reg\", best_no, f\"{tau1_opt:.6g}\",f\"{tau2_opt:.6g}\",\n",
    "                    f\"{rmse_opt:.6f}\",f\"{mae_opt:.6f}\",f\"{r2_opt:.6f}\",\n",
    "                    f\"{rmse_reg_test:.6f}\",f\"{mae_reg_test:.6f}\",f\"{r2_reg_test:.6f}\"])\n",
    "\n",
    "    print(f\"Dataset {dataset_seed}: TFT {rmse_tft:.3f} | Unreg {rmse_unreg:.3f} | Reg {rmse_opt:.3f} (test {rmse_reg_test:.3f})\")\n",
    "\n",
    "    return {\n",
    "        'TFT':   {'rmse_val': rmse_tft, 'rmse_test': rmse_tft_test, 'r2_val': r2_tft, 'r2_test': r2_tft_test},\n",
    "        'unreg': {'rmse_val': rmse_unreg, 'rmse_test': rmse_unreg_test, 'r2_val': r2_unreg, 'r2_test': r2_unreg_test},\n",
    "        'reg':   {'rmse_val': rmse_opt, 'rmse_test': rmse_reg_test, 'r2_val': r2_opt, 'r2_test': r2_reg_test,\n",
    "                  'tau1': tau1_opt, 'tau2': tau2_opt}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6642f",
   "metadata": {},
   "source": [
    "## 6. Run Full Experiment Suite\n",
    "before test epoch change to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ababa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for seed in range(EXPERIMENT_TRIALS_CONFIG['seed_range_start'], EXPERIMENT_TRIALS_CONFIG['seed_range_end']):\n",
    "    print(f\"\\nStarting experiment for seed {seed}\")\n",
    "    results = run_one_experiment(seed, n_trials=EXPERIMENT_TRIALS_CONFIG['n_trials_per_seed'])\n",
    "    all_results.append(results)\n",
    "    cache.clear()\n",
    "    clear_gpu_memory()\n",
    "    print(f\"âœ… Completed seed {seed}\")\n",
    "\n",
    "print(\"\\nğŸ‰ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0388ee",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization\n",
    "\n",
    "é‚„æœªå®Œå…¨æ”¹å®Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_df = pd.read_csv(\"metrics_summary_TFT.csv\")\n",
    "print(\"ğŸ“Š Results Summary:\")\n",
    "print(results_df.groupby('model')[['rmse_val', 'rmse_test', 'r2_val', 'r2_test']].mean())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_val', ax=axes[0,0])\n",
    "axes[0,0].set_title('Validation RMSE')\n",
    "axes[0,0].set_ylabel('RMSE')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_test', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test RMSE')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# RÂ² comparison\n",
    "sns.boxplot(data=results_df, x='model', y='r2_val', ax=axes[1,0])\n",
    "axes[1,0].set_title('Validation RÂ²')\n",
    "axes[1,0].set_ylabel('RÂ²')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='r2_test', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test RÂ²')\n",
    "axes[1,1].set_ylabel('RÂ²')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best hyperparameters for regularized model\n",
    "reg_results = results_df[results_df['model'] == 'Reg']\n",
    "print(\"\\nğŸ”§ Best Hyperparameters for Regularized Model:\")\n",
    "print(reg_results[['tau1', 'tau2', 'rmse_val', 'rmse_test']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison summary (TFT baseline)\n",
    "print(\"=== Performance Comparison Summary (TFT as baseline) ===\")\n",
    "\n",
    "# Means by model\n",
    "tft_mean_rmse   = results_df[results_df['model'] == 'TFT']['rmse_test'].mean()\n",
    "unreg_mean_rmse = results_df[results_df['model'] == 'Unreg']['rmse_test'].mean()\n",
    "reg_mean_rmse   = results_df[results_df['model'] == 'Reg']['rmse_test'].mean()\n",
    "\n",
    "print(f\"TFT (baseline)  - Mean Test RMSE: {tft_mean_rmse:.4f}\")\n",
    "print(f\"Unregularized   - Mean Test RMSE: {unreg_mean_rmse:.4f} \"\n",
    "      f\"({(1 - unreg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement vs TFT)\")\n",
    "print(f\"Regularized     - Mean Test RMSE: {reg_mean_rmse:.4f} \"\n",
    "      f\"({(1 - reg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement vs TFT)\")\n",
    "\n",
    "# Statistical significance test (paired by seed): TFT vs Regularized\n",
    "from scipy import stats\n",
    "\n",
    "pivot = results_df.pivot_table(index='seed', columns='model', values='rmse_test', aggfunc='mean')\n",
    "paired = pivot.dropna(subset=['TFT', 'Reg'])  # keep only seeds that have both\n",
    "t_stat, p_value = stats.ttest_rel(paired['TFT'].values, paired['Reg'].values)\n",
    "\n",
    "print(f\"\\nStatistical Test (TFT vs Regularized):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant improvement: {'Yes' if p_value < 0.05 else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
