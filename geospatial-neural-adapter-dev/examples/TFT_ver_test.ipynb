{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS vs. Spatial Adapter Comparison with Tuning Parameter Selection\n",
    "\n",
    "This notebook implements a comprehensive comparison between:\n",
    "1. **TFT** - Linear baseline (no spatial term)\n",
    "2. **Unregularized Spatial Adapter** - Neural spatial model without regularization\n",
    "3. **Regularized Spatial Adapter** - Neural spatial model with optimized tau1, tau2 parameters\n",
    "\n",
    "The experiment uses Optuna for hyperparameter optimization and evaluates performance across multiple random seeds.\n",
    "\n",
    "my work: ols ÊèõÊàê TFT ÁÑ∂ÂæåÂü∑Ë°åÊ®°Êì¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345997df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "# ==== New: Darts / TFT as the backbone (replaces OLS) ====\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "\n",
    "from geospatial_neural_adapter.cpp_extensions import estimate_covariance\n",
    "from geospatial_neural_adapter.utils.experiment import log_covariance_and_basis\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    ModelCache,\n",
    "    clear_gpu_memory,\n",
    "    create_experiment_config,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "from geospatial_neural_adapter.models.spatial_basis_learner import SpatialBasisLearner\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapter\n",
    "from geospatial_neural_adapter.models.trend_model import TrendModel\n",
    "from geospatial_neural_adapter.models.wrapper_examples.tft_wrapper import TFTWrapper\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import (\n",
    "    prepare_all_with_scaling,\n",
    "    denormalize_predictions,\n",
    ")\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Imports successful (TFT backbone enabled; all OLS utilities removed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f86c36",
   "metadata": {},
   "source": [
    "## 1. Parameter Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_time_steps': 1024//2,\n",
    "    'n_locations': 512//2,\n",
    "    'noise_std': 4.0,\n",
    "    'eigenvalue': 16.0,\n",
    "    'latent_dim': 1,\n",
    "    'ckpt_dir': \"admm_bcd_ckpts\",\n",
    "}\n",
    "\n",
    "# ==== New: Split & TFT configs (baseline uses TFT, no spatial term) ====\n",
    "SPLIT_CONFIG = {\n",
    "    \"train_ratio\": 0.70,\n",
    "    \"val_ratio\": 0.15,   # Ââ©È§òÂç≥ÁÇ∫ test_ratio\n",
    "}\n",
    "\n",
    "TFT_CONFIG = {\n",
    "    \"input_chunk_length\": 48,\n",
    "    \"output_chunk_length\": 1,\n",
    "    \"hidden_size\": 64,\n",
    "    \"lstm_layers\": 1,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"dropout\": 0.10,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 10,  #30\n",
    "    \"random_state\": 42,\n",
    "    \"add_relative_index\": True,\n",
    "}\n",
    "\n",
    "PL_TRAINER_KWARGS = (\n",
    "    {\"accelerator\": \"gpu\", \"devices\": 1,\n",
    "     \"logger\": True,                 # ‚Üê ÈñãÂïü logger\n",
    "     \"enable_progress_bar\": True,   # ‚Üê ÁπºÁ∫åÈóúÈñâÈÄ≤Â∫¶Ê¢ù\n",
    "     \"enable_model_summary\": False,\n",
    "     \"num_sanity_val_steps\": 0}\n",
    "    if torch.cuda.is_available()\n",
    "    else {\"accelerator\": \"cpu\", \"devices\": 1,\n",
    "          \"logger\": True,\n",
    "          \"enable_progress_bar\": True,\n",
    "          \"enable_model_summary\": False,\n",
    "          \"num_sanity_val_steps\": 0}\n",
    ")\n",
    "\n",
    "# Spatial Neural Adapter Configuration using dataclasses\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import (\n",
    "    SpatialNeuralAdapterConfig, ADMMConfig, TrainingConfig, BasisConfig\n",
    ")\n",
    "\n",
    "# ADMM Configuration\n",
    "admm_config = ADMMConfig(\n",
    "    rho=1.0,            # Base ADMM penalty parameter\n",
    "    dual_momentum=0.2,  # Dual variable momentum\n",
    "    max_iters=3000,     # Maximum ADMM iterations\n",
    "    min_outer=20,       # Minimum outer iterations before convergence check\n",
    "    tol=1e-4,           # Convergence tolerance\n",
    ")\n",
    "\n",
    "# Training Configuration\n",
    "# ÊèêÈÜíÔºöÈÄôË£°ÁöÑ lr_mu ÂéüÂÖàÊòØÁµ¶ OLS/Á∑öÊÄßË∂®Âã¢Áî®ÔºõÊîπÁÇ∫ TFT ÂæåÔºåË∂®Âã¢Â≠∏ÁøíÊîπÁî± TFT Êú¨Ë∫´ËôïÁêÜ„ÄÇ\n",
    "# Ëã•‰Ω†ÁöÑ SpatialNeuralAdapter ÂÖßÈÉ®‰ªçÂèÉËÄÉË©≤Ê¨Ñ‰ΩçÔºåÂÖà‰øùÁïô‰ª•Á∂≠ÊåÅÁõ∏ÂÆπÔºõÂØ¶ÈöõÂ≠∏ÁøíÁéá‰ª• TFT_CONFIG ÁÇ∫Ê∫ñ„ÄÇ\n",
    "training_config = TrainingConfig(\n",
    "    lr_mu=1e-2,           # ‰øùÁïô‰ª•Á∂≠ÊåÅÁõ∏ÂÆπÔºåÂØ¶ÈöõË∂®Âã¢Áî± TFT Â≠∏\n",
    "    batch_size=64,        # Adapter theta step ÊâπÊ¨°\n",
    "    pretrain_epochs=5,    # Default pretraining epochsÔºàËã•ÊµÅÁ®ãÁî®ÂæóÂà∞Ôºâ\n",
    "    use_mixed_precision=False,\n",
    ")\n",
    "\n",
    "# Basis Configuration\n",
    "basis_config = BasisConfig(\n",
    "    phi_every=5,        # Update basis every N iterations\n",
    "    phi_freeze=200,     # Stop updating basis after N iterations\n",
    "    matrix_reg=1e-6,    # Matrix regularization for basis update\n",
    "    irl1_max_iters=10,  # IRL‚ÇÅ maximum iterations\n",
    "    irl1_eps=1e-6,      # IRL‚ÇÅ epsilon\n",
    "    irl1_tol=5e-4,      # IRL‚ÇÅ inner tolerance\n",
    ")\n",
    "\n",
    "# Complete Spatial Neural Adapter Configuration\n",
    "SPATIAL_CONFIG = SpatialNeuralAdapterConfig(\n",
    "    admm=admm_config,\n",
    "    training=training_config,\n",
    "    basis=basis_config\n",
    ")\n",
    "\n",
    "# Legacy config dict for backward compatibility (if needed)\n",
    "CFG = SPATIAL_CONFIG.to_dict()\n",
    "CFG.update(EXPERIMENT_CONFIG)\n",
    "CFG.update({\n",
    "    \"split\": SPLIT_CONFIG,\n",
    "    \"tft\": TFT_CONFIG,\n",
    "})\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "torch.manual_seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "Path(EXPERIMENT_CONFIG[\"ckpt_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info['device'] == 'cuda':\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Experiment Configuration ===\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Split Configuration ===\")\n",
    "for k, v in SPLIT_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== TFT Configuration (Baseline) ===\")\n",
    "for k, v in TFT_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Spatial Neural Adapter Configuration ===\")\n",
    "SPATIAL_CONFIG.log_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a32565",
   "metadata": {},
   "source": [
    "## 2. Initialize Utilities\n",
    " ÁõÆÂâçÊ∏õÂ∞ëË∑ëÁöÑÊ¨°Êï∏ Á¢∫Ë™çÂÆå‰πãÂæåË¶ÅË∑ëÂÆåÊï¥ÁöÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model cache for hyperparameter optimization\n",
    "cache = ModelCache()\n",
    "\n",
    "# Create experiment configuration\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=20 if torch.cuda.is_available() else 50,\n",
    "    n_dataset_seeds=10,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=11,\n",
    ")\n",
    "\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "print(\"Utilities initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777490a",
   "metadata": {},
   "source": [
    "## 3. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19da385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with meaningful correlations\n",
    "print(\"Generating correlated synthetic data...\")\n",
    "\n",
    "locs = np.linspace(-3, 3, CFG[\"n_locations\"])\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locs,\n",
    "    n_time_steps=CFG[\"n_time_steps\"],\n",
    "    noise_std=CFG[\"noise_std\"],\n",
    "    eigenvalue=CFG[\"eigenvalue\"],\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=CFG[\"seed\"]\n",
    ")\n",
    "\n",
    "# Prepare datasets with scaling\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=SPLIT_CONFIG[\"train_ratio\"],\n",
    "    val_ratio=SPLIT_CONFIG[\"val_ratio\"],\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "\n",
    "# Ôºà‰øùÁïôÔºâÁµ¶ Adapter ÂÖßÈÉ® batch ÂèØËÉΩÊúÉÁî®Âà∞\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG[\"tft\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Extract tensors (scaled)\n",
    "_, train_X, train_y = train_dataset.tensors\n",
    "_, val_X,   val_y   = val_dataset.tensors\n",
    "_, test_X,  test_y  = test_dataset.tensors\n",
    "\n",
    "if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "\n",
    "# ===== New: Â∞áÁ∂ìÈÅé scaling ÁöÑ y ‰∏≤ÂõûÂÆåÊï¥ÊôÇÈñìÔºå‰æõ Darts/TFT ‰ΩøÁî® =====\n",
    "# ÂéüÊú¨ OLS ÊúÉÁî®ÈÄô‰∫õÂºµÈáèÁõ¥Êé•ÂÅöÂõûÊ≠∏ÔºõÁèæÂú®ÊîπÊàêÁµÑÊàêÂ§öËÆäÈáè TimeSeries\n",
    "y_all_scaled = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T_full, N)\n",
    "x_all_scaled = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T_full, N, F)\n",
    "\n",
    "T_full = y_all_scaled.shape[0]\n",
    "N      = y_all_scaled.shape[1]\n",
    "F      = x_all_scaled.shape[2]\n",
    "\n",
    "# ‰æùÊôÇÈñìÂàáÂàÜÁ¥¢ÂºïÔºàËàá‰∏äÈù¢ÁöÑ ratio ‰∏ÄËá¥Ôºâ\n",
    "train_T = int(T_full * SPLIT_CONFIG[\"train_ratio\"])\n",
    "val_T   = int(T_full * (SPLIT_CONFIG[\"train_ratio\"] + SPLIT_CONFIG[\"val_ratio\"]))\n",
    "test_T  = T_full\n",
    "\n",
    "# Âª∫Á´ã Darts Â§öËÆäÈáè TimeSeriesÔºà‰ª• y ÁöÑÊ≠∑Âè≤‰ΩúÁÇ∫ÂîØ‰∏ÄËº∏ÂÖ•ÔºõËã•Ë¶ÅÂä†ÂÖ• covariates ‰πãÂæåÂÜçÊé•Ôºâ\n",
    "series_all   = TimeSeries.from_values(y_all_scaled)      # shape (T_full, N)\n",
    "series_train = series_all[:train_T]\n",
    "series_val   = series_all[train_T:val_T]\n",
    "series_test  = series_all[val_T:]\n",
    "\n",
    "# ‰øùÁïôÁµ¶ÂæåÁ∫åË©ï‰º∞Â∞çÈΩä\n",
    "y_true_future = y_all_scaled[train_T:]   # (T_val+T_test, N)\n",
    "\n",
    "#Ôºà‰øùÁïôÔºâÂéüÊú¨‰Ω†Â∞±ÊúÉÂç∞ÁöÑË≥áË®ä\n",
    "p_dim = train_X.shape[-1]\n",
    "print(f\"Data shapes (cont_features, targets): {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")\n",
    "print(f\"Feature dimension: {p_dim}\")\n",
    "print(f\"T_full={T_full}, N={N}, F={F} | splits: train={train_T}, val={val_T-train_T}, test={test_T-val_T}\")\n",
    "print(\"Scaled series prepared for TFT (Darts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize data characteristics\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Plot 1: Target distribution\n",
    "# axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "# axes[0, 0].set_title('Target Distribution')\n",
    "# axes[0, 0].set_xlabel('Target Value')\n",
    "# axes[0, 0].set_ylabel('Frequency')\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 2: Spatial pattern at first time step\n",
    "# axes[0, 1].plot(locs, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "# axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "# axes[0, 1].set_xlabel('Location')\n",
    "# axes[0, 1].set_ylabel('Target Value')\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 3: Temporal pattern at middle location\n",
    "# time_steps = np.arange(len(targets))\n",
    "# axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "# axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "# axes[1, 0].set_xlabel('Time Step')\n",
    "# axes[1, 0].set_ylabel('Target Value')\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 4: Feature correlations\n",
    "# feature_corrs = []\n",
    "# for i in range(cont_features.shape[-1]):\n",
    "#     corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "#     feature_corrs.append(corr)\n",
    "\n",
    "# axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "# axes[1, 1].set_title('Feature-Target Correlations')\n",
    "# axes[1, 1].set_xlabel('Feature Index')\n",
    "# axes[1, 1].set_ylabel('Correlation')\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fb6b7",
   "metadata": {},
   "source": [
    "## 4. Baseline Implementation\n",
    " OLS ÊîπÊàê TFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==== TFT Baseline (demo style; no spatial term, no future covariates) ====\n",
    "# print(\"Training TFT baseline (demo-only, no spatial)...\")\n",
    "\n",
    "# pl.seed_everything(CFG[\"tft\"][\"random_state\"])\n",
    "\n",
    "# # 1) Per-location target & past covariatesÔºà‰∏çÂª∫ future covariatesÔºâ\n",
    "# series_list, past_cov_list = [], []\n",
    "# for i in range(N):\n",
    "#     s_i   = TimeSeries.from_values(y_all_scaled[:, i])       # target (T,) -> TS\n",
    "#     cov_i = TimeSeries.from_values(x_all_scaled[:, i, :])    # cont features (T, F) -> TS\n",
    "#     series_list.append(s_i)\n",
    "#     past_cov_list.append(cov_i)\n",
    "\n",
    "# # 2) SplitÔºàtrain/val Â∞çÈΩäÔºâ\n",
    "# series_train_list = [s[:train_T]      for s in series_list]\n",
    "# series_val_list   = [s[train_T:val_T] for s in series_list]\n",
    "# past_cov_train    = [c[:train_T]      for c in past_cov_list]\n",
    "# val_past_covs     = [c[train_T:val_T] for c in past_cov_list]\n",
    "\n",
    "# # 3) Train TFTÔºàÂè™ÂÇ≥ pastÔºõÈ©óË≠â‰πüÂÇ≥ val_past_covariatesÔºâ\n",
    "# tft = TFTModel(\n",
    "#     **TFT_CONFIG,                      # ÈúÄÂê´ add_relative_index=True ÊàñÁ≠âÂÉπ encoders\n",
    "#     pl_trainer_kwargs=PL_TRAINER_KWARGS\n",
    "# )\n",
    "# tft.fit(\n",
    "#     series=series_train_list,\n",
    "#     val_series=series_val_list,\n",
    "#     past_covariates=past_cov_train,\n",
    "#     val_past_covariates=val_past_covs,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # 4) One-step rolling forecast via historical_forecastsÔºàÂè™Áî® past_covariatesÔºâ\n",
    "# val_len  = val_y.shape[0]\n",
    "# test_len = test_y.shape[0]\n",
    "\n",
    "# yhat_val_list, yhat_test_list = [], []\n",
    "# for i in range(N):\n",
    "#     yhat_i = tft.historical_forecasts(\n",
    "#         series=series_list[i],\n",
    "#         past_covariates=past_cov_list[i],\n",
    "#         start=train_T,\n",
    "#         forecast_horizon=1,\n",
    "#         retrain=False,\n",
    "#         verbose=False\n",
    "#     ).values()  # (Tval+Ttest,)\n",
    "#     yhat_val_list.append(yhat_i[:val_len])\n",
    "#     yhat_test_list.append(yhat_i[val_len:val_len + test_len])\n",
    "\n",
    "# # ÂõûÂà∞ (T, N)Ôºàscaled spaceÔºâ\n",
    "# yhat_val_sc  = np.stack(yhat_val_list,  axis=1)\n",
    "# yhat_test_sc = np.stack(yhat_test_list, axis=1)\n",
    "\n",
    "# # 5) ÊåáÊ®ôÂú®ÂéüÂßãÂ∞∫Â∫¶Ë®àÁÆóÔºàdenorm ‚Üí torch.Tensor 2DÔºâ\n",
    "# def to_tensor_2d(x):\n",
    "#     if isinstance(x, np.ndarray): x = torch.from_numpy(x)\n",
    "#     if x.ndim == 3: x = x.squeeze(-1)\n",
    "#     return x.float()\n",
    "\n",
    "# y_val_den_t     = to_tensor_2d(denormalize_predictions(val_y.squeeze(-1),  preprocessor))               # (Tval, N)\n",
    "# y_test_den_t    = to_tensor_2d(denormalize_predictions(test_y.squeeze(-1), preprocessor))               # (Ttest, N)\n",
    "# yhat_val_den_t  = to_tensor_2d(denormalize_predictions(torch.from_numpy(yhat_val_sc).float(),  preprocessor))\n",
    "# yhat_test_den_t = to_tensor_2d(denormalize_predictions(torch.from_numpy(yhat_test_sc).float(), preprocessor))\n",
    "\n",
    "# rmse_tft_val,  mae_tft_val,  r2_tft_val  = compute_metrics(y_val_den_t,  yhat_val_den_t)\n",
    "# rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(y_test_den_t, yhat_test_den_t)\n",
    "\n",
    "# print(f\"TFT Validation - RMSE: {rmse_tft_val:.4f}, R¬≤: {r2_tft_val:.4f}\")\n",
    "# print(f\"TFT Test       - RMSE: {rmse_tft_test:.4f}, R¬≤: {r2_tft_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285ba2",
   "metadata": {},
   "source": [
    "## 5. Main Experiment Function\n",
    "Â≠òÊ™îÂæûseedÊîπÁÇ∫TFT_seed, \n",
    " ÂëºÂè´ OLS ÁöÑÂáΩÂºèÊîπÊàêÂëºÂè´ TFT ÂáΩÂºè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc94b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mk_series_per_station(\n",
    "    train_y, val_y, test_y, train_X, val_X, test_X, split_cfg\n",
    ") -> Tuple[List[TimeSeries], List[TimeSeries], List[TimeSeries], int, int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Â∞á (T, N, F) / (T, N[,1]) ÁöÑÂºµÈáè‰∏≤ÂõûÂÆåÊï¥ÊôÇÈñìÔºåËΩâÊàêÊØèÁ´ô‰∏ÄÊ¢ù TimeSeries + covariatesÔºàdemo È¢®Ê†ºÔºâ„ÄÇ\n",
    "    ÂõûÂÇ≥Ôºöseries_list, past_cov_list, future_cov_list, T_tr, T_va, val_len, test_len, N, F\n",
    "    \"\"\"\n",
    "    if train_y.ndim == 2: train_y = train_y.unsqueeze(-1)\n",
    "    if val_y.ndim   == 2: val_y   = val_y.unsqueeze(-1)\n",
    "    if test_y.ndim  == 2: test_y  = test_y.unsqueeze(-1)\n",
    "\n",
    "    y_all_scaled = torch.cat([train_y, val_y, test_y], dim=0).squeeze(-1).cpu().numpy()  # (T_full, N)\n",
    "    x_all_scaled = torch.cat([train_X, val_X, test_X], dim=0).cpu().numpy()              # (T_full, N, F)\n",
    "\n",
    "    T_full = y_all_scaled.shape[0]\n",
    "    N      = y_all_scaled.shape[1]\n",
    "    F      = x_all_scaled.shape[2]\n",
    "\n",
    "    T_tr = int(T_full * split_cfg[\"train_ratio\"])\n",
    "    T_va = int(T_full * (split_cfg[\"train_ratio\"] + split_cfg[\"val_ratio\"]))\n",
    "    val_len  = T_va - T_tr\n",
    "    test_len = T_full - T_va\n",
    "\n",
    "    series_list, past_cov_list, future_cov_list = [], [], []\n",
    "    for i in range(N):\n",
    "        s_i   = TimeSeries.from_values(y_all_scaled[:, i])    # (T,)\n",
    "        cov_i = TimeSeries.from_values(x_all_scaled[:, i, :]) # (T, F)\n",
    "        series_list.append(s_i)\n",
    "        past_cov_list.append(cov_i)\n",
    "        future_cov_list.append(cov_i)  # ‰øùÁïô‰ªãÈù¢ÔºõËã•‰∏çÁî® future cov ÂèØÂøΩÁï•\n",
    "\n",
    "    return series_list, past_cov_list, future_cov_list, T_tr, T_va, val_len, test_len, N, F\n",
    "\n",
    "\n",
    "def build_tft_and_data(\n",
    "    *,\n",
    "    cat_features: np.ndarray,          # (T, N, ?) ÁõÆÂâçÊú™Áî®Ôºõ‰øùÁïôÂèÉÊï∏‰ª•Áõ∏ÂÆπ\n",
    "    cont_features: np.ndarray,         # (T, N, F)\n",
    "    targets: np.ndarray,               # (T, N)\n",
    "    split_cfg: Dict[str, Any],         # {\"train_ratio\": 0.7, \"val_ratio\": 0.15}\n",
    "    tft_cfg: Dict[str, Any],           # Ë¶ÅÂê´ add_relative_index=True ÊàñÁ≠âÂÉπ encoders\n",
    "    pl_trainer_kwargs: Dict[str, Any],\n",
    ") -> Dict[str, Any]:\n",
    "    # 1) scaling\n",
    "    train_ds, val_ds, test_ds, preproc = prepare_all_with_scaling(\n",
    "        cat_features=cat_features,\n",
    "        cont_features=cont_features,\n",
    "        targets=targets,\n",
    "        train_ratio=split_cfg[\"train_ratio\"],\n",
    "        val_ratio=split_cfg[\"val_ratio\"],\n",
    "        feature_scaler_type=\"standard\",\n",
    "        target_scaler_type=\"standard\",\n",
    "        fit_on_train_only=True,\n",
    "    )\n",
    "    _, train_X, train_y = train_ds.tensors\n",
    "    _, val_X,   val_y   = val_ds.tensors\n",
    "    _, test_X,  test_y  = test_ds.tensors\n",
    "\n",
    "    # 2) ÊØèÁ´ô‰∏ÄÊ¢ù seriesÔºàdemo È¢®Ê†ºÔºâ‚Äî ‰ΩøÁî®Â§ñÂ±§Â∑•ÂÖ∑ÂáΩÂºè\n",
    "    (series_list, past_cov_list, _future_cov_list,\n",
    "     T_tr, T_va, val_len, test_len, N, F) = _mk_series_per_station(\n",
    "        train_y, val_y, test_y, train_X, val_X, test_X, split_cfg\n",
    "    )\n",
    "\n",
    "    # 3) Ë®ìÁ∑¥ TFTÔºàÂè™Áî® past covÔºõÈ©óË≠â‰πüÁµ¶ val_past_covariatesÔºâ\n",
    "    pl.seed_everything(tft_cfg.get(\"random_state\", 42))\n",
    "    tft_model = TFTModel(**tft_cfg, pl_trainer_kwargs=pl_trainer_kwargs)\n",
    "    tft_model.fit(\n",
    "        series=[s[:T_tr] for s in series_list],\n",
    "        val_series=[s[T_tr:T_va] for s in series_list],\n",
    "        past_covariates=[c[:T_tr] for c in past_cov_list],\n",
    "        val_past_covariates=[c[T_tr:T_va] for c in past_cov_list],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 4) Ê≠∑Âè≤‰∏ÄÊ≠•Ôºàval+testÔºâÈ†êÊ∏¨ÔºàscaledÔºâ\n",
    "    yhat_val_list, yhat_test_list = [], []\n",
    "    for i in range(N):\n",
    "        preds = tft_model.historical_forecasts(\n",
    "            series=series_list[i],\n",
    "            past_covariates=past_cov_list[i],\n",
    "            start=T_tr, forecast_horizon=1, retrain=False, verbose=False\n",
    "        ).values()  # (Tval+Ttest,)\n",
    "        yhat_val_list.append(preds[:val_len])\n",
    "        yhat_test_list.append(preds[val_len:val_len+test_len])\n",
    "    yhat_val_sc = np.stack(yhat_val_list, 1)\n",
    "    yhat_test_sc = np.stack(yhat_test_list, 1)\n",
    "\n",
    "    # 5) ÂèçÊ®ôÊ∫ñÂåñÔºàdemo ‰ª•ÂéüÂ∞∫Â∫¶Ë©ï‰º∞Ôºâ\n",
    "    def _t2d(x):\n",
    "        if isinstance(x, np.ndarray): x = torch.from_numpy(x)\n",
    "        if x.ndim == 3: x = x.squeeze(-1)\n",
    "        return x.float()\n",
    "\n",
    "    val_y_den_t     = _t2d(denormalize_predictions(val_y.squeeze(-1),  preproc))\n",
    "    test_y_den_t    = _t2d(denormalize_predictions(test_y.squeeze(-1), preproc))\n",
    "    yhat_val_den_t  = _t2d(denormalize_predictions(torch.from_numpy(yhat_val_sc).float(),  preproc))\n",
    "    yhat_test_den_t = _t2d(denormalize_predictions(torch.from_numpy(yhat_test_sc).float(), preproc))\n",
    "\n",
    "    return {\n",
    "        \"tft_model\": tft_model,\n",
    "        \"preprocessor\": preproc,\n",
    "        \"split_idx\": {\"T_tr\": T_tr, \"T_va\": T_va, \"val_len\": val_len, \"test_len\": test_len},\n",
    "        \"N\": N, \"F\": F,\n",
    "        \"val_y_den_t\": val_y_den_t, \"test_y_den_t\": test_y_den_t,\n",
    "        \"yhat_val_den_t\": yhat_val_den_t, \"yhat_test_den_t\": yhat_test_den_t,\n",
    "        \"val_y_sc\": val_y.squeeze(-1).cpu().numpy(),\n",
    "        \"test_y_sc\": test_y.squeeze(-1).cpu().numpy(),\n",
    "        \"yhat_val_sc\": yhat_val_sc, \"yhat_test_sc\": yhat_test_sc,\n",
    "    }\n",
    "\n",
    "\n",
    "def build_spatial_adapter_from_demo(\n",
    "    *,\n",
    "    tft_model,\n",
    "    device,\n",
    "    n_locations: int,\n",
    "    latent_dim: int,\n",
    "    num_features: int,          # ÂøÖÂÇ≥Áµ¶ TFTWrapper\n",
    "    train_loader,\n",
    "    val_cont: torch.Tensor,\n",
    "    val_y: torch.Tensor,\n",
    "    locs,\n",
    "    adapter_cfg,                # ‚Üê ÂèÉÊï∏ÊîπÂêç\n",
    "    tau1: float,\n",
    "    tau2: float,\n",
    "    writer=None,\n",
    "):\n",
    "    # Èò≤ÂëÜÔºöÁ¢∫‰øùÊòØ dataclassÔºåËÄå‰∏çÊòØ tuple/dict\n",
    "    from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapterConfig\n",
    "    assert isinstance(adapter_cfg, SpatialNeuralAdapterConfig), f\"adapter_cfg type error: {type(adapter_cfg)}\"\n",
    "\n",
    "    trend_backbone = TFTWrapper(\n",
    "        tft_model=tft_model,\n",
    "        num_locations=n_locations,\n",
    "        num_features=num_features,\n",
    "    )\n",
    "    basis = SpatialBasisLearner(n_locations, latent_dim).to(device)\n",
    "\n",
    "    adapter = SpatialNeuralAdapter(\n",
    "        trend_backbone, basis,\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_cont.to(device),\n",
    "        val_y=val_y.to(device),\n",
    "        locs=locs,\n",
    "        config=adapter_cfg,   # ÈÄôË£°Áî®ÊîπÂêçÂæåÁöÑ adapter_cfg\n",
    "        device=device,\n",
    "        writer=writer,\n",
    "        tau1=tau1,\n",
    "        tau2=tau2,\n",
    "    )\n",
    "    return adapter\n",
    "\n",
    "\n",
    "def train_unregularized_adapter(\n",
    "    adapter: SpatialNeuralAdapter,\n",
    "    pretrain_epochs: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DEMO È¢®Ê†º: tau1=tau2=0 ÁöÑ adapter Ë®ìÁ∑¥ÊµÅÁ®ã„ÄÇ\n",
    "    ÂõûÂÇ≥Ôºö{\"adapter\": adapter}\n",
    "    \"\"\"\n",
    "    adapter.pretrain_trend(epochs=pretrain_epochs)\n",
    "    adapter.init_basis_dense()\n",
    "    adapter.run()\n",
    "    return {\"adapter\": adapter}\n",
    "\n",
    "\n",
    "def train_regularized_adapter_with_optuna(\n",
    "    build_adapter_fn,                             # closure: (tau1, tau2) -> SpatialNeuralAdapter\n",
    "    val_y_den_t: torch.Tensor,\n",
    "    predict_val_den_fn,                           # closure: (adapter) -> torch.Tensor denorm predictions on val\n",
    "    n_trials: int = 30,\n",
    "    study_name: str = \"TFT_spatial_adapter_reg\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    DEMO È¢®Ê†ºÔºöOptuna Êêú tau1„ÄÅtau2ÔºåÊåáÊ®ôÂú® **denormalized** Á©∫ÈñìË®àÁÆó„ÄÇ\n",
    "    \"\"\"\n",
    "    def objective(trial: optuna.trial.Trial):\n",
    "        tau1 = trial.suggest_float(\"tau1\", 1e-4, 1e8, log=True)\n",
    "        tau2 = trial.suggest_float(\"tau2\", 1e-4, 1e8, log=True)\n",
    "        adapter = build_adapter_fn(tau1, tau2)\n",
    "        adapter.pretrain_trend(epochs=3)\n",
    "        adapter.init_basis_dense()\n",
    "        adapter.run()\n",
    "\n",
    "        y_val_pred_den = predict_val_den_fn(adapter)\n",
    "        rmse, mae, r2 = compute_metrics(val_y_den_t, y_val_pred_den)\n",
    "\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\",  r2)\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name, direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=MedianPruner(n_warmup_steps=5),\n",
    "        load_if_exists=False,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    best = study.best_trial\n",
    "    return {\n",
    "        \"tau1\": best.params[\"tau1\"],\n",
    "        \"tau2\": best.params[\"tau2\"],\n",
    "        \"rmse\": best.user_attrs[\"rmse\"],\n",
    "        \"mae\":  best.user_attrs[\"mae\"],\n",
    "        \"r2\":   best.user_attrs[\"r2\"],\n",
    "        \"best_trial\": best.number,\n",
    "        \"study\": study,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_adapter_on_test_from_demo(\n",
    "    adapter: SpatialNeuralAdapter,\n",
    "    denorm_true_test: torch.Tensor,\n",
    "    predict_test_den_fn,          # closure: (adapter) -> torch.Tensor denorm predictions on test\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    DEMO È¢®Ê†ºÔºöÂú® test ÈõÜ‰∏äÁî® adapter Êé®Ë´ñÔºåÂõûÂÇ≥ (rmse, mae, r2)ÔºàÂéüÂ∞∫Â∫¶Ôºâ„ÄÇ\n",
    "    \"\"\"\n",
    "    y_test_pred_den = predict_test_den_fn(adapter)\n",
    "    rmse, mae, r2 = compute_metrics(denorm_true_test, y_test_pred_den)\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_experiment(dataset_seed: int, n_trials: int = 30):\n",
    "    log_root = Path(\"TFT_runs\") / f\"TFT_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ----- 1) Data -----\n",
    "    catf, conf, tgts = generate_time_synthetic_data(\n",
    "        locs=locs,\n",
    "        n_time_steps=CFG[\"n_time_steps\"],\n",
    "        noise_std=CFG[\"noise_std\"],\n",
    "        eigenvalue=CFG[\"eigenvalue\"],\n",
    "        eta_rho=0.8,\n",
    "        f_rho=0.6,\n",
    "        global_mean=50.0,\n",
    "        feature_noise_std=0.1,\n",
    "        non_linear_strength=0.2,\n",
    "        seed=dataset_seed,\n",
    "    )\n",
    "    train_ds, val_ds, test_ds, preproc = prepare_all_with_scaling(\n",
    "        cat_features=catf, cont_features=conf, targets=tgts,\n",
    "        train_ratio=SPLIT_CONFIG[\"train_ratio\"], val_ratio=SPLIT_CONFIG[\"val_ratio\"],\n",
    "        feature_scaler_type=\"standard\", target_scaler_type=\"standard\", fit_on_train_only=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds, batch_size=SPATIAL_CONFIG.training.batch_size, shuffle=True)\n",
    "    _, val_X,  val_y  = val_ds.tensors\n",
    "    _, test_X, test_y = test_ds.tensors\n",
    "\n",
    "    # ----- 2) TFT baseline -----\n",
    "    tft_pack = build_tft_and_data(\n",
    "        cat_features=catf, cont_features=conf, targets=tgts,\n",
    "        split_cfg=SPLIT_CONFIG, tft_cfg=TFT_CONFIG, pl_trainer_kwargs=PL_TRAINER_KWARGS,\n",
    "    )\n",
    "    tft_model        = tft_pack[\"tft_model\"]\n",
    "    val_y_den_t      = tft_pack[\"val_y_den_t\"]\n",
    "    test_y_den_t     = tft_pack[\"test_y_den_t\"]\n",
    "    yhat_val_den_t   = tft_pack[\"yhat_val_den_t\"]\n",
    "    yhat_test_den_t  = tft_pack[\"yhat_test_den_t\"]\n",
    "\n",
    "    rmse_tft, mae_tft, r2_tft = compute_metrics(val_y_den_t,  yhat_val_den_t)\n",
    "    rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(test_y_den_t, yhat_test_den_t)\n",
    "\n",
    "    # ----- 3) Unregularized adapter -----\n",
    "    writer_boot = SummaryWriter(log_dir=log_root / \"bootstrap\")\n",
    "    adapter_unreg = build_spatial_adapter_from_demo(\n",
    "        tft_model=tft_model,\n",
    "        device=DEVICE,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "        num_features=val_X.shape[-1],\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_X,\n",
    "        val_y=val_y,\n",
    "        locs=locs,\n",
    "        adapter_cfg=SPATIAL_CONFIG,   # ‚Üê Áî® adapter_cfg\n",
    "        tau1=0.0, tau2=0.0,\n",
    "        writer=writer_boot,\n",
    "    )\n",
    "    train_unregularized_adapter(adapter_unreg, pretrain_epochs=5)\n",
    "    writer_boot.close()\n",
    "\n",
    "    def predict_val_den_fn(adapter):\n",
    "        y_pred_sc = adapter.predict(val_X.to(DEVICE), val_y.to(DEVICE))\n",
    "        if y_pred_sc.ndim == 3: y_pred_sc = y_pred_sc.squeeze(-1)\n",
    "        y_pred_den = denormalize_predictions(y_pred_sc, preproc)\n",
    "        if isinstance(y_pred_den, np.ndarray): y_pred_den = torch.from_numpy(y_pred_den).float()\n",
    "        return y_pred_den\n",
    "\n",
    "    def predict_test_den_fn(adapter):\n",
    "        y_pred_sc = adapter.predict(test_X.to(DEVICE), test_y.to(DEVICE))\n",
    "        if y_pred_sc.ndim == 3: y_pred_sc = y_pred_sc.squeeze(-1)\n",
    "        y_pred_den = denormalize_predictions(y_pred_sc, preproc)\n",
    "        if isinstance(y_pred_den, np.ndarray): y_pred_den = torch.from_numpy(y_pred_den).float()\n",
    "        return y_pred_den\n",
    "\n",
    "    y_unreg_val_den_t  = predict_val_den_fn(adapter_unreg)\n",
    "    y_unreg_test_den_t = predict_test_den_fn(adapter_unreg)\n",
    "    rmse_unreg, mae_unreg, r2_unreg = compute_metrics(val_y_den_t,  y_unreg_val_den_t)\n",
    "    rmse_unreg_test, mae_unreg_test, r2_unreg_test = compute_metrics(test_y_den_t, y_unreg_test_den_t)\n",
    "\n",
    "    # ----- 4) Regularized (Optuna) -----\n",
    "    def build_adapter_fn(tau1: float, tau2: float):\n",
    "        writer = SummaryWriter(log_dir=log_root / f\"trial_tau1_{tau1:.3g}_tau2_{tau2:.3g}\")\n",
    "        adapter = build_spatial_adapter_from_demo(\n",
    "            tft_model=tft_model,\n",
    "            device=DEVICE,\n",
    "            n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "            latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "            num_features=val_X.shape[-1],\n",
    "            train_loader=train_loader,\n",
    "            val_cont=val_X,\n",
    "            val_y=val_y,\n",
    "            locs=locs,\n",
    "            adapter_cfg=SPATIAL_CONFIG,   # ‚Üê ‰∏ÄÂæã adapter_cfg\n",
    "            tau1=tau1, tau2=tau2,\n",
    "            writer=writer,\n",
    "        )\n",
    "        adapter._tmp_writer = writer\n",
    "        return adapter\n",
    "\n",
    "    def predict_val_den_for_optuna(adapter):\n",
    "        y_pred = predict_val_den_fn(adapter)\n",
    "        if hasattr(adapter, \"_tmp_writer\") and adapter._tmp_writer is not None:\n",
    "            adapter._tmp_writer.close()\n",
    "            adapter._tmp_writer = None\n",
    "        return y_pred\n",
    "\n",
    "    reg_search = train_regularized_adapter_with_optuna(\n",
    "        build_adapter_fn=build_adapter_fn,\n",
    "        val_y_den_t=val_y_den_t,\n",
    "        predict_val_den_fn=predict_val_den_for_optuna,\n",
    "        n_trials=n_trials,\n",
    "        study_name=f\"TFT_spatial_adapter_reg_ds{dataset_seed}\",\n",
    "    )\n",
    "    tau1_opt, tau2_opt = reg_search[\"tau1\"], reg_search[\"tau2\"]\n",
    "    rmse_opt, mae_opt, r2_opt = reg_search[\"rmse\"], reg_search[\"mae\"], reg_search[\"r2\"]\n",
    "    best_no = reg_search[\"best_trial\"]\n",
    "\n",
    "    # Best adapter ‚Üí retrain ‚Üí test\n",
    "    writer_best = SummaryWriter(log_dir=log_root / f\"best_tau1_{tau1_opt:.3g}_tau2_{tau2_opt:.3g}\")\n",
    "    adapter_best = build_spatial_adapter_from_demo(\n",
    "        tft_model=tft_model,\n",
    "        device=DEVICE,\n",
    "        n_locations=EXPERIMENT_CONFIG[\"n_locations\"],\n",
    "        latent_dim=EXPERIMENT_CONFIG[\"latent_dim\"],\n",
    "        num_features=val_X.shape[-1],\n",
    "        train_loader=train_loader,\n",
    "        val_cont=val_X,\n",
    "        val_y=val_y,\n",
    "        locs=locs,\n",
    "        adapter_cfg=SPATIAL_CONFIG,   # ‚Üê ‰øÆÊ≠£Ôºö‰Ω†ÂéüÊú¨ÈÄôË£°ÂÇ≥Êàê config=...\n",
    "        tau1=tau1_opt, tau2=tau2_opt,\n",
    "        writer=writer_best,\n",
    "    )\n",
    "    train_unregularized_adapter(adapter_best, pretrain_epochs=5)\n",
    "    writer_best.close()\n",
    "\n",
    "    y_reg_test_den_t = predict_test_den_fn(adapter_best)\n",
    "    rmse_reg_test, mae_reg_test, r2_reg_test = compute_metrics(test_y_den_t, y_reg_test_den_t)\n",
    "\n",
    "    # ----- 5) CSV -----\n",
    "    csv_path = Path(\"metrics_summary_TFT.csv\")\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\"seed\",\"model\",\"trial\",\"tau1\",\"tau2\",\"rmse_val\",\"mae_val\",\"r2_val\",\"rmse_test\",\"mae_test\",\"r2_test\"])\n",
    "        w.writerow([dataset_seed,\"TFT\",\"\", \"\", \"\", f\"{rmse_tft:.6f}\",f\"{mae_tft:.6f}\",f\"{r2_tft:.6f}\",\n",
    "                    f\"{rmse_tft_test:.6f}\",f\"{mae_tft_test:.6f}\",f\"{r2_tft_test:.6f}\"])\n",
    "        w.writerow([dataset_seed,\"Unreg\",\"\", \"0\",\"0\", f\"{rmse_unreg:.6f}\",f\"{mae_unreg:.6f}\",f\"{r2_unreg:.6f}\",\n",
    "                    f\"{rmse_unreg_test:.6f}\",f\"{mae_unreg_test:.6f}\",f\"{r2_unreg_test:.6f}\"])\n",
    "        w.writerow([dataset_seed,\"Reg\", best_no, f\"{tau1_opt:.6g}\",f\"{tau2_opt:.6g}\",\n",
    "                    f\"{rmse_opt:.6f}\",f\"{mae_opt:.6f}\",f\"{r2_opt:.6f}\",\n",
    "                    f\"{rmse_reg_test:.6f}\",f\"{mae_reg_test:.6f}\",f\"{r2_reg_test:.6f}\"])\n",
    "\n",
    "    print(\n",
    "        f\"Dataset {dataset_seed}: \"\n",
    "        f\"TFT {rmse_tft:.3f} | Unreg {rmse_unreg:.3f} | \"\n",
    "        f\"Reg {rmse_opt:.3f} (test {rmse_reg_test:.3f})\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"TFT\":   {\"rmse_val\": rmse_tft, \"rmse_test\": rmse_tft_test, \"r2_val\": r2_tft, \"r2_test\": r2_tft_test},\n",
    "        \"unreg\": {\"rmse_val\": rmse_unreg, \"rmse_test\": rmse_unreg_test, \"r2_val\": r2_unreg, \"r2_test\": r2_unreg_test},\n",
    "        \"reg\":   {\"rmse_val\": rmse_opt, \"rmse_test\": rmse_reg_test, \"r2_val\": r2_opt, \"r2_test\": r2_reg_test,\n",
    "                  \"tau1\": tau1_opt, \"tau2\": tau2_opt},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6642f",
   "metadata": {},
   "source": [
    "## 6. Run Full Experiment Suite\n",
    "before test epoch change to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ababa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for seed in range(EXPERIMENT_TRIALS_CONFIG['seed_range_start'], EXPERIMENT_TRIALS_CONFIG['seed_range_end']):\n",
    "    print(f\"\\nStarting experiment for seed {seed}\")\n",
    "    results = run_one_experiment(seed, n_trials=EXPERIMENT_TRIALS_CONFIG['n_trials_per_seed'])\n",
    "    all_results.append(results)\n",
    "    cache.clear()\n",
    "    clear_gpu_memory()\n",
    "    print(f\"‚úÖ Completed seed {seed}\")\n",
    "\n",
    "print(\"\\nüéâ All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0388ee",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization\n",
    "\n",
    "ÈÇÑÊú™ÂÆåÂÖ®ÊîπÂÆå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_df = pd.read_csv(\"metrics_summary_TFT.csv\")\n",
    "print(\"üìä Results Summary:\")\n",
    "print(results_df.groupby('model')[['rmse_val', 'rmse_test', 'r2_val', 'r2_test']].mean())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_val', ax=axes[0,0])\n",
    "axes[0,0].set_title('Validation RMSE')\n",
    "axes[0,0].set_ylabel('RMSE')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_test', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test RMSE')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤ comparison\n",
    "sns.boxplot(data=results_df, x='model', y='r2_val', ax=axes[1,0])\n",
    "axes[1,0].set_title('Validation R¬≤')\n",
    "axes[1,0].set_ylabel('R¬≤')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='r2_test', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test R¬≤')\n",
    "axes[1,1].set_ylabel('R¬≤')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best hyperparameters for regularized model\n",
    "reg_results = results_df[results_df['model'] == 'Reg']\n",
    "print(\"\\nüîß Best Hyperparameters for Regularized Model:\")\n",
    "print(reg_results[['tau1', 'tau2', 'rmse_val', 'rmse_test']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison summary (TFT baseline)\n",
    "print(\"=== Performance Comparison Summary (TFT as baseline) ===\")\n",
    "\n",
    "# Means by model\n",
    "tft_mean_rmse   = results_df[results_df['model'] == 'TFT']['rmse_test'].mean()\n",
    "unreg_mean_rmse = results_df[results_df['model'] == 'Unreg']['rmse_test'].mean()\n",
    "reg_mean_rmse   = results_df[results_df['model'] == 'Reg']['rmse_test'].mean()\n",
    "\n",
    "print(f\"TFT (baseline)  - Mean Test RMSE: {tft_mean_rmse:.4f}\")\n",
    "print(f\"Unregularized   - Mean Test RMSE: {unreg_mean_rmse:.4f} \"\n",
    "      f\"({(1 - unreg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement vs TFT)\")\n",
    "print(f\"Regularized     - Mean Test RMSE: {reg_mean_rmse:.4f} \"\n",
    "      f\"({(1 - reg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement vs TFT)\")\n",
    "\n",
    "# Statistical significance test (paired by seed): TFT vs Regularized\n",
    "from scipy import stats\n",
    "\n",
    "pivot = results_df.pivot_table(index='seed', columns='model', values='rmse_test', aggfunc='mean')\n",
    "paired = pivot.dropna(subset=['TFT', 'Reg'])  # keep only seeds that have both\n",
    "t_stat, p_value = stats.ttest_rel(paired['TFT'].values, paired['Reg'].values)\n",
    "\n",
    "print(f\"\\nStatistical Test (TFT vs Regularized):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant improvement: {'Yes' if p_value < 0.05 else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
