{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS vs. Spatial Adapter Comparison with Tuning Parameter Selection\n",
    "\n",
    "This notebook implements a comprehensive comparison between:\n",
    "1. **OLS (Ordinary Least Squares)** - Linear baseline\n",
    "2. **Unregularized Spatial Adapter** - Neural spatial model without regularization\n",
    "3. **Regularized Spatial Adapter** - Neural spatial model with optimized tau1, tau2 parameters\n",
    "\n",
    "The experiment uses Optuna for hyperparameter optimization and evaluates performance across multiple random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345997df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import csv\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from optuna.pruners import MedianPruner\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Tuple, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "# Darts / TFT\n",
    "from darts import TimeSeries\n",
    "from darts.models import TFTModel\n",
    "import pytorch_lightning as pl  # 供 Darts 背後使用\n",
    "\n",
    "# Local imports\n",
    "from geospatial_neural_adapter.cpp_extensions import estimate_covariance\n",
    "from geospatial_neural_adapter.utils.experiment import log_covariance_and_basis\n",
    "from geospatial_neural_adapter.utils import (\n",
    "    ModelCache,\n",
    "    clear_gpu_memory,\n",
    "    create_experiment_config,\n",
    "    print_experiment_summary,\n",
    "    get_device_info,\n",
    ")\n",
    "# ⚠️ OLS 相關匯入已移除：compute_ols_coefficients, predict_ols, TrendModel\n",
    "\n",
    "from geospatial_neural_adapter.models.spatial_basis_learner import SpatialBasisLearner\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import SpatialNeuralAdapter\n",
    "from geospatial_neural_adapter.models.pretrained_trend_model import PretrainedTrendModel\n",
    "from geospatial_neural_adapter.models.wrapper_examples.tft_wrapper import TFTWrapper\n",
    "\n",
    "from geospatial_neural_adapter.data.generators import generate_time_synthetic_data\n",
    "from geospatial_neural_adapter.data.preprocessing import prepare_all_with_scaling, denormalize_predictions\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful (TFT version)!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f86c36",
   "metadata": {},
   "source": [
    "## 1. Parameter Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "EXPERIMENT_CONFIG = {\n",
    "    'seed': 42,\n",
    "    'n_time_steps': 1024,\n",
    "    'n_locations': 512,\n",
    "    'noise_std': 4.0,\n",
    "    'eigenvalue': 16.0,\n",
    "    'latent_dim': 1,\n",
    "    'ckpt_dir': \"admm_bcd_ckpts\",\n",
    "}\n",
    "\n",
    "# Spatial Neural Adapter Configuration using dataclasses\n",
    "from geospatial_neural_adapter.models.spatial_neural_adapter import (\n",
    "    SpatialNeuralAdapterConfig, ADMMConfig, TrainingConfig, BasisConfig\n",
    ")\n",
    "\n",
    "# ADMM Configuration\n",
    "admm_config = ADMMConfig(\n",
    "    rho=1.0,  # Base ADMM penalty parameter\n",
    "    dual_momentum=0.2,  # Dual variable momentum\n",
    "    max_iters=3000,  # Maximum ADMM iterations\n",
    "    min_outer=20,  # Minimum outer iterations before convergence check\n",
    "    tol=1e-4,  # Convergence tolerance\n",
    ")\n",
    "\n",
    "# Training Configuration\n",
    "training_config = TrainingConfig(\n",
    "    lr_mu=1e-2,  # Learning rate for trend parameters\n",
    "    batch_size=64,  # Batch size for theta step\n",
    "    pretrain_epochs=5,  # Default pretraining epochs\n",
    "    use_mixed_precision=False,  # Whether to use mixed precision\n",
    ")\n",
    "\n",
    "# Basis Configuration\n",
    "basis_config = BasisConfig(\n",
    "    phi_every=5,  # Update basis every N iterations\n",
    "    phi_freeze=200,  # Stop updating basis after N iterations\n",
    "    matrix_reg=1e-6,  # Matrix regularization for basis update\n",
    "    irl1_max_iters=10,  # IRL₁ maximum iterations\n",
    "    irl1_eps=1e-6,  # IRL₁ epsilon\n",
    "    irl1_tol=5e-4,  # IRL₁ inner tolerance\n",
    ")\n",
    "\n",
    "# Complete Spatial Neural Adapter Configuration\n",
    "SPATIAL_CONFIG = SpatialNeuralAdapterConfig(\n",
    "    admm=admm_config,\n",
    "    training=training_config,\n",
    "    basis=basis_config\n",
    ")\n",
    "\n",
    "# Legacy config dict for backward compatibility (if needed)\n",
    "CFG = SPATIAL_CONFIG.to_dict()\n",
    "CFG.update(EXPERIMENT_CONFIG)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(EXPERIMENT_CONFIG[\"seed\"])\n",
    "Path(EXPERIMENT_CONFIG[\"ckpt_dir\"]).mkdir(exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_info = get_device_info()\n",
    "print(f\"Using {device_info['device'].upper()}: {device_info['device_name']}\")\n",
    "if device_info['device'] == 'cuda':\n",
    "    print(f\"   Memory: {device_info['memory_gb']} GB\")\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Experiment Configuration ===\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n=== Spatial Neural Adapter Configuration ===\")\n",
    "SPATIAL_CONFIG.log_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a32565",
   "metadata": {},
   "source": [
    "## 2. Initialize Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d3a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model cache for hyperparameter optimization\n",
    "cache = ModelCache()\n",
    "\n",
    "# Create experiment configuration\n",
    "EXPERIMENT_TRIALS_CONFIG = create_experiment_config(\n",
    "    n_trials_per_seed=20 if torch.cuda.is_available() else 50,\n",
    "    n_dataset_seeds=10,\n",
    "    seed_range_start=1,\n",
    "    seed_range_end=11,\n",
    ")\n",
    "\n",
    "print_experiment_summary(EXPERIMENT_TRIALS_CONFIG)\n",
    "print(\"Utilities initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777490a",
   "metadata": {},
   "source": [
    "## 3. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19da385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with meaningful correlations\n",
    "print(\"Generating correlated synthetic data...\")\n",
    "\n",
    "locs = np.linspace(-3, 3, CFG[\"n_locations\"])\n",
    "cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "    locs=locs,\n",
    "    n_time_steps=CFG[\"n_time_steps\"],\n",
    "    noise_std=CFG[\"noise_std\"],\n",
    "    eigenvalue=CFG[\"eigenvalue\"],\n",
    "    eta_rho=0.8,\n",
    "    f_rho=0.6,\n",
    "    global_mean=50.0,\n",
    "    feature_noise_std=0.1,\n",
    "    non_linear_strength=0.2,\n",
    "    seed=CFG[\"seed\"]\n",
    ")\n",
    "\n",
    "# Prepare datasets with scaling\n",
    "train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "    cat_features=cat_features,\n",
    "    cont_features=cont_features,\n",
    "    targets=targets,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    feature_scaler_type=\"standard\",\n",
    "    target_scaler_type=\"standard\",\n",
    "    fit_on_train_only=True\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Extract tensors\n",
    "_, train_X, train_y = train_dataset.tensors\n",
    "_, val_X, val_y = val_dataset.tensors\n",
    "_, test_X, test_y = test_dataset.tensors\n",
    "\n",
    "p_dim = train_X.shape[-1]\n",
    "\n",
    "print(f\"Data shapes: {cont_features.shape}, {targets.shape}\")\n",
    "print(f\"Original targets - Mean: {targets.mean():.2f}, Std: {targets.std():.2f}\")\n",
    "print(f\"Original targets - Range: {targets.min():.2f} to {targets.max():.2f}\")\n",
    "print(f\"Feature dimension: {p_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize data characteristics\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Plot 1: Target distribution\n",
    "# axes[0, 0].hist(targets.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "# axes[0, 0].set_title('Target Distribution')\n",
    "# axes[0, 0].set_xlabel('Target Value')\n",
    "# axes[0, 0].set_ylabel('Frequency')\n",
    "# axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 2: Spatial pattern at first time step\n",
    "# axes[0, 1].plot(locs, targets[0, :], 'o-', linewidth=2, markersize=4)\n",
    "# axes[0, 1].set_title('Spatial Pattern at t=0')\n",
    "# axes[0, 1].set_xlabel('Location')\n",
    "# axes[0, 1].set_ylabel('Target Value')\n",
    "# axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 3: Temporal pattern at middle location\n",
    "# time_steps = np.arange(len(targets))\n",
    "# axes[1, 0].plot(time_steps, targets[:, 25], linewidth=2)\n",
    "# axes[1, 0].set_title('Temporal Pattern at Location 25')\n",
    "# axes[1, 0].set_xlabel('Time Step')\n",
    "# axes[1, 0].set_ylabel('Target Value')\n",
    "# axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# # Plot 4: Feature correlations\n",
    "# feature_corrs = []\n",
    "# for i in range(cont_features.shape[-1]):\n",
    "#     corr = np.corrcoef(targets.flatten(), cont_features[:, :, i].flatten())[0, 1]\n",
    "#     feature_corrs.append(corr)\n",
    "\n",
    "# axes[1, 1].bar(range(len(feature_corrs)), feature_corrs, alpha=0.7, edgecolor='black')\n",
    "# axes[1, 1].set_title('Feature-Target Correlations')\n",
    "# axes[1, 1].set_xlabel('Feature Index')\n",
    "# axes[1, 1].set_ylabel('Correlation')\n",
    "# axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fb6b7",
   "metadata": {},
   "source": [
    "## 4. OLS Baseline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f360a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# === Replace OLS trend with TFT trend ======\n",
    "# ===========================================\n",
    "\n",
    "print(\"⚙️ Training TFT trend model... (TFT replaces the original OLS baseline)\")\n",
    "\n",
    "# 取用前面前處理後的資料張量\n",
    "_, train_X, train_y = train_dataset.tensors\n",
    "_, val_X,   val_y   = val_dataset.tensors\n",
    "_, test_X,  test_y  = test_dataset.tensors\n",
    "\n",
    "# 轉成 numpy（使用標準化後的資料）\n",
    "train_X_np = train_X.numpy().astype(np.float32)                # (Ttr, N, F)\n",
    "val_X_np   = val_X.numpy().astype(np.float32)                  # (Tv,  N, F)\n",
    "test_X_np  = test_X.numpy().astype(np.float32)                 # (Tte, N, F)\n",
    "\n",
    "train_y_np = train_y.numpy().squeeze(-1).astype(np.float32)    # (Ttr, N)\n",
    "val_y_np   = val_y.numpy().squeeze(-1).astype(np.float32)      # (Tv,  N)\n",
    "test_y_np  = test_y.numpy().squeeze(-1).astype(np.float32)     # (Tte, N)\n",
    "\n",
    "# 基本維度\n",
    "Ttr, N, F = train_X_np.shape[0], train_X_np.shape[1], train_X_np.shape[2]\n",
    "p_dim = F\n",
    "\n",
    "# === 建立 Darts 的 multivariate TimeSeries（target 與 past covariates）===\n",
    "# 將 train/val/test 串成完整序列供 Darts 建 index；訓練只切到 training_cutoff\n",
    "cont_np_full    = np.concatenate([train_X_np, val_X_np, test_X_np], axis=0)   # (T_full, N, F)\n",
    "targets_np_full = np.concatenate([train_y_np, val_y_np, test_y_np], axis=0)   # (T_full, N)\n",
    "T_full = cont_np_full.shape[0]\n",
    "\n",
    "time_index = pd.RangeIndex(start=0, stop=T_full, step=1)\n",
    "\n",
    "# 目標（多變量：每個 location 一欄）\n",
    "target_df = pd.DataFrame(targets_np_full, index=time_index, columns=[f\"loc_{i}\" for i in range(N)])\n",
    "target_ts = TimeSeries.from_dataframe(target_df, fill_missing_dates=True)\n",
    "\n",
    "# past covariates（展平成 (T_full, N*F)）\n",
    "cov_cols = [f\"cov_{j}_loc_{i}\" for i in range(N) for j in range(F)]\n",
    "cov_df   = pd.DataFrame(cont_np_full.reshape(T_full, N*F), index=time_index, columns=cov_cols)\n",
    "cov_ts   = TimeSeries.from_dataframe(cov_df, fill_missing_dates=True)\n",
    "\n",
    "# 訓練只用 training_cutoff\n",
    "training_cutoff = len(train_dataset)  # = Ttr\n",
    "\n",
    "# === 輕量 TFT：以 trend 抽取為目的 ===\n",
    "from darts.models import TFTModel\n",
    "import torch\n",
    "\n",
    "input_chunk_length = max(8, min(64, Ttr // 4))\n",
    "tft_model = TFTModel(\n",
    "    input_chunk_length=input_chunk_length,\n",
    "    output_chunk_length=1,\n",
    "    n_epochs=10,\n",
    "    hidden_size=64,\n",
    "    num_attention_heads=2,\n",
    "    dropout=0.1,\n",
    "    random_state=42,\n",
    "    force_reset=True,\n",
    "    pl_trainer_kwargs={\n",
    "        \"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"devices\": -1,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"enable_model_summary\": False,\n",
    "        \"enable_checkpointing\": False,\n",
    "        \"max_epochs\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Training TFT (input_chunk_length={input_chunk_length}, epochs=10)...\")\n",
    "tft_model.fit(\n",
    "    target_ts[:training_cutoff],\n",
    "    past_covariates=cov_ts[:training_cutoff],\n",
    "    verbose=True\n",
    ")\n",
    "print(\"✅ TFT training done.\")\n",
    "\n",
    "# === 包裝成 PretrainedTrendModel（TFTWrapper + PretrainedTrendModel）===\n",
    "from geospatial_neural_adapter.models.pretrained_trend_model import PretrainedTrendModel\n",
    "from geospatial_neural_adapter.models.wrapper_examples.tft_wrapper import TFTWrapper\n",
    "\n",
    "wrapper = TFTWrapper(\n",
    "    tft_model=tft_model,\n",
    "    num_locations=N,\n",
    "    num_features=F\n",
    ")\n",
    "tft_trend_model = PretrainedTrendModel(\n",
    "    pretrained_model=wrapper,\n",
    "    input_shape=(None, N, F),\n",
    "    output_shape=(None, N),\n",
    "    freeze_backbone=True,      # TFT backbone 凍結\n",
    "    add_residual_head=True,    # 加一層可訓練 head，讓 adapter 訓練更穩\n",
    "    residual_hidden_dim=64,\n",
    "    dropout_rate=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"✅ TFT trend model (wrapper) ready!\")\n",
    "\n",
    "# === 取得 VAL / TEST 的 trend 預測（用 wrapper 逐步取出）===\n",
    "import torch\n",
    "\n",
    "def wrapper_predict_seq(x_np):\n",
    "    \"\"\"x_np: (T, N, F) -> return (T, N) using tft_trend_model\"\"\"\n",
    "    preds = []\n",
    "    tft_trend_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for t in range(x_np.shape[0]):\n",
    "            x_t = torch.tensor(x_np[t:t+1], dtype=torch.float32, device=DEVICE)  # (1, N, F)\n",
    "            y_t = tft_trend_model(x_t)  # (1, N)\n",
    "            preds.append(y_t.squeeze(0).cpu())\n",
    "    return torch.stack(preds, dim=0)  # (T, N)\n",
    "\n",
    "y_trend_val  = wrapper_predict_seq(val_X_np)   # (Tv,  N)\n",
    "y_trend_test = wrapper_predict_seq(test_X_np)  # (Tte, N)\n",
    "\n",
    "# === 以 TFT 殘差估共變異，初始化 top-K eigen-basis（取代 OLS 殘差）===\n",
    "residuals_val = torch.tensor(val_y_np,  dtype=torch.float32, device=DEVICE) - y_trend_val.to(DEVICE)  # (Tv, N)\n",
    "covariance_matrix = residuals_val.transpose(0, 1) @ residuals_val  # (N, N)\n",
    "\n",
    "K = CFG[\"latent_dim\"]\n",
    "eig = torch.linalg.eigh(covariance_matrix)\n",
    "eigenvectors = eig.eigenvectors[:, -K:]  # 取最大 K 個特徵向量\n",
    "\n",
    "from geospatial_neural_adapter.models.spatial_basis_learner import SpatialBasisLearner\n",
    "tft_basis = SpatialBasisLearner(CFG[\"n_locations\"], K).to(DEVICE)\n",
    "tft_basis.basis.data.copy_(eigenvectors)\n",
    "\n",
    "print(f\"Initialized spatial basis from TFT residuals: basis shape = {tft_basis.basis.shape}\")\n",
    "\n",
    "# === （Optional）保留真實空間基底做參考：與 OLS/TFT 無關 ===\n",
    "phi_true = np.exp(-(locs**2))[:, None]\n",
    "phi_true /= np.linalg.norm(phi_true)\n",
    "sigma_true_spatial = CFG[\"eigenvalue\"] * (phi_true @ phi_true.T)\n",
    "\n",
    "# === TFT baseline metrics（取代 OLS metrics）===\n",
    "from geospatial_neural_adapter.metrics import compute_metrics\n",
    "\n",
    "rmse_tft_val,  mae_tft_val,  r2_tft_val  = compute_metrics(\n",
    "    torch.tensor(val_y_np,  dtype=torch.float32, device=DEVICE),\n",
    "    y_trend_val.to(DEVICE)\n",
    ")\n",
    "rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(\n",
    "    torch.tensor(test_y_np, dtype=torch.float32, device=DEVICE),\n",
    "    y_trend_test.to(DEVICE)\n",
    ")\n",
    "\n",
    "print(f\"TFT Validation - RMSE: {rmse_tft_val:.4f}, R²: {r2_tft_val:.4f}\")\n",
    "print(f\"TFT Test       - RMSE: {rmse_tft_test:.4f}, R²: {r2_tft_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3285ba2",
   "metadata": {},
   "source": [
    "## 5. Main Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_experiment(dataset_seed: int, n_trials: int = 30):\n",
    "    \"\"\"\n",
    "    Run a complete experiment for one dataset seed, using TFT as the trend model\n",
    "    (replacing the original OLS trend). We compare:\n",
    "      1) TFT (trend-only baseline)\n",
    "      2) Unregularized Spatial Adapter (tau1=tau2=0)\n",
    "      3) Regularized Spatial Adapter (Optuna tuned tau1, tau2)\n",
    "    \"\"\"\n",
    "    from darts import TimeSeries\n",
    "    from darts.models import TFTModel\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    import optuna\n",
    "    from optuna.pruners import MedianPruner\n",
    "    import torch\n",
    "\n",
    "    log_root = Path(\"TFT_runs\") / f\"TFT_seed_{dataset_seed}\"\n",
    "    log_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1) Generate dataset for this seed\n",
    "    # -------------------------------\n",
    "    cat_features, cont_features, targets = generate_time_synthetic_data(\n",
    "        locs=locs,\n",
    "        n_time_steps=CFG[\"n_time_steps\"],\n",
    "        noise_std=CFG[\"noise_std\"],\n",
    "        eigenvalue=CFG[\"eigenvalue\"],\n",
    "        eta_rho=0.8,\n",
    "        f_rho=0.6,\n",
    "        global_mean=50.0,\n",
    "        feature_noise_std=0.1,\n",
    "        non_linear_strength=0.2,\n",
    "        seed=dataset_seed\n",
    "    )\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset, preprocessor = prepare_all_with_scaling(\n",
    "        cat_features=cat_features,\n",
    "        cont_features=cont_features,\n",
    "        targets=targets,\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15,\n",
    "        feature_scaler_type=\"standard\",\n",
    "        target_scaler_type=\"standard\",\n",
    "        fit_on_train_only=True\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=SPATIAL_CONFIG.training.batch_size, shuffle=True)\n",
    "\n",
    "    # unpack tensors\n",
    "    _, train_X, train_y = train_dataset.tensors\n",
    "    _, val_X,   val_y   = val_dataset.tensors\n",
    "    _, test_X,  test_y  = test_dataset.tensors\n",
    "\n",
    "    p_dim = train_X.shape[-1]  # num features\n",
    "    N     = train_X.shape[1]   # num locations\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2) Train TFT trend (multivariate) on TRAIN only\n",
    "    # -------------------------------\n",
    "    # Build TimeSeries for Darts: use scaled data\n",
    "    train_X_np = train_X.numpy()                  # (Ttr, N, F)\n",
    "    train_y_np = train_y.numpy().squeeze(-1)      # (Ttr, N)\n",
    "    val_X_np   = val_X.numpy()                    # (Tv,  N, F)\n",
    "    val_y_np   = val_y.numpy().squeeze(-1)        # (Tv,  N)\n",
    "    test_X_np  = test_X.numpy()                   # (Tte, N, F)\n",
    "    test_y_np  = test_y.numpy().squeeze(-1)       # (Tte, N)\n",
    "\n",
    "    Ttr = train_X_np.shape[0]\n",
    "    Tv  = val_X_np.shape[0]\n",
    "    Tte = test_X_np.shape[0]\n",
    "\n",
    "    # For Darts multivariate target\n",
    "    idx_tr = pd.RangeIndex(start=0, stop=T_full, step=1)\n",
    "    target_tr_df = pd.DataFrame(train_y_np, index=idx_tr, columns=[f\"loc_{i}\" for i in range(N)])\n",
    "    ts_target_tr = TimeSeries.from_dataframe(target_tr_df, fill_missing_dates=True)\n",
    "\n",
    "    # Lightweight TFT config for trend\n",
    "    input_chunk_length = max(8, min(64, Ttr // 4))\n",
    "    tft_model = TFTModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=1,\n",
    "        n_epochs=10,\n",
    "        hidden_size=64,\n",
    "        num_attention_heads=2,\n",
    "        dropout=0.1,\n",
    "        random_state=42,\n",
    "        force_reset=True,\n",
    "        pl_trainer_kwargs={\n",
    "            \"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"devices\": -1,\n",
    "            \"enable_progress_bar\": True,\n",
    "            \"enable_model_summary\": False,\n",
    "            \"enable_checkpointing\": False,\n",
    "            \"max_epochs\": 10,\n",
    "        },\n",
    "    )\n",
    "    tft_model.fit(ts_target_tr, verbose=True)\n",
    "\n",
    "    # Wrap TFT as PretrainedTrendModel\n",
    "    wrapper = TFTWrapper(tft_model=tft_model, num_locations=N, num_features=p_dim)\n",
    "    tft_trend_model = PretrainedTrendModel(\n",
    "        pretrained_model=wrapper,\n",
    "        input_shape=(None, N, p_dim),\n",
    "        output_shape=(None, N),\n",
    "        freeze_backbone=True,      # keep TFT frozen during adapter training\n",
    "        add_residual_head=True,    # small residual head trainable if needed\n",
    "        residual_hidden_dim=64,\n",
    "        dropout_rate=0.1,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 3) Evaluate TFT-only (trend baseline) on VAL/TEST\n",
    "    # -------------------------------\n",
    "    def predict_with_wrapper(x_np):\n",
    "        # x_np: (T, N, F), return (T, N)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for t in range(x_np.shape[0]):\n",
    "                x_t = torch.tensor(x_np[t:t+1], dtype=torch.float32, device=DEVICE)  # (1,N,F)\n",
    "                y_t = tft_trend_model(x_t)  # (1,N)\n",
    "                preds.append(y_t.squeeze(0).cpu().numpy())\n",
    "        return np.stack(preds, axis=0)\n",
    "\n",
    "    y_tft_val_np  = predict_with_wrapper(val_X_np)   # (Tv, N)\n",
    "    y_tft_test_np = predict_with_wrapper(test_X_np)  # (Tte, N)\n",
    "\n",
    "    # metrics (standardized space)\n",
    "    rmse_tft, mae_tft, r2_tft = compute_metrics(\n",
    "        torch.from_numpy(val_y_np).to(DEVICE),\n",
    "        torch.from_numpy(y_tft_val_np).to(DEVICE)\n",
    "    )\n",
    "    rmse_tft_test, mae_tft_test, r2_tft_test = compute_metrics(\n",
    "        torch.from_numpy(test_y_np).to(DEVICE),\n",
    "        torch.from_numpy(y_tft_test_np).to(DEVICE)\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # 4) Bootstrap: Unregularized Spatial Adapter (tau1=tau2=0)\n",
    "    # -------------------------------\n",
    "    cache.clear()\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # Create fresh trend & basis for bootstrap\n",
    "    boot_trend = PretrainedTrendModel(\n",
    "        pretrained_model=wrapper,\n",
    "        input_shape=(None, N, p_dim),\n",
    "        output_shape=(None, N),\n",
    "        freeze_backbone=True,\n",
    "        add_residual_head=True,\n",
    "        residual_hidden_dim=64,\n",
    "        dropout_rate=0.1,\n",
    "    ).to(DEVICE)\n",
    "    boot_basis = SpatialBasisLearner(N, CFG[\"latent_dim\"]).to(DEVICE)\n",
    "\n",
    "    boot_writer = SummaryWriter(log_dir=log_root / \"bootstrap\")\n",
    "    boot = SpatialNeuralAdapter(\n",
    "        boot_trend,\n",
    "        boot_basis,\n",
    "        train_loader,\n",
    "        val_cont=val_X.to(DEVICE),\n",
    "        val_y=val_y.to(DEVICE),\n",
    "        locs=locs,\n",
    "        config=SPATIAL_CONFIG,\n",
    "        device=DEVICE,\n",
    "        writer=boot_writer,\n",
    "        tau1=0.0,\n",
    "        tau2=0.0,\n",
    "    )\n",
    "    # skip heavy pretrain; TFT backbone is frozen & pretrained\n",
    "    boot.init_basis_dense()\n",
    "    boot.run()\n",
    "    cache.store(0.0, 0.0, boot_trend.state_dict(), boot_basis.state_dict())\n",
    "    boot_writer.close()\n",
    "\n",
    "    # Unregularized predictions\n",
    "    y_boot_val = boot.predict(val_X.to(DEVICE), val_y.to(DEVICE))\n",
    "    rmse_boot, mae_boot, r2_boot = compute_metrics(val_y.to(DEVICE), y_boot_val)\n",
    "    y_boot_test = boot.predict(test_X.to(DEVICE), test_y.to(DEVICE))\n",
    "    rmse_boot_test, mae_boot_test, r2_boot_test = compute_metrics(test_y.to(DEVICE), y_boot_test)\n",
    "\n",
    "    # Clean up bootstrap models\n",
    "    del boot_trend, boot_basis, boot\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    # -------------------------------\n",
    "    # 5) Optuna objective: tune (tau1, tau2) for Regularized Adapter\n",
    "    # -------------------------------\n",
    "    def objective(trial):\n",
    "        dev = DEVICE\n",
    "        tau1 = trial.suggest_float(\"tau1\", 1e-4, 1e8, log=True)\n",
    "        tau2 = trial.suggest_float(\"tau2\", 1e-4, 1e8, log=True)\n",
    "\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        # fresh trend & basis\n",
    "        trend = PretrainedTrendModel(\n",
    "            pretrained_model=wrapper,\n",
    "            input_shape=(None, N, p_dim),\n",
    "            output_shape=(None, N),\n",
    "            freeze_backbone=True,\n",
    "            add_residual_head=True,\n",
    "            residual_hidden_dim=64,\n",
    "            dropout_rate=0.1,\n",
    "        ).to(dev)\n",
    "        basis = SpatialBasisLearner(N, CFG[\"latent_dim\"]).to(dev)\n",
    "\n",
    "        # warm-start from nearest cached pair (if any)\n",
    "        cache.load_nearest(trend, basis, tau1, tau2)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=log_root / f\"trial_{trial.number:03d}\")\n",
    "        trainer = SpatialNeuralAdapter(\n",
    "            trend,\n",
    "            basis,\n",
    "            train_loader,\n",
    "            val_cont=val_X.to(dev),\n",
    "            val_y=val_y.to(dev),\n",
    "            locs=locs,\n",
    "            config=SPATIAL_CONFIG,\n",
    "            device=dev,\n",
    "            writer=writer,\n",
    "            tau1=tau1,\n",
    "            tau2=tau2,\n",
    "        )\n",
    "        trainer.init_basis_dense()\n",
    "        trainer.run()\n",
    "\n",
    "        y_pred = trainer.predict(val_X.to(dev), val_y.to(dev))\n",
    "        rmse, mae, r2 = compute_metrics(val_y.to(dev), y_pred)\n",
    "\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\", r2)\n",
    "\n",
    "        writer.close()\n",
    "        cache.store(tau1, tau2, trend.state_dict(), basis.state_dict())\n",
    "\n",
    "        # cleanup\n",
    "        del trend, basis, trainer, y_pred\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"spatial_adapter_tft_ds{dataset_seed}\",\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=MedianPruner(n_warmup_steps=5),\n",
    "        load_if_exists=False,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=1)\n",
    "\n",
    "    # Best regularized model\n",
    "    best = study.best_trial\n",
    "    rmse_opt = best.user_attrs[\"rmse\"]\n",
    "    mae_opt  = best.user_attrs[\"mae\"]\n",
    "    r2_opt   = best.user_attrs[\"r2\"]\n",
    "    tau1_opt = best.params[\"tau1\"]\n",
    "    tau2_opt = best.params[\"tau2\"]\n",
    "    best_no  = best.number\n",
    "\n",
    "    dev_best = DEVICE\n",
    "    trend_best = PretrainedTrendModel(\n",
    "        pretrained_model=wrapper,\n",
    "        input_shape=(None, N, p_dim),\n",
    "        output_shape=(None, N),\n",
    "        freeze_backbone=True,\n",
    "        add_residual_head=True,\n",
    "        residual_hidden_dim=64,\n",
    "        dropout_rate=0.1,\n",
    "    ).to(dev_best)\n",
    "    basis_best = SpatialBasisLearner(N, CFG[\"latent_dim\"]).to(dev_best)\n",
    "\n",
    "    # load cached states for best taus\n",
    "    sd_t, sd_b = cache.cache[(tau1_opt, tau2_opt)]\n",
    "    trend_best.load_state_dict(sd_t)\n",
    "    basis_best.load_state_dict(sd_b)\n",
    "\n",
    "    trend_best.eval(); basis_best.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test = test_X.to(dev_best)\n",
    "        y_test = test_y.to(dev_best)\n",
    "        y_reg_test = SpatialNeuralAdapter.predict_static(trend_best, basis_best, X_test, y_test)\n",
    "    rmse_test, mae_test, r2_test = compute_metrics(y_test, y_reg_test)\n",
    "\n",
    "    # -------------------------------\n",
    "    # 6) Write CSV summary (TFT / Unreg / Reg)\n",
    "    # -------------------------------\n",
    "    csv_path = Path(\"metrics_summary_TFT.csv\")\n",
    "    write_header = not csv_path.exists()\n",
    "    with csv_path.open(\"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow([\n",
    "                \"seed\", \"model\", \"trial\", \"tau1\", \"tau2\",\n",
    "                \"rmse_val\", \"mae_val\", \"r2_val\",\n",
    "                \"rmse_test\", \"mae_test\", \"r2_test\"\n",
    "            ])\n",
    "\n",
    "        # TFT trend-only baseline\n",
    "        w.writerow([\n",
    "            dataset_seed, \"TFT\", \"\", \"\", \"\",\n",
    "            f\"{rmse_tft:.6f}\", f\"{mae_tft:.6f}\", f\"{r2_tft:.6f}\",\n",
    "            f\"{rmse_tft_test:.6f}\", f\"{mae_tft_test:.6f}\", f\"{r2_tft_test:.6f}\"\n",
    "        ])\n",
    "\n",
    "        # Unregularized Adapter\n",
    "        w.writerow([\n",
    "            dataset_seed, \"Unreg\", \"\", \"0\", \"0\",\n",
    "            f\"{rmse_boot:.6f}\", f\"{mae_boot:.6f}\", f\"{r2_boot:.6f}\",\n",
    "            f\"{rmse_boot_test:.6f}\", f\"{mae_boot_test:.6f}\", f\"{r2_boot_test:.6f}\"\n",
    "        ])\n",
    "\n",
    "        # Regularized Adapter (best)\n",
    "        w.writerow([\n",
    "            dataset_seed, \"Reg\", best_no, f\"{tau1_opt:.6g}\", f\"{tau2_opt:.6g}\",\n",
    "            f\"{rmse_opt:.6f}\", f\"{mae_opt:.6f}\", f\"{r2_opt:.6f}\",\n",
    "            f\"{rmse_test:.6f}\", f\"{mae_test:.6f}\", f\"{r2_test:.6f}\"\n",
    "        ])\n",
    "\n",
    "    print(\n",
    "        f\"Dataset {dataset_seed}:  \"\n",
    "        f\"TFT RMSE={rmse_tft:.3f} | \"\n",
    "        f\"Unreg RMSE={rmse_boot:.3f} | \"\n",
    "        f\"Reg RMSE={rmse_opt:.3f} (test {rmse_test:.3f})\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'tft':  {'rmse_val': rmse_tft,  'rmse_test': rmse_tft_test,  'r2_val': r2_tft,  'r2_test': r2_tft_test},\n",
    "        'unreg':{'rmse_val': rmse_boot, 'rmse_test': rmse_boot_test, 'r2_val': r2_boot, 'r2_test': r2_boot},\n",
    "        'reg':  {'rmse_val': rmse_opt,  'rmse_test': rmse_test,      'r2_val': r2_opt,  'r2_test': r2_test,\n",
    "                 'tau1': tau1_opt, 'tau2': tau2_opt}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6642f",
   "metadata": {},
   "source": [
    "## 6. Run Full Experiment Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ababa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for seed in range(EXPERIMENT_TRIALS_CONFIG['seed_range_start'], EXPERIMENT_TRIALS_CONFIG['seed_range_end']):\n",
    "    print(f\"\\nStarting experiment for seed {seed}\")\n",
    "    results = run_one_experiment(seed, n_trials=EXPERIMENT_TRIALS_CONFIG['n_trials_per_seed'])\n",
    "    all_results.append(results)\n",
    "    # Clear cache between seeds to free memory\n",
    "    cache.clear()\n",
    "    clear_gpu_memory()\n",
    "    print(f\"✅ Completed seed {seed}\")\n",
    "\n",
    "print(\"\\n🎉 All experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0388ee",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_df = pd.read_csv(\"metrics_summary_TFT.csv\")\n",
    "print(\"📊 Results Summary:\")\n",
    "print(results_df.groupby('model')[['rmse_val', 'rmse_test', 'r2_val', 'r2_test']].mean())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# RMSE comparison\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_val', ax=axes[0,0])\n",
    "axes[0,0].set_title('Validation RMSE')\n",
    "axes[0,0].set_ylabel('RMSE')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='rmse_test', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test RMSE')\n",
    "axes[0,1].set_ylabel('RMSE')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "sns.boxplot(data=results_df, x='model', y='r2_val', ax=axes[1,0])\n",
    "axes[1,0].set_title('Validation R²')\n",
    "axes[1,0].set_ylabel('R²')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=results_df, x='model', y='r2_test', ax=axes[1,1])\n",
    "axes[1,1].set_title('Test R²')\n",
    "axes[1,1].set_ylabel('R²')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best hyperparameters for regularized model\n",
    "reg_results = results_df[results_df['model'] == 'Reg']\n",
    "print(\"\\n🔧 Best Hyperparameters for Regularized Model:\")\n",
    "print(reg_results[['tau1', 'tau2', 'rmse_val', 'rmse_test']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Performance comparison summary (TFT is the baseline now) ===\n",
    "print(\"=== Performance Comparison Summary ===\")\n",
    "\n",
    "tft_mean_rmse   = results_df[results_df['model'] == 'TFT']['rmse_test'].mean()\n",
    "unreg_mean_rmse = results_df[results_df['model'] == 'Unreg']['rmse_test'].mean()\n",
    "reg_mean_rmse   = results_df[results_df['model'] == 'Reg']['rmse_test'].mean()\n",
    "\n",
    "print(f\"TFT (baseline) - Mean Test RMSE: {tft_mean_rmse:.4f}\")\n",
    "print(f\"Unregularized  - Mean Test RMSE: {unreg_mean_rmse:.4f} ({(1 - unreg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement)\")\n",
    "print(f\"Regularized    - Mean Test RMSE: {reg_mean_rmse:.4f} ({(1 - reg_mean_rmse/tft_mean_rmse)*100:.1f}% improvement)\")\n",
    "\n",
    "# Statistical significance test (pair by seed if你有對應關係；此處簡單比較分布)\n",
    "from scipy import stats\n",
    "tft_scores = results_df[results_df['model'] == 'TFT']['rmse_test'].values\n",
    "reg_scores = results_df[results_df['model'] == 'Reg']['rmse_test'].values\n",
    "\n",
    "# 注意：若不同 row 不完全配對同一 seed，這裡用獨立樣本 t 檢定較合理；若完全一一對應，可改 ttest_rel\n",
    "t_stat, p_value = stats.ttest_ind(tft_scores, reg_scores, equal_var=False)\n",
    "print(f\"\\nStatistical Test (TFT vs Regularized):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant improvement: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial-neural-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
